<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.mdp API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.mdp</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from datetime import datetime
from inspect import signature
from matplotlib import animation, colors, patches
from matplotlib import pyplot as plt
from scipy.optimize import milp, LinearConstraint
from scipy.spatial.distance import cdist
from tqdm.auto import trange
from typing import Self, Union, Tuple

import copy
import json
import os
import pandas as pd
import random

import numpy as np
gpu_support = False
try:
    import cupy as cp
    gpu_support = True
except:
    print(&#39;[Warning] Cupy could not be loaded: GPU support is not available.&#39;)


COLOR_LIST = [{
    &#39;name&#39;: item.replace(&#39;tab:&#39;,&#39;&#39;),
    &#39;id&#39;: item,
    &#39;hex&#39;: value,
    &#39;rgb&#39;: [int(value.lstrip(&#39;#&#39;)[i:i + (len(value)-1) // 3], 16) for i in range(0, (len(value)-1), (len(value)-1) // 3)]
    } for item, value in colors.TABLEAU_COLORS.items()] # type: ignore

COLOR_ARRAY = np.array([c[&#39;rgb&#39;] for c in COLOR_LIST])


def log(content:str) -&gt; None:
    &#39;&#39;&#39;
    Function to print a log line with a timestamp.

    Parameters
    ----------
    content : str
        The content to be printed as a log.
    &#39;&#39;&#39;
    print(f&#39;[{datetime.now().strftime(&#34;%m/%d/%Y, %H:%M:%S&#34;)}] &#39; + content)


class Model:
    &#39;&#39;&#39;
    MDP Model class.

    ...

    Parameters
    ----------
    states : int or list[str] or list[list[str]]
        A list of state labels or an amount of states to be used. Also allows to provide a matrix of states to define a grid model.
    actions : int or list
        A list of action labels or an amount of actions to be used.
    transitions : array-like or function, optional
        The transitions between states, an array can be provided and has to be |S| x |A| x |S| or a function can be provided. 
        If a function is provided, it has be able to deal with np.array arguments.
        If none is provided, it will be randomly generated.
    reachable_states : array-like, optional
        A list of states that can be reached from each state and actions. It must be a matrix of size |S| x |A| x |R| where |R| is the max amount of states reachable from any given state and action pair.
        It is optional but useful for speedup purposes.
    rewards : array-like or function, optional
        The reward matrix, has to be |S| x |A| x |S|.
        A function can also be provided here but it has to be able to deal with np.array arguments.
        If provided, it will be use in combination with the transition matrix to fill to expected rewards.
    rewards_are_probabilistic : bool, default=False
        Whether the rewards provided are probabilistic or pure rewards. If probabilist 0 or 1 will be the reward with a certain probability.
    state_grid : array-like, optional
        If provided, the model will be converted to a grid model.
    start_probabilities : list, optional
        The distribution of chances to start in each state. If not provided, there will be an uniform chance for each state.
    end_states : list, optional
        Entering either state in the list during a simulation will end the simulation.
    end_action : list, optional
        Playing action of the list during a simulation will end the simulation.
    
    Attributes
    ----------
    states : np.ndarray
        A 1D array of states indices. Used to loop over states.
    state_labels : list[str]
        A list of state labels. (To be mainly used for plotting)
    state_count : int
        How many states are in the Model.
    state_grid : np.ndarray
        The state indices organized as a 2D grid. (Used for plotting purposes)
    actions : np.ndarry
        A 1D array of action indices. Used to loop over actions.
    action_labels : list[str]
        A list of action labels. (To be mainly used for plotting)
    action_count : int
        How many action are in the Model.
    transition_table : np.ndarray
        A 3D matrix of the transition probabilities.
        Can be None in the case a transition function is provided instead.
        Note: When possible, use reachable states and reachable probabilities instead.
    transition_function : function
        A callable function taking 3 arguments: s, a, s_p; and returning a float between 0.0 and 1.0.
        Can be None in the case a transition table is provided instead.
        Note: When possible, use reachable states and reachable probabilities instead.
    reachable_states : np.ndarray
        A 3D array of the shape S x A x R, where R is max amount to states that can be reached from any state-action pair.
    reachable_probabilities : np.ndarray
        A 3D array of the same shape as reachable_states, the array represent the probability of reaching the state pointed by the reachable_states matrix.
    reachable_state_count : int
        The maximum of states that can be reached from any state-action combination.
    immediate_reward_table : np.ndarray
        A 3D matrix of shape S x A x S of the reward that will received when taking action a, in state s and landing in state s_p.
        Can be None in the case an immediate rewards function is provided instead.
    immediate_reward_function : function
        A callable function taking 3 argments: s, a, s_p and returning the immediate reward the agent will receive.
        Can be None in the case an immediate rewards function is provided instead.
    expected_reward_table : np.ndarray
        A 2D array of shape S x A. It represents the rewards that is expected to be received when taking action a from state s.
        It is made by taking the weighted average of immediate rewards and the transitions.
    start_probabilities : np.ndarray
        A 1D array of length |S| containing the probility distribution of the agent starting in each state.
    rewards_are_probabilisitic : bool
        Whether the immediate rewards are probabilitic, ie: returning a 0 or 1 based on the reward that is considered to be a probability.
    end_states : list[int]
        A list of states that, when reached, terminate a simulation.
    end_actions : list[int]
        A list of actions that, when taken, terminate a simulation.
    is_on_gpu : bool
        Whether the numpy array of the model are stored on the gpu or not.
    gpu_model : mdp.Model
        An equivalent model with the np.ndarray objects on GPU. (If already on GPU, returns self)
    cpu_model : mdp.Model
        An equivalent model with the np.ndarray objects on CPU. (If already on CPU, returns self)
    &#39;&#39;&#39;
    def __init__(self,
                 states:Union[int, list[str], list[list[str]]],
                 actions:Union[int, list],
                 transitions=None,
                 reachable_states=None,
                 rewards=None,
                 rewards_are_probabilistic:bool=False,
                 state_grid=None,
                 start_probabilities:Union[list,None]=None,
                 end_states:list[int]=[],
                 end_actions:list[int]=[]
                 ):
        
        # Empty variable
        self._alt_model = None
        self.is_on_gpu = False
        
        log(&#39;Instantiation of MDP Model:&#39;)
        
        # ------------------------- States -------------------------
        self.state_grid = None
        if isinstance(states, int): # State count
            self.state_labels = [f&#39;s_{i}&#39; for i in range(states)]

        elif isinstance(states, list) and all(isinstance(item, list) for item in states): # 2D list of states
            dim1 = len(states)
            dim2 = len(states[0])
            assert all(len(state_dim) == dim2 for state_dim in states), &#34;All sublists of states must be of equal size&#34;
            
            self.state_labels = []
            for state_dim in states:
                for state in state_dim:
                    self.state_labels.append(state)

            self.state_grid = np.arange(dim1 * dim2).reshape(dim1, dim2)

        else: # Default: single of list of string items
            self.state_labels = [item for item in states if isinstance(item, str)]

        self.state_count = len(self.state_labels)
        self.states = np.arange(self.state_count)

        log(f&#39;- {self.state_count} states&#39;)

        # ------------------------- Actions -------------------------
        if isinstance(actions, int):
            self.action_labels = [f&#39;a_{i}&#39; for i in range(actions)]
        else:
            self.action_labels = actions
        self.action_count = len(self.action_labels)
        self.actions = np.arange(self.action_count)

        log(f&#39;- {self.action_count} actions&#39;)

        # ------------------------- Reachable states provided -------------------------
        self.reachable_states = None
        if reachable_states is not None:
            self.reachable_states = np.array(reachable_states)
            assert self.reachable_states.shape[:2] == (self.state_count, self.action_count), f&#34;Reachable states provided is not of the expected shape (received {self.reachable_states.shape}, expected ({self.state_count}, {self.action_count}, :))&#34;
            self.reachable_state_count = self.reachable_states.shape[2]

            log(f&#39;- At most {self.reachable_state_count} reachable states per state-action pair&#39;)

        # ------------------------- Transitions -------------------------
        log(&#39;- Starting generation of transitions table&#39;)
        start_ts = datetime.now()

        self.transition_table = None
        self.transition_function = None
        if transitions is None:
            if reachable_states is None:
                # If no transitiong matrix and no reachable states given, generate random one
                print(&#39;[Warning] No transition matrix and no reachable states have provided so a random transition matrix is generated...&#39;)
                random_probs = np.random.rand(self.state_count, self.action_count, self.state_count)

                # Normalization to have s_p probabilies summing to 1
                self.transition_table = random_probs / np.sum(random_probs, axis=2, keepdims=True)
            else:
                # Make uniform transition probabilities over reachable states
                print(f&#39;[Warning] No transition matrix or function provided but reachable states are, so probability to reach any reachable states will &#34;1 / reachable state count&#34; so here: {1/self.reachable_state_count:.3f}.&#39;)

        elif callable(transitions): # Transition function
            self.transition_function = transitions
            # Attempt to create transition table in memory
            t_arr = None
            try:
                t_arr = np.fromfunction(self.transition_function, (self.state_count, self.action_count, self.state_count))
            except MemoryError:
                print(&#39;[Warning] Not enough memory to store transition table, using transition function provided...&#39;)
            else:
                self.transition_table = t_arr

        else: # Array like
            self.transition_table = np.array(transitions)
            t_shape = self.transition_table.shape
            exp_shape = (self.state_count, self.action_count, self.state_count)
            assert t_shape == exp_shape, f&#34;Transitions table provided doesnt have the right shape, it should be SxAxS (expected {exp_shape}, received {t_shape})&#34;

        duration = (datetime.now() - start_ts).total_seconds()
        log(f&#39;    &gt; Done in {duration:.3f}s&#39;)
        if duration &gt; 1:
            log(f&#39;    &gt; /!\\ Transition table generation took long, if not done already, try to use the reachable_states parameter to speedup the process.&#39;)

        # ------------------------- Rewards are probabilistic toggle -------------------------
        self.rewards_are_probabilistic = rewards_are_probabilistic

        # ------------------------- State grid -------------------------
        log(&#39;- Generation of state grid&#39;)
        if state_grid is None and self.state_grid is None:
            self.state_grid = np.arange(self.state_count).reshape((1,self.state_count))
        
        elif state_grid is not None:
            assert all(isinstance(l, list) for l in state_grid), &#34;The provided states grid must be a list of lists.&#34;

            grid_shape = (len(state_grid), len(state_grid[0]))
            assert all(len(l) == grid_shape[1] for l in state_grid), &#34;All rows must have the same length.&#34;

            if all(all(isinstance(e, int) for e in l) for l in state_grid):
                state_grid = np.array(state_grid)
                try:
                    self.states[state_grid]
                except:
                    raise Exception(&#39;An error occured with the list of state indices provided...&#39;)
                else:
                    self.state_grid = state_grid

            else:
                log(&#39;    &gt; Warning: looping through all grid states provided to find the corresponding states, can take a while...&#39;)
                
                np_state_grid = np.zeros(grid_shape, dtype=int)
                states_covered = 0
                for i, row in enumerate(state_grid):
                    for j, element in enumerate(state_grid):
                        if isinstance(element, str) and (element in self.state_labels):
                            states_covered += 1
                            np_state_grid[i,j] = self.state_labels.index(element)
                        elif isinstance(element, int) and (element &lt; self.state_count):
                            np_state_grid[i,j] = element
                        
                        else:
                            raise Exception(f&#39;Countains a state (\&#39;{state}\&#39;) not in the list of states...&#39;)

                assert states_covered == self.state_count, &#34;Some states of the state list are missing...&#34;

        # ------------------------- Start state probabilities -------------------------
        log(&#39;- Generating start probabilities table&#39;)
        if start_probabilities is not None:
            assert len(start_probabilities) == self.state_count
            self.start_probabilities = np.array(start_probabilities,dtype=float)
        else:
            self.start_probabilities = np.full((self.state_count), 1/self.state_count)

        # ------------------------- End state conditions -------------------------
        self.end_states = end_states
        self.end_actions = end_actions
        
        # ------------------------- Reachable states -------------------------
        # If not set yet
        if self.reachable_states is None:
            log(&#39;- Starting computation of reachable states from transition data&#39;)
            
            if self.state_count &gt; 1000:
                log(&#39;-    &gt; Warning: For models with large amounts of states, this operation can take time. Try generating it advance and use the parameter \&#39;reachable_states\&#39;...&#39;)
            
            start_ts = datetime.now()

            self.reachable_states = []
            self.reachable_state_count = 0
            for s in self.states:
                reachable_states_for_action = []
                for a in self.actions:
                    reachable_list = []
                    if self.transition_table is not None:
                        reachable_list = np.argwhere(self.transition_table[s,a,:] &gt; 0)[:,0].tolist()
                    else:
                        for sn in self.states:
                            if self.transition_function(s,a,sn) &gt; 0:
                                reachable_list.append(sn)
                    reachable_states_for_action.append(reachable_list)
                    
                    if len(reachable_list) &gt; self.reachable_state_count:
                        self.reachable_state_count = len(reachable_list)

                self.reachable_states.append(reachable_states_for_action)

            # In case some state-action pairs lead to more states than other, we fill with the 1st non states not used
            for s in self.states:
                for a in self.actions:
                    to_add = 0
                    while len(self.reachable_states[s][a]) &lt; self.reachable_state_count:
                        if to_add not in self.reachable_states[s][a]:
                            self.reachable_states[s][a].append(to_add)
                        to_add += 1

            # Converting to ndarray
            self.reachable_states = np.array(self.reachable_states, dtype=int)

            duration = (datetime.now() - start_ts).total_seconds()
            log(f&#39;    &gt; Done in {duration:.3f}s&#39;)
            log(f&#39;- At most {self.reachable_state_count} reachable states per state-action pair&#39;)

        # ------------------------- Reachable state probabilities -------------------------
        log(&#39;- Starting computation of reachable state probabilities from transition data&#39;)
        start_ts = datetime.now()

        if self.transition_function is None and self.transition_table is None:
            self.reachable_probabilities = np.full(self.reachable_states.shape, 1/self.reachable_state_count)
        elif self.transition_table is not None:
            self.reachable_probabilities = self.transition_table[self.states[:,None,None], self.actions[None,:,None], self.reachable_states]
        else:
            self.reachable_probabilities = np.fromfunction((lambda s,a,ri: self.transition_function(s.astype(int), a.astype(int), self.reachable_states[s.astype(int), a.astype(int), ri.astype(int)])), self.reachable_states.shape)
            
        duration = (datetime.now() - start_ts).total_seconds()
        log(f&#39;    &gt; Done in {duration:.3f}s&#39;)

        # ------------------------- Rewards -------------------------
        self.immediate_reward_table = None
        self.immediate_reward_function = None
        if rewards == -1: # If -1 is set, it means the rewards are defined in the superclass POMDP
            pass
        elif rewards is None:
            # If no reward matrix given, generate random one
            self.immediate_reward_table = np.random.rand(self.state_count, self.action_count, self.state_count)
        elif callable(rewards):
            # Rewards is a function
            self.immediate_reward_function = rewards
            assert len(signature(rewards).parameters) == 3, &#34;Reward function should accept 3 parameters: s, a, sn...&#34;
        else:
            # Array like
            self.immediate_reward_table = np.array(rewards)
            r_shape = self.immediate_reward_table.shape
            exp_shape = (self.state_count, self.action_count, self.state_count)
            assert r_shape == exp_shape, f&#34;Rewards table doesnt have the right shape, it should be SxAxS (expected: {exp_shape}, received {r_shape})&#34;

        # ------------------------- Expected rewards -------------------------
        self.expected_rewards_table = None
        if rewards != -1:
            log(&#39;- Starting generation of expected rewards table&#39;)
            start_ts = datetime.now()

            reachable_rewards = None
            if self.immediate_reward_table is not None:
                reachable_rewards = self.immediate_reward_table[self.states[:,None,None], self.actions[None,:,None], self.reachable_states]
            else:
                def reach_reward_func(s,a,ri):
                    s = s.astype(int)
                    a = a.astype(int)
                    ri = ri.astype(int)
                    return self.immediate_reward_function(s,a,self.reachable_states[s,a,ri])
                
                reachable_rewards = np.fromfunction(reach_reward_func, self.reachable_states.shape)
                
            self.expected_rewards_table = np.einsum(&#39;sar,sar-&gt;sa&#39;, self.reachable_probabilities, reachable_rewards)

            duration = (datetime.now() - start_ts).total_seconds()
            log(f&#39;    &gt; Done in {duration:.3f}s&#39;)

    
    def transition(self, s:int, a:int) -&gt; int:
        &#39;&#39;&#39;
        Returns a random posterior state knowing we take action a in state t and weighted on the transition probabilities.

        Parameters
        ----------
        s : int 
            The current state
        a : int
            The action to take

        Returns
        -------
        s_p : int
            The posterior state
        &#39;&#39;&#39;
        xp = cp if self.is_on_gpu else np
        s_p = int(xp.random.choice(a=self.reachable_states[s,a], size=1, p=self.reachable_probabilities[s,a])[0])
        return s_p
    

    def reward(self, s:int, a:int, s_p:int) -&gt; Union[int,float]:
        &#39;&#39;&#39;
        Returns the rewards of playing action a when in state s and landing in state s_p.
        If the rewards are probabilistic, it will return 0 or 1.

        Parameters
        ----------
        s : int
            The current state
        a : int
            The action taking in state s
        s_p : int
            The state landing in after taking action a in state s

        Returns
        -------
        reward : int or float
            The reward received.
        &#39;&#39;&#39;
        reward = self.immediate_reward_table[s,a,s_p] if self.immediate_reward_table is not None else self.immediate_reward_function(s,a,s_p)
        if self.rewards_are_probabilistic:
            rnd = random.random()
            return 1 if rnd &lt; reward else 0
        else:
            return reward
    

    def save(self, file_name:str, path:str=&#39;./Models&#39;) -&gt; None:
        &#39;&#39;&#39;
        Function to save the current model in a json file.
        By default, the model will be saved in &#39;Models&#39; directory in the current working directory but this can be changed using the &#39;path&#39; parameter.

        Parameters
        ----------
        file_name : str
            The name of the json file the model will be saved in.
        path : str, default=&#39;./Models&#39;
            The path at which the model will be saved.
        &#39;&#39;&#39;
        if not os.path.exists(path):
            print(&#39;Folder does not exist yet, creating it...&#39;)
            os.makedirs(path)

        if not file_name.endswith(&#39;.json&#39;):
            file_name += &#39;.json&#39;

        # Taking the arguments of the CPU version of the model
        argument_dict = self.cpu_model.__dict__

        # Converting the numpy array to lists
        for k,v in argument_dict.items():
            argument_dict.__setattr__(k, v.tolist() if isinstance(v, np.ndarray) else v)

        json_object = json.dumps(argument_dict, indent=4)
        with open(path + &#39;/&#39; + file_name, &#39;w&#39;) as outfile:
            outfile.write(json_object)


    @classmethod
    def load_from_json(cls, file:str) -&gt; Self:
        &#39;&#39;&#39;
        Function to load a MDP model from a json file. The json structure must contain the same items as in the constructor of this class.

        Parameters
        ----------
        file : str
            The file and path of the model to be loaded.
                
        Returns
        -------
        loaded_model : mdp.Model
            An instance of the loaded model.
        &#39;&#39;&#39;
        with open(file, &#39;r&#39;) as openfile:
            json_model = json.load(openfile)

        loaded_model = super().__new__(cls)
        for k,v in json_model.items():
            loaded_model.__setattr__(k, np.array(v) if isinstance(v, list) else v)

        if &#39;grid_states&#39; in json_model:
            loaded_model.convert_to_grid(json_model[&#39;grid_states&#39;])

        return loaded_model


    @property
    def gpu_model(self) -&gt; Self:
        &#39;&#39;&#39;
        The same model but on the GPU instead of the CPU. If already on the GPU, the current model object is returned.
        &#39;&#39;&#39;
        if self.is_on_gpu:
            return self
        
        assert gpu_support, &#34;GPU Support is not available, try installing cupy...&#34;
        
        if self._alt_model is None:
            log(&#39;Sending Model to GPU...&#39;)
            start = datetime.now()

            # Setting all the arguments of the new class and convert to cupy if numpy array
            new_model = super().__new__(self.__class__)
            for arg, val in self.__dict__.items():
                new_model.__setattr__(arg, cp.array(val) if isinstance(val, np.ndarray) else val)

            # GPU/CPU variables
            new_model.is_on_gpu = True
            new_model._alt_model = self
            self._alt_model = new_model
            
            duration = (datetime.now() - start).total_seconds()
            log(f&#39;    &gt; Done in {duration:.3f}s&#39;)

        return self._alt_model


    @property
    def cpu_model(self) -&gt; Self:
        &#39;&#39;&#39;
        The same model but on the CPU instead of the GPU. If already on the CPU, the current model object is returned.
        &#39;&#39;&#39;
        if not self.is_on_gpu:
            return self
        
        assert gpu_support, &#34;GPU Support is not available, try installing cupy...&#34;

        if self._alt_model is None:
            log(&#39;Sending Model to CPU...&#39;)
            start = datetime.now()

            # Setting all the arguments of the new class and convert to numpy if cupy array
            new_model = super().__new__(self.__class__)
            for arg, val in self.__dict__.items():
                new_model.__setattr__(arg, cp.asnumpy(val) if isinstance(val, cp.ndarray) else val)
            
            # GPU/CPU variables
            new_model.is_on_gpu = False
            new_model._alt_model = self
            self._alt_model = new_model
            
            duration = (datetime.now() - start).total_seconds()
            log(f&#39;    &gt; Done in {duration:.3f}s&#39;)

        return self._alt_model


class AlphaVector:
    &#39;&#39;&#39;
    A class to represent an Alpha Vector, a vector representing a plane in |S| dimension for POMDP models.

    ...

    Parameters
    ----------
    values : np.ndarray
        The actual vector with the value for each state.
    action : int
        The action associated with the vector.
    &#39;&#39;&#39;
    def __init__(self, values:np.ndarray, action:int) -&gt; None:
        self.values = values
        self.action = action


class ValueFunction:
    &#39;&#39;&#39;
    Class representing a set of AlphaVectors. One such set approximates the value function of the MDP model.

    ...

    Parameters
    ----------
    model : mdp.Model
        The model the value function is associated with.
    alpha_vectors : list[AlphaVector] or np.ndarray, optional
        The alpha vectors composing the value function, if none are provided, it will be empty to start with and AlphaVectors can be appended.
    action_list : list[int], optional
        The actions associated with alpha vectors in the case the alpha vectors are provided as an numpy array.
    
    Attributes
    ----------
    model : mdp.Model
        The model the value function is associated with.
    alpha_vector_list : list[AlphaVector]
    alpha_vector_array : np.ndarray
    actions : list[int]
    &#39;&#39;&#39;
    def __init__(self, model:Model, alpha_vectors:Union[list[AlphaVector], np.ndarray]=[], action_list:list[int]=[]):
        self.model = model

        self._vector_list = None
        self._vector_array = None
        self._actions = None

        self.is_on_gpu = False

        # List of alpha vectors
        if isinstance(alpha_vectors, list):
            assert all(v.values.shape[0] == model.state_count for v in alpha_vectors), f&#34;Some or all alpha vectors in the list provided dont have the right size, they should be of shape: {model.state_count}&#34;
            self._vector_list = alpha_vectors
            
            # Check if on gpu and make sure all vectors are also on the gpu
            if (len(alpha_vectors) &gt; 0) and gpu_support and cp.get_array_module(alpha_vectors[0].values) == cp:
                assert all(cp.get_array_module(v.values) == cp for v in alpha_vectors), &#34;Either all or none of the alpha vectors should be on the GPU, not just some.&#34;
                self.is_on_gpu = True
        
        # As numpy array
        else:
            av_shape = alpha_vectors.shape
            exp_shape = (len(action_list), model.state_count)
            assert av_shape == exp_shape, f&#34;Alpha vector array does not have the right shape (received: {av_shape}; expected: {exp_shape})&#34;

            self._vector_array = alpha_vectors
            self._actions = action_list

            # Check if array is on gpu
            if gpu_support and cp.get_array_module(alpha_vectors) == cp:
                self.is_on_gpu = True


    @property
    def alpha_vector_list(self) -&gt; list[AlphaVector]:
        &#39;&#39;&#39;
        A list of AlphaVector objects. If the value function is defined as an matrix of vectors and a list of actions, the list of AlphaVectors will be generated from them.
        &#39;&#39;&#39;
        if self._vector_list is None:
            self._vector_list = []
            for alpha_vect, action in zip(self._vector_array, self._actions):
                self._vector_list.append(AlphaVector(alpha_vect, action))
        return self._vector_list
    

    @property
    def alpha_vector_array(self) -&gt; np.ndarray:
        &#39;&#39;&#39;
        A matrix of size N x S, containing all the alpha vectors making up the value function. (N is the number of alpha vectors and S the amount of states in the model)
        If the value function is defined as a list of AlphaVector objects, the matrix will the generated from them.
        &#39;&#39;&#39;
        xp = cp if (gpu_support and self.is_on_gpu) else np

        if self._vector_array is None:
            self._vector_array = xp.array([v.values for v in self._vector_list])
            self._actions = [v.action for v in self._vector_list]
        return self._vector_array
    

    @property
    def actions(self) -&gt; list[int]:
        &#39;&#39;&#39;
        A list of N actions corresponding to the N alpha vectors making up the value function.
        If the value function is defined as a list of AlphaVector objects, the list will the generated from the actions of those alpha vector objects.
        &#39;&#39;&#39;
        xp = cp if (gpu_support and self.is_on_gpu) else np

        if self._actions is None:
            self._vector_array = xp.array(self._vector_list)
            self._actions = [v.action for v in self._vector_list]
        return self._actions
    

    def __len__(self) -&gt; int:
        return len(self._vector_list) if self._vector_list is not None else self._vector_array.shape[0]
    

    def append(self, alpha_vector:AlphaVector) -&gt; None:
        &#39;&#39;&#39;
        Function to add an alpha vector to the value function.

        Parameters
        ----------
        alpha_vector : AlphaVector
            The alpha vector to be added to the value function.
        &#39;&#39;&#39;
        # Make sure size is correct
        assert alpha_vector.values.shape[0] == self.model.state_count, f&#34;Vector to add to value function doesn&#39;t have the right size (received: {alpha_vector.values.shape[0]}, expected: {self.model.state_count})&#34;
        
        # GPU support check
        xp = cp if (gpu_support and self.is_on_gpu) else np
        assert gpu_support and cp.get_array_module(alpha_vector.values) == xp, f&#34;Vector is{&#39; not&#39; if self.is_on_gpu else &#39;&#39;} on GPU while value function is{&#39;&#39; if self.is_on_gpu else &#39; not&#39;}.&#34;

        if self._vector_array is not None:
            self._vector_array = xp.append(self._vector_array, alpha_vector[None,:], axis=0)
            self._actions.append(alpha_vector.action)
        
        if self._vector_list is not None:
            self._vector_list.append(alpha_vector)


    def to_gpu(self) -&gt; Self:
        &#39;&#39;&#39;
        Function returning an equivalent value function object with the arrays stored on GPU instead of CPU.

        Returns
        -------
        gpu_value_function : ValueFunction
            A new value function with arrays on GPU.
        &#39;&#39;&#39;
        assert gpu_support, &#34;GPU support is not enabled, unable to execute this function&#34;

        gpu_model = self.model.gpu_model

        gpu_value_function = None
        if self._vector_array is not None:
            gpu_vector_array = cp.array(self._vector_array)
            gpu_actions = self._actions if isinstance(self._actions, list) else cp.array(self._actions)
            gpu_value_function = ValueFunction(gpu_model, gpu_vector_array, gpu_actions)
        
        else:
            gpu_alpha_vectors = [AlphaVector(cp.array(av.values), av.action) for av in self._vector_list]
            gpu_value_function = ValueFunction(gpu_model, gpu_alpha_vectors)

        return gpu_value_function
    

    def to_cpu(self) -&gt; Self:
        &#39;&#39;&#39;
        Function returning an equivalent value function object with the arrays stored on CPU instead of GPU.

        Returns
        -------
        cpu_value_function : ValueFunction
            A new value function with arrays on CPU.
        &#39;&#39;&#39;
        assert gpu_support, &#34;GPU support is not enabled, unable to execute this function&#34;

        cpu_model = self.model.cpu_model

        cpu_value_function = None
        if self._vector_array is not None:
            cpu_vector_array = cp.asnumpy(self._vector_array)
            cpu_actions = self._actions if isinstance(self._actions, list) else cp.asnumpy(self._actions)
            cpu_value_function = ValueFunction(cpu_model, cpu_vector_array, cpu_actions)
        
        else:
            cpu_alpha_vectors = [AlphaVector(cp.asnumpy(av.values), av.action) for av in self._vector_list]
            cpu_value_function = ValueFunction(cpu_model, cpu_alpha_vectors)

        return cpu_value_function


    def prune(self, level:int=1) -&gt; Self:
        &#39;&#39;&#39;
        Function returning a new value function with the set of alpha vector composing it being it pruned.
        The pruning is as thorough as the level:
            - 0: No pruning, returns a value function with the alpha vector set being an exact copy of the current one.
            - 1: Simple deduplication of the alpha vectors.
            - 2: 1+ Check of absolute domination (check if dominated at each state).
            - 3: 2+ Solves Linear Programming problem for each alpha vector to see if it is dominated by combinations of other vectors.
        
        Note that the higher the level, the heavier the time impact will be.

        Parameters
        ----------
        level : int, default=1
            Between 0 and 3, how thorough the alpha vector pruning should be.
            
        Returns
        -------
        new_value_function : ValueFunction
            A new value function with a pruned set of alpha vectors.
        &#39;&#39;&#39;
        # GPU support check
        xp = cp if (gpu_support and self.is_on_gpu) else np

        if level &lt; 1:
            return ValueFunction(self.model, xp.copy(self))
        
        # Level 1 pruning: Check for duplicates - works equally for cupy array (on gpu)
        L = {alpha_vector.values.tobytes(): alpha_vector for alpha_vector in self.alpha_vector_list}
        pruned_alpha_set = ValueFunction(self.model, list(L.values()))

        # Level 2 pruning: Check for absolute domination
        if level &gt;= 2:
            # Beyond this point, gpu can&#39;t be used due to the functions used so if on gpu, converting it back to cpu
            if pruned_alpha_set.is_on_gpu:
                pruned_alpha_set = pruned_alpha_set.to_cpu()

            alpha_vector_array = pruned_alpha_set.alpha_vector_array
            X = cdist(alpha_vector_array, alpha_vector_array, metric=(lambda a,b:(a &lt;= b).all() and not (a == b).all())).astype(bool)
            non_dominated_vector_indices = np.invert(X).all(axis=1)

            non_dominated_vectors = alpha_vector_array[non_dominated_vector_indices]
            non_dominated_actions = np.array(pruned_alpha_set.actions)[non_dominated_vector_indices].tolist()

            pruned_alpha_set = ValueFunction(self.model, non_dominated_vectors, non_dominated_actions)

        # Level 3 pruning: LP to check for more complex domination
        if level &gt;= 3:
            alpha_set = pruned_alpha_set.alpha_vector_list
            pruned_alpha_set = ValueFunction(self.model)

            for i, alpha_vect in enumerate(alpha_set):
                other_alphas = alpha_set[:i] + alpha_set[(i+1):]

                # Objective function
                c = np.concatenate([np.array([1]), -1*alpha_vect])

                # Alpha vector contraints
                other_count = len(other_alphas)
                A = np.c_[np.ones(other_count), np.multiply(np.array(other_alphas), -1)]
                alpha_constraints = LinearConstraint(A, 0, np.inf)

                # Constraints that sum of beliefs is 1
                belief_constraint = LinearConstraint(np.array([0] + ([1]*self.model.state_count)), 1, 1)

                # Solve problem
                res = milp(c=c, constraints=[alpha_constraints, belief_constraint])

                # Check if dominated
                is_dominated = (res.x[0] - np.dot(res.x[1:], alpha_vect)) &gt;= 0
                if is_dominated:
                    print(alpha_vect)
                    print(&#39; -&gt; Dominated\n&#39;)
                else:
                    pruned_alpha_set.append(alpha_vect)
        
        # If initial value function was on gpu, and intermediate array was converted to cpu, convert it back to gpu
        if self.is_on_gpu and not pruned_alpha_set.is_on_gpu:
            pruned_alpha_set = pruned_alpha_set.to_cpu()

        return pruned_alpha_set
    

    def save(self, path:str=&#39;./ValueFunctions&#39;, file_name:Union[str,None]=None) -&gt; None:
        &#39;&#39;&#39;
        Function to save the save function in a file at a given path. If no path is provided, it will be saved in a subfolder (ValueFunctions) inside the current working directory.
        If no file_name is provided, it be saved as &#39;&lt;current_timestamp&gt;_value_function.csv&#39;.

        Parameters
        ----------
        path : str, default=&#39;./ValueFunctions&#39;
            The path at which the csv will be saved.
        file_name : str, default=&#39;&lt;current_timestamp&gt;_value_function.csv&#39;
            The file name used to save in.
        &#39;&#39;&#39;
        if not os.path.exists(path):
            print(&#39;Folder does not exist yet, creating it...&#39;)
            os.makedirs(path)
            
        if file_name is None:
            timestamp = datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)
            file_name = timestamp + &#39;_value_function.csv&#39;

        vector_array = self.alpha_vector_array
        actions = self.actions

        # Convert arrays to numpy if on gpu
        if self.is_on_gpu:
            vector_array = cp.asnumpy(vector_array)
            actions = cp.asnumpy(actions)

        data = np.concatenate((np.array(self.actions)[:,None], self.alpha_vector_array), axis=1)
        columns = [&#39;action&#39;, *self.model.state_labels]

        df = pd.DataFrame(data)
        df.to_csv(path + &#39;/&#39; + file_name, index=False, header=columns)


    @classmethod
    def load_from_file(cls, file:str, model:Model) -&gt; Self:
        &#39;&#39;&#39;
        Function to load the value function from a csv file.

        Parameters
        ----------
        file : str
            The path and file_name of the value function to be loaded.
        model : mdp.Model
            The model the value function is linked to.
            
        Returns
        -------
        loaded_value_function : ValueFunction
            The loaded value function.
        &#39;&#39;&#39;
        df = pd.read_csv(file, header=0, index_col=False)
        alpha_vectors = df.to_numpy()

        return ValueFunction(model, alpha_vectors[:,1:], alpha_vectors[:,0].astype(int))


    def plot(self,
             as_grid:bool=False,
             size:int=5,
             belief_set:np.ndarray=None
             ) -&gt; None:
        &#39;&#39;&#39;
        Function to plot out the value function in 2 or 3 dimensions if possible and the as_grid parameter is kept to false. Else, the value function is plot as a grid.
        If a belief set array is provided and the model is a 2- or 3-model, it will be plot alongside the value function.

        Parameters
        ----------
        as_grid : bool, default=False
            Forces the plot to be plot as a grid.
        size : int, default=5
            The actual plot scale.
        belief_set : np.ndarray, optional
            A set of belief to plot the belief points that were explored.
        &#39;&#39;&#39;
        assert len(self) &gt; 0, &#34;Value function is empty, plotting is impossible...&#34;
        
        # If on GPU, convert to CPU and plot that one
        if self.is_on_gpu:
            print(&#39;[Warning] Value function on GPU, converting to numpy before plotting...&#39;)
            cpu_value_function = self.to_cpu()
            cpu_value_function.plot(as_grid, size, belief_set)
            return

        func = None
        if as_grid:
            func = self._plot_grid
        elif self.model.state_count == 2:
            func = self._plot_2D
        elif self.model.state_count == 3:
            func = self._plot_3D
        else:
            print(&#39;[Warning] \&#39;as_grid\&#39; parameter set to False but state count is &gt;3 so it will be plotted as a grid&#39;)
            func = self._plot_grid

        func(size, belief_set)


    def _plot_2D(self, size, belief_set=None):
        x = np.linspace(0, 1, 100)

        plt.figure(figsize=(int(size*1.5),size))
        grid_spec = {&#39;height_ratios&#39;: ([1] if belief_set is None else [19,1])}
        _, ax = plt.subplots((2 if belief_set is not None else 1),1,sharex=True,gridspec_kw=grid_spec)

        # Vector plotting
        alpha_vects = self.alpha_vector_array

        m = alpha_vects[:,1] - alpha_vects[:,0] # type: ignore
        m = m.reshape(m.shape[0],1)

        x = x.reshape((1,x.shape[0])).repeat(m.shape[0],axis=0)
        y = (m*x) + alpha_vects[:,0].reshape(m.shape[0],1)

        ax1 = ax[0] if belief_set is not None else ax
        for i, alpha in enumerate(self.alpha_vector_list):
            ax1.plot(x[i,:], y[i,:], color=COLOR_LIST[alpha.action][&#39;id&#39;]) # type: ignore

        # X-axis setting
        ticks = [0,0.25,0.5,0.75,1]
        x_ticks = [str(t) for t in ticks]
        x_ticks[0] = self.model.state_labels[0]
        x_ticks[-1] = self.model.state_labels[1]

        ax1.set_xticks(ticks, x_ticks) # type: ignore

        # Action legend
        proxy = [patches.Rectangle((0,0),1,1,fc = COLOR_LIST[a][&#39;id&#39;]) for a in self.model.actions]
        ax1.legend(proxy, self.model.action_labels) # type: ignore

        # Belief plotting
        if belief_set is not None:
            beliefs_x = belief_set.belief_array[:,1]
            ax[1].scatter(beliefs_x, np.zeros(beliefs_x.shape[0]), c=&#39;red&#39;)
            ax[1].get_yaxis().set_visible(False)
            ax[1].axhline(0, color=&#39;black&#39;)


    def _plot_3D(self, size, belief_set=None):

        def get_alpha_vect_z(xx, yy, alpha_vect):
            x0, y0, z0 = [0, 0, alpha_vect[0]]
            x1, y1, z1 = [1, 0, alpha_vect[1]]
            x2, y2, z2 = [0, 1, alpha_vect[2]]

            ux, uy, uz = u = [x1-x0, y1-y0, z1-z0]
            vx, vy, vz = v = [x2-x0, y2-y0, z2-z0]

            u_cross_v = [uy*vz-uz*vy, uz*vx-ux*vz, ux*vy-uy*vx]

            point  = np.array([0, 0, alpha_vect[0]])
            normal = np.array(u_cross_v)

            d = -point.dot(normal)

            z = (-normal[0] * xx - normal[1] * yy - d) * 1. / normal[2]
            
            return z

        def get_plane_gradient(alpha_vect):
        
            x0, y0, z0 = [0, 0, alpha_vect[0]]
            x1, y1, z1 = [1, 0, alpha_vect[1]]
            x2, y2, z2 = [0, 1, alpha_vect[2]]

            ux, uy, uz = u = [x1-x0, y1-y0, z1-z0]
            vx, vy, vz = v = [x2-x0, y2-y0, z2-z0]

            u_cross_v = [uy*vz-uz*vy, uz*vx-ux*vz, ux*vy-uy*vx]
            
            normal_vector = np.array(u_cross_v)
            normal_vector_norm = float(np.linalg.norm(normal_vector))
            normal_vector = np.divide(normal_vector, normal_vector_norm)
            normal_vector[2] = 0
            
            return np.linalg.norm(normal_vector)

        # Actual plotting
        x = np.linspace(0, 1, 1000)
        y = np.linspace(0, 1, 1000)

        xx, yy = np.meshgrid(x, y)

        max_z = np.zeros((xx.shape[0], yy.shape[0]))
        best_a = (np.zeros((xx.shape[0], yy.shape[0])))
        plane = (np.zeros((xx.shape[0], yy.shape[0])))
        gradients = (np.zeros((xx.shape[0], yy.shape[0])))

        for alpha in self.alpha_vector_list:

            z = get_alpha_vect_z(xx, yy, alpha)

            # Action array update
            new_a_mask = np.argmax(np.array([max_z, z]), axis=0)

            best_a[new_a_mask == 1] = alpha.action
            
            plane[new_a_mask == 1] = random.randrange(100)
            
            alpha_gradient = get_plane_gradient(alpha)
            gradients[new_a_mask == 1] = alpha_gradient

            # Max z update
            max_z = np.max(np.array([max_z, z]), axis=0)
            
        for x_i, x_val in enumerate(x):
            for y_i, y_val in enumerate(y):
                if (x_val+y_val) &gt; 1:
                    max_z[x_i, y_i] = np.nan
                    plane[x_i, y_i] = np.nan
                    gradients[x_i, y_i] = np.nan
                    best_a[x_i, y_i] = np.nan

        belief_points = None
        if belief_set is not None:
            belief_points = np.array(belief_set)[:,1:]
                    
        fig, ((ax1, ax2),(ax3,ax4)) = plt.subplots(2, 2, figsize=(size*4,size*3.5), sharex=True, sharey=True)

        # Set ticks
        ticks = [0,0.25,0.5,0.75,1]
        x_ticks = [str(t) for t in ticks]
        x_ticks[0] = self.model.state_labels[0]
        x_ticks[-1] = self.model.state_labels[1]
        
        y_ticks = [str(t) for t in ticks]
        y_ticks[0] = self.model.state_labels[0]
        y_ticks[-1] = self.model.state_labels[2]

        plt.setp([ax1,ax2,ax3,ax4], xticks=ticks, xticklabels=x_ticks, yticks=ticks, yticklabels=y_ticks)

        # Value function ax
        ax1.set_title(&#34;Value function&#34;)
        ax1_plot = ax1.contourf(x, y, max_z, 100, cmap=&#34;viridis&#34;)
        plt.colorbar(ax1_plot, ax=ax1)

        # Alpha planes ax
        ax2.set_title(&#34;Alpha planes&#34;)
        ax2_plot = ax2.contourf(x, y, plane, 100, cmap=&#34;viridis&#34;)
        plt.colorbar(ax2_plot, ax=ax2)
        
        # Gradient of planes ax
        ax3.set_title(&#34;Gradients of planes&#34;)
        ax3_plot = ax3.contourf(x, y, gradients, 100, cmap=&#34;Blues&#34;)
        plt.colorbar(ax3_plot, ax=ax3)

        # Action policy ax
        ax4.set_title(&#34;Action policy&#34;)
        ax4.contourf(x, y, best_a, 1, colors=[c[&#39;id&#39;] for c in COLOR_LIST])
        proxy = [patches.Rectangle((0,0),1,1,fc = COLOR_LIST[int(a)][&#39;id&#39;]) for a in self.model.actions]
        ax4.legend(proxy, self.model.action_labels)

        if belief_points is not None:
            for ax in [ax1,ax2,ax3,ax4]:
                ax.scatter(belief_points[:,0], belief_points[:,1], s=1, c=&#39;black&#39;)


    def _plot_grid(self, size=5, belief_set=None):
        value_table = np.max(self.alpha_vector_array, axis=0)[self.model.state_grid]
        best_action_table = np.array(self.actions)[np.argmax(self.alpha_vector_array, axis=0)][self.model.state_grid]
        best_action_colors = COLOR_ARRAY[best_action_table]

        dimensions = self.model.state_grid.shape

        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(size*2, size), width_ratios=(0.55,0.45))

        # Ticks
        x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))
        y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))

        ax1.set_title(&#39;Value function&#39;)
        ax1_plot = ax1.imshow(value_table)
        plt.colorbar(ax1_plot, ax=ax1)
        ax1.set_xticks(x_ticks)
        ax1.set_yticks(y_ticks)

        ax2.set_title(&#39;Action policy&#39;)
        ax2.imshow(best_action_colors)
        p = [ patches.Patch(color=COLOR_LIST[int(i)][&#39;id&#39;], label=str(self.model.action_labels[int(i)])) for i in self.model.actions]
        ax2.legend(handles=p, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
        ax2.set_xticks(x_ticks)
        ax2.set_yticks(y_ticks)


class SolverHistory:
    &#39;&#39;&#39;
    Class to represent the solving history of a solver.
    The purpose of this class is to allow plotting of the solution and plotting the evolution of the value function over the training process.
    This class is not meant to be instanciated manually, it meant to be used when returned by the solve() method of a Solver object.

    ...

    Parameters
    ----------
    tracking_level : int
        The tracking level of the solver.
    model : mdp.Model
        The model that has been solved by the Solver.
    gamma : float
        The gamma parameter used by the solver (learning rate).
    eps : float
        The epsilon parameter used by the solver (covergence bound).
    initial_value_function : ValueFunction, optional
        The initial value function the solver will use to start the solving process.
    
    Attributes
    ----------
    tracking_level : int
    model : mdp.Model
    gamma : float
    eps : float
    run_ts : datetime
        The time at which the SolverHistory object was instantiated which is assumed to be the start of the solving run.
    iteration_times : list[float]
        A list of recorded iteration times.
    value_function_changes : list[float]
        A list of recorded value function changes (the maximum changed value between 2 value functions).
    value_functions : list[ValueFunction]
        A list of recorded value functions.
    solution : ValueFunction
    summary : str
    &#39;&#39;&#39;
    def __init__(self,
                 tracking_level:int,
                 model:Model,
                 gamma:float,
                 eps:float,
                 initial_value_function:Union[ValueFunction,None]=None
                 ):
        self.tracking_level = tracking_level
        self.model = model
        self.gamma = gamma
        self.eps = eps
        self.run_ts = datetime.now()

        # Tracking metrics
        self.iteration_times = []
        self.value_function_changes = []

        self.value_functions = []
        if self.tracking_level &gt;= 2:
            self.value_functions.append(initial_value_function)


    @property
    def solution(self) -&gt; ValueFunction:
        &#39;&#39;&#39;
        The last value function of the solving process.
        &#39;&#39;&#39;
        assert self.tracking_level &gt;= 2, &#34;Tracking level is set too low, increase it to 2 if you want to have value function tracking as well.&#34;
        return self.value_functions[-1]
    

    def add(self,
            iteration_time:float,
            value_function_change:float,
            value_function:ValueFunction
            ) -&gt; None:
        &#39;&#39;&#39;
        Function to add a step in the simulation history.

        Parameters
        ----------
        iteration_time : float
            The time it took to run the iteration.
        value_function_change : float
            The change between the value function of this iteration and of the previous iteration.
        value_function : ValueFunction
            The value function resulting after a step of the solving process.
        &#39;&#39;&#39;
        if self.tracking_level &gt;= 1:
            self.iteration_times.append(float(iteration_time))
            self.value_function_changes.append(float(value_function_change))

        if self.tracking_level &gt;= 2:
            self.value_functions.append(value_function if not value_function.is_on_gpu else value_function.to_cpu())
    

    @property
    def summary(self) -&gt; str:
        &#39;&#39;&#39;
        A summary as a string of the information recorded.
        &#39;&#39;&#39;
        summary_str =  f&#39;Summary of Value Iteration run&#39;
        summary_str += f&#39;\n  - Model: {self.model.state_count}-state, {self.model.action_count}-action&#39;
        summary_str += f&#39;\n  - Converged in {len(self.iteration_times)} iterations and {sum(self.iteration_times):.4f} seconds&#39;
        
        if self.tracking_level &gt;= 1:
            summary_str += f&#39;\n  - Took on average {sum(self.iteration_times) / len(self.iteration_times):.4f}s per iteration&#39;
        
        return summary_str
    

    def plot_changes(self) -&gt; None:
        &#39;&#39;&#39;
        Function to plot the value function changes over the solving process.
        &#39;&#39;&#39;
        assert self.tracking_level &gt;= 1, &#34;To plot the change of the value function over time, use tracking level 1 or higher.&#34;
        plt.plot(np.arange(len(self.value_function_changes)), self.value_function_changes)
        plt.show()


class Solver:
    &#39;&#39;&#39;
    MDP Model Solver - Abstract class.
    &#39;&#39;&#39;
    def __init__(self) -&gt; None:
        raise Exception(&#34;Not an implementable class, please use a subclass...&#34;)
    
    def solve(self, model: Model) -&gt; tuple[ValueFunction, SolverHistory]:
        raise Exception(&#34;Method has to be implemented by subclass...&#34;)


class VI_Solver(Solver):
    &#39;&#39;&#39;
    Solver for MDP Models. This solver implements Value Iteration.
    It works by iteratively updating the value function that maps states to actions.
    
    ...

    Parameters
    ----------
    horizon : int, default=1000
        Controls for how many epochs the learning can run for (works as an infinite loop safety).
    gamma : float, default=0.99
        Controls the learning rate, how fast the rewards are discounted at each epoch.
    eps : float, default=0.001
        Controls the threshold to determine whether the value functions has settled. If the max change of value for a state is lower than eps, then it has converged.
        
    Attributes
    ----------
    horizon : int
    gamma : float
    eps : float
    &#39;&#39;&#39;
    def __init__(self, horizon:int=10000, gamma:float=0.99, eps:float=0.001):
        self.horizon = horizon
        self.gamma = gamma
        self.eps = eps


    def solve(self, 
              model: Model,
              initial_value_function:Union[ValueFunction,None]=None,
              use_gpu:bool=False,
              history_tracking_level:int=1,
              print_progress:bool=True
              ) -&gt; tuple[ValueFunction, SolverHistory]:
        &#39;&#39;&#39;
        Function to solve an MDP model using Value Iteration.
        If an initial value function is not provided, the value function will be initiated with the expected rewards.

        Parameters
        ----------
        model : mdp.Model
            The model on which to run value iteration.
        initial_value_function : ValueFunction, optional
            An optional initial value function to kick-start the value iteration process.
        use_gpu : bool, default=False
            Whether to use the GPU with cupy array to accelerate solving.
        history_tracking_level : int, default=1
            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)
        print_progress : bool, default=True
            Whether or not to print out the progress of the value iteration process.

        Returns
        -------
        value_function: ValueFunction
            The resulting value function solution to the model.
        history : SolverHistory
            The tracking of the solution over time.
        &#39;&#39;&#39;
        # numpy or cupy module
        xp = np

        # If GPU usage
        if use_gpu:
            assert gpu_support, &#34;GPU support is not enabled, Cupy might need to be installed...&#34;
            model = model.gpu_model

            # Replace numpy module by cupy for computations
            xp = cp

        # Value function initialization
        if initial_value_function is None:
            V = ValueFunction(model, model.expected_rewards_table.T, model.actions)
        else:
            V = initial_value_function.to_gpu() if use_gpu else initial_value_function
        V_opt = xp.max(V.alpha_vector_array, axis=0)

        # History tracking setup
        solve_history = SolverHistory(tracking_level=history_tracking_level,
                                      model=model,
                                      gamma=self.gamma,
                                      eps=self.eps,
                                      initial_value_function=V)

        # Computing max allowed change from epsilon and gamma parameters
        max_allowed_change = self.eps * (self.gamma / (1-self.gamma))

        for _ in trange(self.horizon) if print_progress else range(self.horizon):
            old_V_opt = V_opt
            
            start = datetime.now()

            # Computing the new alpha vectors
            alpha_vectors = model.expected_rewards_table.T + (self.gamma * xp.einsum(&#39;sar,sar-&gt;as&#39;, model.reachable_probabilities, V_opt[model.reachable_states]))
            V = ValueFunction(model, alpha_vectors, model.actions)

            V_opt = xp.max(V.alpha_vector_array, axis=0)
            
            # Change computation
            max_change = xp.max(xp.abs(V_opt - old_V_opt))

            # Tracking the history
            iteration_time = (datetime.now() - start).total_seconds()
            solve_history.add(iteration_time=iteration_time,
                              value_function_change=max_change,
                              value_function=V)

            # Convergence check
            if max_change &lt; max_allowed_change:
                break

        return V, solve_history


class RewardSet(list):
    &#39;&#39;&#39;
    Class to represent a list of rewards with some functionality to plot them. Plotting options:
        - Totals: to plot a graph of the accumulated rewards over time.
        - Moving average: to plot the moving average of the rewards received over time.
        - Histogram: to plot a histogram of the various rewards received.

    ...

    Parameters
    ----------
    items : list, default=[]
        The rewards in the set.
    &#39;&#39;&#39;
    def __init__(self, items:list=[]):
        self.extend(items)


    def plot(self,
             type:str=&#39;total&#39;,
             size:int=5,
             max_reward=None,
             compare_with:Union[Self, list[Self]]=[],
             graph_names:list[str]=[]
             ) -&gt; None:
        &#39;&#39;&#39;
        The method to plot summaries of the rewards received over time.
        The plots available:
            - Total (&#39;total&#39; or &#39;t&#39;): to plot the total reward as a cummulative sum over time.
            - Moving average (&#39;moving_average&#39; or &#39;ma&#39;): to plot the moving average of the rewards
            - Hisotgram (&#39;histogram&#39; or &#39;h&#39;): to plot the various reward in bins to plot a histogram of what was received

        Parameters
        ----------
        type : str, default=&#39;total&#39;
            The type of plot to generate.
        size : int, default=5
            The plot scale.
        max_reward : optional
            An upper bound to rewards that can be received at each timestep.
        compare_with : RewardSet or list[RewardSet], default=[]
            One or more RewardSets to plot onlonside this one for comparison.
        graph_names : list[str], default=[]
            A list of the names of the comparison graphs.
        &#39;&#39;&#39;
        plt.figure(figsize=(size*2,size))

        # Histories
        reward_sets = [self]
        if isinstance(compare_with, RewardSet):
            reward_sets.append(compare_with)
        else:
            reward_sets += compare_with
        
        assert len(reward_sets) &lt; len(COLOR_LIST), &#34;Not enough colors to plot all the comparisson graphs&#34;

        # Names
        names = []
        if len(graph_names) == 0:
            names.append(&#39;Main graph&#39;)
            for i in range(1, len(reward_sets)):
                names.append(f&#39;Comparisson {i}&#39;)
        else:
            assert len(graph_names) == len(reward_sets), &#34;Names for the graphs are provided but not enough&#34;
            names = copy.deepcopy(graph_names)

        # Actual plot
        if type in [&#39;total&#39;, &#39;t&#39;]:
            plt.title(&#39;Cummulative reward received of time&#39;)
            self._plot_total(reward_sets, names, max_reward)

        elif type in [&#39;moving_average&#39;, &#39;ma&#39;]:
            plt.title(&#39;Average rewards received of time&#39;)
            self._plot_moving_average(reward_sets, names, max_reward)

        elif type in [&#39;histogram&#39;, &#39;h&#39;]:
            plt.title(&#39;Histogram of rewards received&#39;)
            self._plot_histogram(reward_sets, names, max_reward)

        # Finalization
        plt.legend(loc=&#39;upper left&#39;)
        plt.show()


    def _plot_total(self, reward_sets, names, max_reward=None):
        x = np.arange(len(reward_sets[0]))

        # If given plot upper bound
        if max_reward is not None:
            y_best = max_reward * x
            plt.plot(x, y_best, color=&#39;red&#39;, linestyle=&#39;--&#39;, label=&#39;Max rewards&#39;)

        # Plot rewards
        for i, (rh, name) in enumerate(zip(reward_sets, names)):
            cum_rewards = np.cumsum([r for r in rh])
            plt.plot(x, cum_rewards, label=name, c=COLOR_LIST[i][&#39;id&#39;])
    

    def _plot_moving_average(self, reward_sets, names, max_reward=None):
        x = np.arange(len(reward_sets[0]))

        # If given plot upper bound
        if max_reward is not None:
            y_best = np.ones(len(reward_sets[0])) * max_reward
            plt.plot(x, y_best, color=&#39;red&#39;, linestyle=&#39;--&#39;, label=&#39;Max rewards&#39;)

        # Plot rewards
        for i, (rh, name) in enumerate(zip(reward_sets, names)):
            moving_avg = np.divide(np.cumsum([r for r in rh]), (x+1))
            plt.plot(x, moving_avg, label=name, c=COLOR_LIST[i][&#39;id&#39;])


    def _plot_histogram(self, reward_sets, names, max_rewards=None):
        max_unique = -np.inf
        for rh in reward_sets:
            unique_count = np.unique([r for r in rh]).shape[0]
            if max_unique &lt; unique_count:
                max_unique = unique_count

        bin_count = int(max_unique) if max_unique &lt; 10 else 10

        # Plot rewards
        for i, (rh, name) in enumerate(zip(reward_sets, names)):
            plt.hist([r for r in rh], bin_count, label=name, color=COLOR_LIST[i][&#39;id&#39;])


class SimulationHistory:
    &#39;&#39;&#39;
    Class to represent a list of the steps that happened during a Simulation with:
        - the state the agent passes by (&#39;state&#39;)
        - the action the agent takes (&#39;action&#39;)
        - the state the agent lands in (&#39;next_state)
        - the reward received (&#39;reward&#39;)

    ...

    Parameters
    ----------
    model : mdp.Model
        The model on which the simulation happened on.
    start_state : int
        The initial state in the simulation.
        
    Attributes
    ----------
    model : mdp.Model
    states : list[int]
        A list of recorded states through which the agent passed by during the simulation process.
    grid_point_sequence : list[list[int]]
        A list of 2D points of the grid state through which the agent passed by during the simulation process.
    actions : list[int]
        A list of recorded actions the agent took during the simulation process.
    rewards: RewardSet
        The set of rewards received by the agent throughout the simulation process.
    &#39;&#39;&#39;

    def __init__(self, model:Model, start_state:int):
        self.model = model

        self.states = [start_state]
        self.grid_point_sequence = [[i[0] for i in np.where(self.model.state_grid == start_state)]]
        self.actions = []
        self.rewards = RewardSet()


    def add(self, action:int, reward, next_state:int) -&gt; None:
        &#39;&#39;&#39;
        Function to add a step in the simulation history

        Parameters
        ----------
        action : int
            The action that was taken by the agent.
        reward
            The reward received by the agent after having taken action.
        next_state : int
            The state that was reached by the agent after having taken action.
        &#39;&#39;&#39;
        self.actions.append(action)
        self.rewards.append(reward)
        self.states.append(next_state)
        self.grid_point_sequence.append([i[0] for i in np.where(self.model.state_grid == next_state)])
    

    def plot_simulation_steps(self, size:int=5):
        &#39;&#39;&#39;
        Plotting the path that was taken during the simulation.

        Parameters
        ----------
        size : int, default=5
            The scale of the plot.
        &#39;&#39;&#39;
        plt.figure(figsize=(size,size))

        # Ticks
        dimensions = self.model.state_grid.shape
        plt.xticks([i for i in range(dimensions[1])])
        plt.yticks([i for i in range(dimensions[0])])

        ax = plt.gca()
        ax.invert_yaxis()

        # Actual plotting
        data = np.array(self.grid_point_sequence)
        plt.plot(data[:,1], data[:,0], color=&#39;red&#39;)
        plt.scatter(data[:,1], data[:,0], color=&#39;red&#39;)
        plt.show()


    def _plot_to_frame_on_ax(self, frame_i, ax):
        # Data
        data = np.array(self.grid_point_sequence)[:(frame_i+1),:]

        # Ticks
        dimensions = self.model.state_grid.shape
        x_ticks = [i for i in range(dimensions[1])]
        y_ticks = [i for i in range(dimensions[0])]

        # Plotting
        ax.clear()
        ax.set_title(f&#39;Simulation (Frame {frame_i})&#39;)

        ax.plot(data[:,1], data[:,0], color=&#39;red&#39;)
        ax.scatter(data[:,1], data[:,0], color=&#39;red&#39;)

        ax.set_xticks(x_ticks)
        ax.set_yticks(y_ticks)
        ax.invert_yaxis()


    def save_simulation_video(self, custom_name:Union[str,None]=None, fps:int=1) -&gt; None:
        &#39;&#39;&#39;
        Function to save a video of the simulation history with all the states it passes through.

        Parameters
        ----------
        custom_name : str, optional
            By default, the file name will be a combination of the state count, the action count and the run timestamp. If a custom name is provided, it will be prepended to the rest of the info.
        fps : int, default=1
            The amount of steps per second appearing in the video.
        &#39;&#39;&#39;
        fig = plt.figure()
        ax = plt.gca()
        steps = len(self.states)

        ani = animation.FuncAnimation(fig, (lambda frame_i: self._plot_to_frame_on_ax(frame_i, ax)), frames=steps, interval=500, repeat=False)
        
        # File Title
        solved_time = datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)

        video_title = f&#39;{custom_name}-&#39; if custom_name is not None else &#39;&#39; # Base
        video_title += f&#39;s{self.model.state_count}-a{self.model.action_count}-&#39; # Model params
        video_title += f&#39;{solved_time}.mp4&#39;

        # Video saving
        if not os.path.exists(&#39;./Sim Videos&#39;):
            print(&#39;Folder does not exist yet, creating it...&#39;)
            os.makedirs(&#39;./Sim Videos&#39;)

        writervideo = animation.FFMpegWriter(fps=fps)
        ani.save(&#39;./Sim Videos/&#39; + video_title, writer=writervideo)
        print(f&#39;Video saved at \&#39;Sim Videos/{video_title}\&#39;...&#39;)
        plt.close()


class Simulation:
    &#39;&#39;&#39;
    Class to reprensent a simulation process for a POMDP model.
    An initial random state is given and action can be applied to the model that impact the actual state of the agent along with returning a reward and an observation.

    Can be overwritten to be fit simulation needs of particular problems.

    ...
    Parameters
    ----------
    model: mdp.Model
        The MDP model the simulation will be applied on.

    Attributes
    ----------
    model: mdp.Model
    agent_state : int
        The agent&#39;s state in the running simulation
    is_done : bool
        Whether or not the agent has reached an end state or performed an ending action.
    &#39;&#39;&#39;
    def __init__(self, model:Model) -&gt; None:
        self.model = model
        
        # Simulation variables
        self.agent_state = -1
        self.is_done = True # Need to run initialization first

        self.initialize_simulation()


    def initialize_simulation(self, start_state:Union[int, None]=None) -&gt; int:
        &#39;&#39;&#39;
        Function to initialize the simulation by setting a random start state (according to the start probabilities) to the agent.

        Parameters
        ----------
        start_state : int, optional
            The state the agent should start in. (Default: randomly over model&#39;s start probabilities)

        Returns
        -------
        state : int
            The state the agent will start in.
        &#39;&#39;&#39;
        if start_state is None:
            self.agent_state = int(np.random.choice(a=self.model.states, size=1, p=self.model.start_probabilities)[0])
        else:
            self.agent_state = start_state
        
        self.is_done = False
        return self.agent_state

    
    def run_action(self, a:int) -&gt; Tuple[Union[int, float], int]:
        &#39;&#39;&#39;
        Run one step of simulation with action a.

        Parameters
        ----------
        a : int
            The action to take in the simulation.

        Returns
        -------
        r
            The reward given when doing action a in state s.
        s_p : int
            The state the agent lands in.
        &#39;&#39;&#39;
        assert not self.is_done, &#34;Action run when simulation is done.&#34;

        s = self.agent_state
        s_p = self.model.transition(s,a)
        r = self.model.reward(s,a,s_p)

        # Update agent state
        self.agent_state = s_p

        # State Done check
        if s_p in self.model.end_states:
            self.is_done = True

        # Action Done check
        if a in self.model.end_actions:
            self.is_done = True

        return r, s_p


class Agent:
    &#39;&#39;&#39;
    The class of an Agent running on a MDP model.
    It has the ability to train using a given mdp solver.
    Then, once trained, it can simulate actions with a given Simulation,
    either for a given amount of steps or until a single reward is received.

    ...

    Parameters
    ----------
    model: mdp.Model
        The model in which the agent can run.
    
    Attributes
    ----------
    model : mdp.Model
    value_function : ValueFunction
        The value function the agent has come up to after training.
    &#39;&#39;&#39;
    def __init__(self, model:Model) -&gt; None:
        super().__init__()

        self.model = model
        self.value_function = None


    def train(self, solver:Union[Solver,None]=None) -&gt; SolverHistory:
        &#39;&#39;&#39;
        Method to train the agent using a given solver.
        The solver will provide a value function that will map states to actions.

        Parameters
        ----------
        solver : Solver, optional
            The solver to run. If not provided, will default to the Value-Iteration solver with default parameters

        Returns
        -------
        solve_history : SolverHistory
            The history of the solving process.
        &#39;&#39;&#39;
        if solver is None:
            solver = VI_Solver()
        
        self.value_function, solve_history = solver.solve(self.model)
        return solve_history


    def get_best_action(self, state:int) -&gt; int:
        &#39;&#39;&#39;
        Function to retrieve the best action for a given state based on the value function retrieved from the training.

        Parameters
        ----------
        state : int
            The state to get the best action with.
                
        Returns
        -------
        best_action : int
            The best action found.
        &#39;&#39;&#39;
        assert self.value_function is not None, &#34;No value function, training probably has to be run...&#34;

        best_vector = np.argmax(self.value_function.alpha_vector_array[:,state])
        best_action = self.value_function.actions[best_vector]

        return best_action


    def simulate(self,
                 simulator:Union[Simulation,None]=None,
                 max_steps:int=1000,
                 start_state:Union[int,None]=None,
                 print_progress:bool=True,
                 print_stats:bool=True
                 ) -&gt; SimulationHistory:
        &#39;&#39;&#39;
        Function to run a simulation with the current agent for up to &#39;max_steps&#39; amount of steps using a Simulation simulator.

        Parameters
        ----------
        simulator : mdp.Simulation, optional
            The simulation that will be used by the agent. If not provided, the default MDP simulator will be used.
        max_steps : int, default=1000
            The max amount of steps the simulation can run for.
        start_state : int, optional
            The state the agent should start in, if not provided, will be set at random based on start probabilities of the model.
        print_progress : bool, default=True
            Whether or not to print out the progress of the simulation.
        print_stats : bool, default=True
            Whether or not to print simulation statistics at the end of the simulation (Default: True)

        Returns
        -------
        history : SimulationHistory
            A step by step history of the simulation with additional functionality to plot rewards for example.
        &#39;&#39;&#39;
        if simulator is None:
            simulator = Simulation(self.model)

        s = simulator.initialize_simulation(start_state=start_state)

        history = SimulationHistory(self.model, s)

        sim_start_ts = datetime.now()

        # Simulation loop
        for _ in (trange(max_steps) if print_progress else range(max_steps)):
            # Play best action
            a = self.get_best_action(s)
            r, s_p = simulator.run_action(a)

            # Track progress
            history.add(action=a, next_state=s_p, reward=r)

            # Update current state
            s = s_p

            # If simulation is considered done, the rewards are simply returned
            if simulator.is_done:
                break

        if print_stats: # Move to SimulationHistory summary function?
            sim_end_ts = datetime.now()
            print(&#39;Simulation done:&#39;)
            print(f&#39;\t- Runtime (s): {(sim_end_ts - sim_start_ts).total_seconds()}&#39;)
            print(f&#39;\t- Steps: {len(history.states)}&#39;)
            print(f&#39;\t- Total rewards: {sum(history.rewards)}&#39;)
            print(f&#39;\t- End state: {self.model.state_labels[history.states[-1]]}&#39;)
            
        return history


    def run_n_simulations(self,
                          simulator:Union[Simulation,None]=None,
                          n:int=1000,
                          max_steps:int=1000,
                          start_state:int=-1,
                          print_progress:bool=True,
                          print_stats:bool=True
                          ) -&gt; RewardSet:
        &#39;&#39;&#39;
        Function to run a set of simulations in a row.
        This is useful when the simulation has a &#39;done&#39; condition.
        In this case, the rewards of individual simulations are summed together under a single number.

        Parameters
        ----------
        simulator : mdp.Simulation, optional
            The simulation that will be used by the agent. If not provided, the default MDP simulator will be used.
        n : int, default=1000
            The amount of simulations to run.
        max_steps : int, default=1000
            The max_steps to run per simulation.
        start_state : int, optional
            The  state the agent should start in, if not provided, will be set at random based on start probabilities of the model.
        print_progress : bool, default=True
            Whether or not to print out the progress of the simulation.
        print_stats : bool, default=True
            Whether or not to print simulation statistics at the end of the simulation.

        Returns
        -------
        all_final_rewards : RewardSet
            A list of the final rewards after each simulation.
        &#39;&#39;&#39;
        if simulator is None:
            simulator = Simulation(self.model)

        sim_start_ts = datetime.now()

        all_final_rewards = RewardSet()
        all_sim_length = []
        for _ in (trange(n) if print_progress else range(n)):
            sim_history = self.simulate(simulator, max_steps, start_state, False, False)
            all_final_rewards.append(sum(sim_history.rewards))
            all_sim_length.append(len(sim_history.states))

        if print_stats:
            sim_end_ts = datetime.now()
            print(f&#39;All {n} simulations done:&#39;)
            print(f&#39;\t- Average runtime (s): {((sim_end_ts - sim_start_ts).total_seconds() / n)}&#39;)
            print(f&#39;\t- Average step count: {(sum(all_sim_length) / n)}&#39;)
            print(f&#39;\t- Average total rewards: {(sum(all_final_rewards) / n)}&#39;)
        
        return all_final_rewards</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.mdp.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>content:str) >None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to print a log line with a timestamp.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>content</code></strong> :&ensp;<code>str</code></dt>
<dd>The content to be printed as a log.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log(content:str) -&gt; None:
    &#39;&#39;&#39;
    Function to print a log line with a timestamp.

    Parameters
    ----------
    content : str
        The content to be printed as a log.
    &#39;&#39;&#39;
    print(f&#39;[{datetime.now().strftime(&#34;%m/%d/%Y, %H:%M:%S&#34;)}] &#39; + content)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.mdp.Agent"><code class="flex name class">
<span>class <span class="ident">Agent</span></span>
<span>(</span><span>model:<a title="src.mdp.Model" href="#src.mdp.Model">Model</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>The class of an Agent running on a MDP model.
It has the ability to train using a given mdp solver.
Then, once trained, it can simulate actions with a given Simulation,
either for a given amount of steps or until a single reward is received.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>The model in which the agent can run.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>value_function</code></strong> :&ensp;<code><a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a></code></dt>
<dd>The value function the agent has come up to after training.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Agent:
    &#39;&#39;&#39;
    The class of an Agent running on a MDP model.
    It has the ability to train using a given mdp solver.
    Then, once trained, it can simulate actions with a given Simulation,
    either for a given amount of steps or until a single reward is received.

    ...

    Parameters
    ----------
    model: mdp.Model
        The model in which the agent can run.
    
    Attributes
    ----------
    model : mdp.Model
    value_function : ValueFunction
        The value function the agent has come up to after training.
    &#39;&#39;&#39;
    def __init__(self, model:Model) -&gt; None:
        super().__init__()

        self.model = model
        self.value_function = None


    def train(self, solver:Union[Solver,None]=None) -&gt; SolverHistory:
        &#39;&#39;&#39;
        Method to train the agent using a given solver.
        The solver will provide a value function that will map states to actions.

        Parameters
        ----------
        solver : Solver, optional
            The solver to run. If not provided, will default to the Value-Iteration solver with default parameters

        Returns
        -------
        solve_history : SolverHistory
            The history of the solving process.
        &#39;&#39;&#39;
        if solver is None:
            solver = VI_Solver()
        
        self.value_function, solve_history = solver.solve(self.model)
        return solve_history


    def get_best_action(self, state:int) -&gt; int:
        &#39;&#39;&#39;
        Function to retrieve the best action for a given state based on the value function retrieved from the training.

        Parameters
        ----------
        state : int
            The state to get the best action with.
                
        Returns
        -------
        best_action : int
            The best action found.
        &#39;&#39;&#39;
        assert self.value_function is not None, &#34;No value function, training probably has to be run...&#34;

        best_vector = np.argmax(self.value_function.alpha_vector_array[:,state])
        best_action = self.value_function.actions[best_vector]

        return best_action


    def simulate(self,
                 simulator:Union[Simulation,None]=None,
                 max_steps:int=1000,
                 start_state:Union[int,None]=None,
                 print_progress:bool=True,
                 print_stats:bool=True
                 ) -&gt; SimulationHistory:
        &#39;&#39;&#39;
        Function to run a simulation with the current agent for up to &#39;max_steps&#39; amount of steps using a Simulation simulator.

        Parameters
        ----------
        simulator : mdp.Simulation, optional
            The simulation that will be used by the agent. If not provided, the default MDP simulator will be used.
        max_steps : int, default=1000
            The max amount of steps the simulation can run for.
        start_state : int, optional
            The state the agent should start in, if not provided, will be set at random based on start probabilities of the model.
        print_progress : bool, default=True
            Whether or not to print out the progress of the simulation.
        print_stats : bool, default=True
            Whether or not to print simulation statistics at the end of the simulation (Default: True)

        Returns
        -------
        history : SimulationHistory
            A step by step history of the simulation with additional functionality to plot rewards for example.
        &#39;&#39;&#39;
        if simulator is None:
            simulator = Simulation(self.model)

        s = simulator.initialize_simulation(start_state=start_state)

        history = SimulationHistory(self.model, s)

        sim_start_ts = datetime.now()

        # Simulation loop
        for _ in (trange(max_steps) if print_progress else range(max_steps)):
            # Play best action
            a = self.get_best_action(s)
            r, s_p = simulator.run_action(a)

            # Track progress
            history.add(action=a, next_state=s_p, reward=r)

            # Update current state
            s = s_p

            # If simulation is considered done, the rewards are simply returned
            if simulator.is_done:
                break

        if print_stats: # Move to SimulationHistory summary function?
            sim_end_ts = datetime.now()
            print(&#39;Simulation done:&#39;)
            print(f&#39;\t- Runtime (s): {(sim_end_ts - sim_start_ts).total_seconds()}&#39;)
            print(f&#39;\t- Steps: {len(history.states)}&#39;)
            print(f&#39;\t- Total rewards: {sum(history.rewards)}&#39;)
            print(f&#39;\t- End state: {self.model.state_labels[history.states[-1]]}&#39;)
            
        return history


    def run_n_simulations(self,
                          simulator:Union[Simulation,None]=None,
                          n:int=1000,
                          max_steps:int=1000,
                          start_state:int=-1,
                          print_progress:bool=True,
                          print_stats:bool=True
                          ) -&gt; RewardSet:
        &#39;&#39;&#39;
        Function to run a set of simulations in a row.
        This is useful when the simulation has a &#39;done&#39; condition.
        In this case, the rewards of individual simulations are summed together under a single number.

        Parameters
        ----------
        simulator : mdp.Simulation, optional
            The simulation that will be used by the agent. If not provided, the default MDP simulator will be used.
        n : int, default=1000
            The amount of simulations to run.
        max_steps : int, default=1000
            The max_steps to run per simulation.
        start_state : int, optional
            The  state the agent should start in, if not provided, will be set at random based on start probabilities of the model.
        print_progress : bool, default=True
            Whether or not to print out the progress of the simulation.
        print_stats : bool, default=True
            Whether or not to print simulation statistics at the end of the simulation.

        Returns
        -------
        all_final_rewards : RewardSet
            A list of the final rewards after each simulation.
        &#39;&#39;&#39;
        if simulator is None:
            simulator = Simulation(self.model)

        sim_start_ts = datetime.now()

        all_final_rewards = RewardSet()
        all_sim_length = []
        for _ in (trange(n) if print_progress else range(n)):
            sim_history = self.simulate(simulator, max_steps, start_state, False, False)
            all_final_rewards.append(sum(sim_history.rewards))
            all_sim_length.append(len(sim_history.states))

        if print_stats:
            sim_end_ts = datetime.now()
            print(f&#39;All {n} simulations done:&#39;)
            print(f&#39;\t- Average runtime (s): {((sim_end_ts - sim_start_ts).total_seconds() / n)}&#39;)
            print(f&#39;\t- Average step count: {(sum(all_sim_length) / n)}&#39;)
            print(f&#39;\t- Average total rewards: {(sum(all_final_rewards) / n)}&#39;)
        
        return all_final_rewards</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="src.mdp.Agent.get_best_action"><code class="name flex">
<span>def <span class="ident">get_best_action</span></span>(<span>self, state:int) >int</span>
</code></dt>
<dd>
<div class="desc"><p>Function to retrieve the best action for a given state based on the value function retrieved from the training.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>int</code></dt>
<dd>The state to get the best action with.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>best_action</code></strong> :&ensp;<code>int</code></dt>
<dd>The best action found.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_best_action(self, state:int) -&gt; int:
    &#39;&#39;&#39;
    Function to retrieve the best action for a given state based on the value function retrieved from the training.

    Parameters
    ----------
    state : int
        The state to get the best action with.
            
    Returns
    -------
    best_action : int
        The best action found.
    &#39;&#39;&#39;
    assert self.value_function is not None, &#34;No value function, training probably has to be run...&#34;

    best_vector = np.argmax(self.value_function.alpha_vector_array[:,state])
    best_action = self.value_function.actions[best_vector]

    return best_action</code></pre>
</details>
</dd>
<dt id="src.mdp.Agent.run_n_simulations"><code class="name flex">
<span>def <span class="ident">run_n_simulations</span></span>(<span>self, simulator:Optional[<a title="src.mdp.Simulation" href="#src.mdp.Simulation">Simulation</a>]=None, n:int=1000, max_steps:int=1000, start_state:int=-1, print_progress:bool=True, print_stats:bool=True) ><a title="src.mdp.RewardSet" href="#src.mdp.RewardSet">RewardSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>Function to run a set of simulations in a row.
This is useful when the simulation has a 'done' condition.
In this case, the rewards of individual simulations are summed together under a single number.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>simulator</code></strong> :&ensp;<code>mdp.Simulation</code>, optional</dt>
<dd>The simulation that will be used by the agent. If not provided, the default MDP simulator will be used.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code>, default=<code>1000</code></dt>
<dd>The amount of simulations to run.</dd>
<dt><strong><code>max_steps</code></strong> :&ensp;<code>int</code>, default=<code>1000</code></dt>
<dd>The max_steps to run per simulation.</dd>
<dt><strong><code>start_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The
state the agent should start in, if not provided, will be set at random based on start probabilities of the model.</dd>
<dt><strong><code>print_progress</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether or not to print out the progress of the simulation.</dd>
<dt><strong><code>print_stats</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether or not to print simulation statistics at the end of the simulation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>all_final_rewards</code></strong> :&ensp;<code><a title="src.mdp.RewardSet" href="#src.mdp.RewardSet">RewardSet</a></code></dt>
<dd>A list of the final rewards after each simulation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_n_simulations(self,
                      simulator:Union[Simulation,None]=None,
                      n:int=1000,
                      max_steps:int=1000,
                      start_state:int=-1,
                      print_progress:bool=True,
                      print_stats:bool=True
                      ) -&gt; RewardSet:
    &#39;&#39;&#39;
    Function to run a set of simulations in a row.
    This is useful when the simulation has a &#39;done&#39; condition.
    In this case, the rewards of individual simulations are summed together under a single number.

    Parameters
    ----------
    simulator : mdp.Simulation, optional
        The simulation that will be used by the agent. If not provided, the default MDP simulator will be used.
    n : int, default=1000
        The amount of simulations to run.
    max_steps : int, default=1000
        The max_steps to run per simulation.
    start_state : int, optional
        The  state the agent should start in, if not provided, will be set at random based on start probabilities of the model.
    print_progress : bool, default=True
        Whether or not to print out the progress of the simulation.
    print_stats : bool, default=True
        Whether or not to print simulation statistics at the end of the simulation.

    Returns
    -------
    all_final_rewards : RewardSet
        A list of the final rewards after each simulation.
    &#39;&#39;&#39;
    if simulator is None:
        simulator = Simulation(self.model)

    sim_start_ts = datetime.now()

    all_final_rewards = RewardSet()
    all_sim_length = []
    for _ in (trange(n) if print_progress else range(n)):
        sim_history = self.simulate(simulator, max_steps, start_state, False, False)
        all_final_rewards.append(sum(sim_history.rewards))
        all_sim_length.append(len(sim_history.states))

    if print_stats:
        sim_end_ts = datetime.now()
        print(f&#39;All {n} simulations done:&#39;)
        print(f&#39;\t- Average runtime (s): {((sim_end_ts - sim_start_ts).total_seconds() / n)}&#39;)
        print(f&#39;\t- Average step count: {(sum(all_sim_length) / n)}&#39;)
        print(f&#39;\t- Average total rewards: {(sum(all_final_rewards) / n)}&#39;)
    
    return all_final_rewards</code></pre>
</details>
</dd>
<dt id="src.mdp.Agent.simulate"><code class="name flex">
<span>def <span class="ident">simulate</span></span>(<span>self, simulator:Optional[<a title="src.mdp.Simulation" href="#src.mdp.Simulation">Simulation</a>]=None, max_steps:int=1000, start_state:Optional[int]=None, print_progress:bool=True, print_stats:bool=True) ><a title="src.mdp.SimulationHistory" href="#src.mdp.SimulationHistory">SimulationHistory</a></span>
</code></dt>
<dd>
<div class="desc"><p>Function to run a simulation with the current agent for up to 'max_steps' amount of steps using a Simulation simulator.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>simulator</code></strong> :&ensp;<code>mdp.Simulation</code>, optional</dt>
<dd>The simulation that will be used by the agent. If not provided, the default MDP simulator will be used.</dd>
<dt><strong><code>max_steps</code></strong> :&ensp;<code>int</code>, default=<code>1000</code></dt>
<dd>The max amount of steps the simulation can run for.</dd>
<dt><strong><code>start_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The state the agent should start in, if not provided, will be set at random based on start probabilities of the model.</dd>
<dt><strong><code>print_progress</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether or not to print out the progress of the simulation.</dd>
<dt><strong><code>print_stats</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether or not to print simulation statistics at the end of the simulation (Default: True)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>history</code></strong> :&ensp;<code><a title="src.mdp.SimulationHistory" href="#src.mdp.SimulationHistory">SimulationHistory</a></code></dt>
<dd>A step by step history of the simulation with additional functionality to plot rewards for example.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulate(self,
             simulator:Union[Simulation,None]=None,
             max_steps:int=1000,
             start_state:Union[int,None]=None,
             print_progress:bool=True,
             print_stats:bool=True
             ) -&gt; SimulationHistory:
    &#39;&#39;&#39;
    Function to run a simulation with the current agent for up to &#39;max_steps&#39; amount of steps using a Simulation simulator.

    Parameters
    ----------
    simulator : mdp.Simulation, optional
        The simulation that will be used by the agent. If not provided, the default MDP simulator will be used.
    max_steps : int, default=1000
        The max amount of steps the simulation can run for.
    start_state : int, optional
        The state the agent should start in, if not provided, will be set at random based on start probabilities of the model.
    print_progress : bool, default=True
        Whether or not to print out the progress of the simulation.
    print_stats : bool, default=True
        Whether or not to print simulation statistics at the end of the simulation (Default: True)

    Returns
    -------
    history : SimulationHistory
        A step by step history of the simulation with additional functionality to plot rewards for example.
    &#39;&#39;&#39;
    if simulator is None:
        simulator = Simulation(self.model)

    s = simulator.initialize_simulation(start_state=start_state)

    history = SimulationHistory(self.model, s)

    sim_start_ts = datetime.now()

    # Simulation loop
    for _ in (trange(max_steps) if print_progress else range(max_steps)):
        # Play best action
        a = self.get_best_action(s)
        r, s_p = simulator.run_action(a)

        # Track progress
        history.add(action=a, next_state=s_p, reward=r)

        # Update current state
        s = s_p

        # If simulation is considered done, the rewards are simply returned
        if simulator.is_done:
            break

    if print_stats: # Move to SimulationHistory summary function?
        sim_end_ts = datetime.now()
        print(&#39;Simulation done:&#39;)
        print(f&#39;\t- Runtime (s): {(sim_end_ts - sim_start_ts).total_seconds()}&#39;)
        print(f&#39;\t- Steps: {len(history.states)}&#39;)
        print(f&#39;\t- Total rewards: {sum(history.rewards)}&#39;)
        print(f&#39;\t- End state: {self.model.state_labels[history.states[-1]]}&#39;)
        
    return history</code></pre>
</details>
</dd>
<dt id="src.mdp.Agent.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, solver:Optional[<a title="src.mdp.Solver" href="#src.mdp.Solver">Solver</a>]=None) ><a title="src.mdp.SolverHistory" href="#src.mdp.SolverHistory">SolverHistory</a></span>
</code></dt>
<dd>
<div class="desc"><p>Method to train the agent using a given solver.
The solver will provide a value function that will map states to actions.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>solver</code></strong> :&ensp;<code><a title="src.mdp.Solver" href="#src.mdp.Solver">Solver</a></code>, optional</dt>
<dd>The solver to run. If not provided, will default to the Value-Iteration solver with default parameters</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>solve_history</code></strong> :&ensp;<code><a title="src.mdp.SolverHistory" href="#src.mdp.SolverHistory">SolverHistory</a></code></dt>
<dd>The history of the solving process.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, solver:Union[Solver,None]=None) -&gt; SolverHistory:
    &#39;&#39;&#39;
    Method to train the agent using a given solver.
    The solver will provide a value function that will map states to actions.

    Parameters
    ----------
    solver : Solver, optional
        The solver to run. If not provided, will default to the Value-Iteration solver with default parameters

    Returns
    -------
    solve_history : SolverHistory
        The history of the solving process.
    &#39;&#39;&#39;
    if solver is None:
        solver = VI_Solver()
    
    self.value_function, solve_history = solver.solve(self.model)
    return solve_history</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.mdp.AlphaVector"><code class="flex name class">
<span>class <span class="ident">AlphaVector</span></span>
<span>(</span><span>values:numpy.ndarray, action:int)</span>
</code></dt>
<dd>
<div class="desc"><p>A class to represent an Alpha Vector, a vector representing a plane in |S| dimension for POMDP models.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>values</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The actual vector with the value for each state.</dd>
<dt><strong><code>action</code></strong> :&ensp;<code>int</code></dt>
<dd>The action associated with the vector.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AlphaVector:
    &#39;&#39;&#39;
    A class to represent an Alpha Vector, a vector representing a plane in |S| dimension for POMDP models.

    ...

    Parameters
    ----------
    values : np.ndarray
        The actual vector with the value for each state.
    action : int
        The action associated with the vector.
    &#39;&#39;&#39;
    def __init__(self, values:np.ndarray, action:int) -&gt; None:
        self.values = values
        self.action = action</code></pre>
</details>
</dd>
<dt id="src.mdp.Model"><code class="flex name class">
<span>class <span class="ident">Model</span></span>
<span>(</span><span>states:Union[int,list[str],list[list[str]]], actions:Union[int,list], transitions=None, reachable_states=None, rewards=None, rewards_are_probabilistic:bool=False, state_grid=None, start_probabilities:Optional[list]=None, end_states:list[int]=[], end_actions:list[int]=[])</span>
</code></dt>
<dd>
<div class="desc"><p>MDP Model class.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>states</code></strong> :&ensp;<code>int</code> or <code>list[str]</code> or <code>list[list[str]]</code></dt>
<dd>A list of state labels or an amount of states to be used. Also allows to provide a matrix of states to define a grid model.</dd>
<dt><strong><code>actions</code></strong> :&ensp;<code>int</code> or <code>list</code></dt>
<dd>A list of action labels or an amount of actions to be used.</dd>
<dt><strong><code>transitions</code></strong> :&ensp;<code>array-like</code> or <code>function</code>, optional</dt>
<dd>The transitions between states, an array can be provided and has to be |S| x |A| x |S| or a function can be provided.
If a function is provided, it has be able to deal with np.array arguments.
If none is provided, it will be randomly generated.</dd>
<dt><strong><code>reachable_states</code></strong> :&ensp;<code>array-like</code>, optional</dt>
<dd>A list of states that can be reached from each state and actions. It must be a matrix of size |S| x |A| x |R| where |R| is the max amount of states reachable from any given state and action pair.
It is optional but useful for speedup purposes.</dd>
<dt><strong><code>rewards</code></strong> :&ensp;<code>array-like</code> or <code>function</code>, optional</dt>
<dd>The reward matrix, has to be |S| x |A| x |S|.
A function can also be provided here but it has to be able to deal with np.array arguments.
If provided, it will be use in combination with the transition matrix to fill to expected rewards.</dd>
<dt><strong><code>rewards_are_probabilistic</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether the rewards provided are probabilistic or pure rewards. If probabilist 0 or 1 will be the reward with a certain probability.</dd>
<dt><strong><code>state_grid</code></strong> :&ensp;<code>array-like</code>, optional</dt>
<dd>If provided, the model will be converted to a grid model.</dd>
<dt><strong><code>start_probabilities</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>The distribution of chances to start in each state. If not provided, there will be an uniform chance for each state.</dd>
<dt><strong><code>end_states</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Entering either state in the list during a simulation will end the simulation.</dd>
<dt><strong><code>end_action</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Playing action of the list during a simulation will end the simulation.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>states</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 1D array of states indices. Used to loop over states.</dd>
<dt><strong><code>state_labels</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>A list of state labels. (To be mainly used for plotting)</dd>
<dt><strong><code>state_count</code></strong> :&ensp;<code>int</code></dt>
<dd>How many states are in the Model.</dd>
<dt><strong><code>state_grid</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The state indices organized as a 2D grid. (Used for plotting purposes)</dd>
<dt><strong><code>actions</code></strong> :&ensp;<code>np.ndarry</code></dt>
<dd>A 1D array of action indices. Used to loop over actions.</dd>
<dt><strong><code>action_labels</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>A list of action labels. (To be mainly used for plotting)</dd>
<dt><strong><code>action_count</code></strong> :&ensp;<code>int</code></dt>
<dd>How many action are in the Model.</dd>
<dt><strong><code>transition_table</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 3D matrix of the transition probabilities.
Can be None in the case a transition function is provided instead.
Note: When possible, use reachable states and reachable probabilities instead.</dd>
<dt><strong><code>transition_function</code></strong> :&ensp;<code>function</code></dt>
<dd>A callable function taking 3 arguments: s, a, s_p; and returning a float between 0.0 and 1.0.
Can be None in the case a transition table is provided instead.
Note: When possible, use reachable states and reachable probabilities instead.</dd>
<dt><strong><code>reachable_states</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 3D array of the shape S x A x R, where R is max amount to states that can be reached from any state-action pair.</dd>
<dt><strong><code>reachable_probabilities</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 3D array of the same shape as reachable_states, the array represent the probability of reaching the state pointed by the reachable_states matrix.</dd>
<dt><strong><code>reachable_state_count</code></strong> :&ensp;<code>int</code></dt>
<dd>The maximum of states that can be reached from any state-action combination.</dd>
<dt><strong><code>immediate_reward_table</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 3D matrix of shape S x A x S of the reward that will received when taking action a, in state s and landing in state s_p.
Can be None in the case an immediate rewards function is provided instead.</dd>
<dt><strong><code>immediate_reward_function</code></strong> :&ensp;<code>function</code></dt>
<dd>A callable function taking 3 argments: s, a, s_p and returning the immediate reward the agent will receive.
Can be None in the case an immediate rewards function is provided instead.</dd>
<dt><strong><code>expected_reward_table</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 2D array of shape S x A. It represents the rewards that is expected to be received when taking action a from state s.
It is made by taking the weighted average of immediate rewards and the transitions.</dd>
<dt><strong><code>start_probabilities</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 1D array of length |S| containing the probility distribution of the agent starting in each state.</dd>
<dt><strong><code>rewards_are_probabilisitic</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the immediate rewards are probabilitic, ie: returning a 0 or 1 based on the reward that is considered to be a probability.</dd>
<dt><strong><code>end_states</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>A list of states that, when reached, terminate a simulation.</dd>
<dt><strong><code>end_actions</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>A list of actions that, when taken, terminate a simulation.</dd>
<dt><strong><code>is_on_gpu</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the numpy array of the model are stored on the gpu or not.</dd>
<dt><strong><code>gpu_model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>An equivalent model with the np.ndarray objects on GPU. (If already on GPU, returns self)</dd>
<dt><strong><code>cpu_model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>An equivalent model with the np.ndarray objects on CPU. (If already on CPU, returns self)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Model:
    &#39;&#39;&#39;
    MDP Model class.

    ...

    Parameters
    ----------
    states : int or list[str] or list[list[str]]
        A list of state labels or an amount of states to be used. Also allows to provide a matrix of states to define a grid model.
    actions : int or list
        A list of action labels or an amount of actions to be used.
    transitions : array-like or function, optional
        The transitions between states, an array can be provided and has to be |S| x |A| x |S| or a function can be provided. 
        If a function is provided, it has be able to deal with np.array arguments.
        If none is provided, it will be randomly generated.
    reachable_states : array-like, optional
        A list of states that can be reached from each state and actions. It must be a matrix of size |S| x |A| x |R| where |R| is the max amount of states reachable from any given state and action pair.
        It is optional but useful for speedup purposes.
    rewards : array-like or function, optional
        The reward matrix, has to be |S| x |A| x |S|.
        A function can also be provided here but it has to be able to deal with np.array arguments.
        If provided, it will be use in combination with the transition matrix to fill to expected rewards.
    rewards_are_probabilistic : bool, default=False
        Whether the rewards provided are probabilistic or pure rewards. If probabilist 0 or 1 will be the reward with a certain probability.
    state_grid : array-like, optional
        If provided, the model will be converted to a grid model.
    start_probabilities : list, optional
        The distribution of chances to start in each state. If not provided, there will be an uniform chance for each state.
    end_states : list, optional
        Entering either state in the list during a simulation will end the simulation.
    end_action : list, optional
        Playing action of the list during a simulation will end the simulation.
    
    Attributes
    ----------
    states : np.ndarray
        A 1D array of states indices. Used to loop over states.
    state_labels : list[str]
        A list of state labels. (To be mainly used for plotting)
    state_count : int
        How many states are in the Model.
    state_grid : np.ndarray
        The state indices organized as a 2D grid. (Used for plotting purposes)
    actions : np.ndarry
        A 1D array of action indices. Used to loop over actions.
    action_labels : list[str]
        A list of action labels. (To be mainly used for plotting)
    action_count : int
        How many action are in the Model.
    transition_table : np.ndarray
        A 3D matrix of the transition probabilities.
        Can be None in the case a transition function is provided instead.
        Note: When possible, use reachable states and reachable probabilities instead.
    transition_function : function
        A callable function taking 3 arguments: s, a, s_p; and returning a float between 0.0 and 1.0.
        Can be None in the case a transition table is provided instead.
        Note: When possible, use reachable states and reachable probabilities instead.
    reachable_states : np.ndarray
        A 3D array of the shape S x A x R, where R is max amount to states that can be reached from any state-action pair.
    reachable_probabilities : np.ndarray
        A 3D array of the same shape as reachable_states, the array represent the probability of reaching the state pointed by the reachable_states matrix.
    reachable_state_count : int
        The maximum of states that can be reached from any state-action combination.
    immediate_reward_table : np.ndarray
        A 3D matrix of shape S x A x S of the reward that will received when taking action a, in state s and landing in state s_p.
        Can be None in the case an immediate rewards function is provided instead.
    immediate_reward_function : function
        A callable function taking 3 argments: s, a, s_p and returning the immediate reward the agent will receive.
        Can be None in the case an immediate rewards function is provided instead.
    expected_reward_table : np.ndarray
        A 2D array of shape S x A. It represents the rewards that is expected to be received when taking action a from state s.
        It is made by taking the weighted average of immediate rewards and the transitions.
    start_probabilities : np.ndarray
        A 1D array of length |S| containing the probility distribution of the agent starting in each state.
    rewards_are_probabilisitic : bool
        Whether the immediate rewards are probabilitic, ie: returning a 0 or 1 based on the reward that is considered to be a probability.
    end_states : list[int]
        A list of states that, when reached, terminate a simulation.
    end_actions : list[int]
        A list of actions that, when taken, terminate a simulation.
    is_on_gpu : bool
        Whether the numpy array of the model are stored on the gpu or not.
    gpu_model : mdp.Model
        An equivalent model with the np.ndarray objects on GPU. (If already on GPU, returns self)
    cpu_model : mdp.Model
        An equivalent model with the np.ndarray objects on CPU. (If already on CPU, returns self)
    &#39;&#39;&#39;
    def __init__(self,
                 states:Union[int, list[str], list[list[str]]],
                 actions:Union[int, list],
                 transitions=None,
                 reachable_states=None,
                 rewards=None,
                 rewards_are_probabilistic:bool=False,
                 state_grid=None,
                 start_probabilities:Union[list,None]=None,
                 end_states:list[int]=[],
                 end_actions:list[int]=[]
                 ):
        
        # Empty variable
        self._alt_model = None
        self.is_on_gpu = False
        
        log(&#39;Instantiation of MDP Model:&#39;)
        
        # ------------------------- States -------------------------
        self.state_grid = None
        if isinstance(states, int): # State count
            self.state_labels = [f&#39;s_{i}&#39; for i in range(states)]

        elif isinstance(states, list) and all(isinstance(item, list) for item in states): # 2D list of states
            dim1 = len(states)
            dim2 = len(states[0])
            assert all(len(state_dim) == dim2 for state_dim in states), &#34;All sublists of states must be of equal size&#34;
            
            self.state_labels = []
            for state_dim in states:
                for state in state_dim:
                    self.state_labels.append(state)

            self.state_grid = np.arange(dim1 * dim2).reshape(dim1, dim2)

        else: # Default: single of list of string items
            self.state_labels = [item for item in states if isinstance(item, str)]

        self.state_count = len(self.state_labels)
        self.states = np.arange(self.state_count)

        log(f&#39;- {self.state_count} states&#39;)

        # ------------------------- Actions -------------------------
        if isinstance(actions, int):
            self.action_labels = [f&#39;a_{i}&#39; for i in range(actions)]
        else:
            self.action_labels = actions
        self.action_count = len(self.action_labels)
        self.actions = np.arange(self.action_count)

        log(f&#39;- {self.action_count} actions&#39;)

        # ------------------------- Reachable states provided -------------------------
        self.reachable_states = None
        if reachable_states is not None:
            self.reachable_states = np.array(reachable_states)
            assert self.reachable_states.shape[:2] == (self.state_count, self.action_count), f&#34;Reachable states provided is not of the expected shape (received {self.reachable_states.shape}, expected ({self.state_count}, {self.action_count}, :))&#34;
            self.reachable_state_count = self.reachable_states.shape[2]

            log(f&#39;- At most {self.reachable_state_count} reachable states per state-action pair&#39;)

        # ------------------------- Transitions -------------------------
        log(&#39;- Starting generation of transitions table&#39;)
        start_ts = datetime.now()

        self.transition_table = None
        self.transition_function = None
        if transitions is None:
            if reachable_states is None:
                # If no transitiong matrix and no reachable states given, generate random one
                print(&#39;[Warning] No transition matrix and no reachable states have provided so a random transition matrix is generated...&#39;)
                random_probs = np.random.rand(self.state_count, self.action_count, self.state_count)

                # Normalization to have s_p probabilies summing to 1
                self.transition_table = random_probs / np.sum(random_probs, axis=2, keepdims=True)
            else:
                # Make uniform transition probabilities over reachable states
                print(f&#39;[Warning] No transition matrix or function provided but reachable states are, so probability to reach any reachable states will &#34;1 / reachable state count&#34; so here: {1/self.reachable_state_count:.3f}.&#39;)

        elif callable(transitions): # Transition function
            self.transition_function = transitions
            # Attempt to create transition table in memory
            t_arr = None
            try:
                t_arr = np.fromfunction(self.transition_function, (self.state_count, self.action_count, self.state_count))
            except MemoryError:
                print(&#39;[Warning] Not enough memory to store transition table, using transition function provided...&#39;)
            else:
                self.transition_table = t_arr

        else: # Array like
            self.transition_table = np.array(transitions)
            t_shape = self.transition_table.shape
            exp_shape = (self.state_count, self.action_count, self.state_count)
            assert t_shape == exp_shape, f&#34;Transitions table provided doesnt have the right shape, it should be SxAxS (expected {exp_shape}, received {t_shape})&#34;

        duration = (datetime.now() - start_ts).total_seconds()
        log(f&#39;    &gt; Done in {duration:.3f}s&#39;)
        if duration &gt; 1:
            log(f&#39;    &gt; /!\\ Transition table generation took long, if not done already, try to use the reachable_states parameter to speedup the process.&#39;)

        # ------------------------- Rewards are probabilistic toggle -------------------------
        self.rewards_are_probabilistic = rewards_are_probabilistic

        # ------------------------- State grid -------------------------
        log(&#39;- Generation of state grid&#39;)
        if state_grid is None and self.state_grid is None:
            self.state_grid = np.arange(self.state_count).reshape((1,self.state_count))
        
        elif state_grid is not None:
            assert all(isinstance(l, list) for l in state_grid), &#34;The provided states grid must be a list of lists.&#34;

            grid_shape = (len(state_grid), len(state_grid[0]))
            assert all(len(l) == grid_shape[1] for l in state_grid), &#34;All rows must have the same length.&#34;

            if all(all(isinstance(e, int) for e in l) for l in state_grid):
                state_grid = np.array(state_grid)
                try:
                    self.states[state_grid]
                except:
                    raise Exception(&#39;An error occured with the list of state indices provided...&#39;)
                else:
                    self.state_grid = state_grid

            else:
                log(&#39;    &gt; Warning: looping through all grid states provided to find the corresponding states, can take a while...&#39;)
                
                np_state_grid = np.zeros(grid_shape, dtype=int)
                states_covered = 0
                for i, row in enumerate(state_grid):
                    for j, element in enumerate(state_grid):
                        if isinstance(element, str) and (element in self.state_labels):
                            states_covered += 1
                            np_state_grid[i,j] = self.state_labels.index(element)
                        elif isinstance(element, int) and (element &lt; self.state_count):
                            np_state_grid[i,j] = element
                        
                        else:
                            raise Exception(f&#39;Countains a state (\&#39;{state}\&#39;) not in the list of states...&#39;)

                assert states_covered == self.state_count, &#34;Some states of the state list are missing...&#34;

        # ------------------------- Start state probabilities -------------------------
        log(&#39;- Generating start probabilities table&#39;)
        if start_probabilities is not None:
            assert len(start_probabilities) == self.state_count
            self.start_probabilities = np.array(start_probabilities,dtype=float)
        else:
            self.start_probabilities = np.full((self.state_count), 1/self.state_count)

        # ------------------------- End state conditions -------------------------
        self.end_states = end_states
        self.end_actions = end_actions
        
        # ------------------------- Reachable states -------------------------
        # If not set yet
        if self.reachable_states is None:
            log(&#39;- Starting computation of reachable states from transition data&#39;)
            
            if self.state_count &gt; 1000:
                log(&#39;-    &gt; Warning: For models with large amounts of states, this operation can take time. Try generating it advance and use the parameter \&#39;reachable_states\&#39;...&#39;)
            
            start_ts = datetime.now()

            self.reachable_states = []
            self.reachable_state_count = 0
            for s in self.states:
                reachable_states_for_action = []
                for a in self.actions:
                    reachable_list = []
                    if self.transition_table is not None:
                        reachable_list = np.argwhere(self.transition_table[s,a,:] &gt; 0)[:,0].tolist()
                    else:
                        for sn in self.states:
                            if self.transition_function(s,a,sn) &gt; 0:
                                reachable_list.append(sn)
                    reachable_states_for_action.append(reachable_list)
                    
                    if len(reachable_list) &gt; self.reachable_state_count:
                        self.reachable_state_count = len(reachable_list)

                self.reachable_states.append(reachable_states_for_action)

            # In case some state-action pairs lead to more states than other, we fill with the 1st non states not used
            for s in self.states:
                for a in self.actions:
                    to_add = 0
                    while len(self.reachable_states[s][a]) &lt; self.reachable_state_count:
                        if to_add not in self.reachable_states[s][a]:
                            self.reachable_states[s][a].append(to_add)
                        to_add += 1

            # Converting to ndarray
            self.reachable_states = np.array(self.reachable_states, dtype=int)

            duration = (datetime.now() - start_ts).total_seconds()
            log(f&#39;    &gt; Done in {duration:.3f}s&#39;)
            log(f&#39;- At most {self.reachable_state_count} reachable states per state-action pair&#39;)

        # ------------------------- Reachable state probabilities -------------------------
        log(&#39;- Starting computation of reachable state probabilities from transition data&#39;)
        start_ts = datetime.now()

        if self.transition_function is None and self.transition_table is None:
            self.reachable_probabilities = np.full(self.reachable_states.shape, 1/self.reachable_state_count)
        elif self.transition_table is not None:
            self.reachable_probabilities = self.transition_table[self.states[:,None,None], self.actions[None,:,None], self.reachable_states]
        else:
            self.reachable_probabilities = np.fromfunction((lambda s,a,ri: self.transition_function(s.astype(int), a.astype(int), self.reachable_states[s.astype(int), a.astype(int), ri.astype(int)])), self.reachable_states.shape)
            
        duration = (datetime.now() - start_ts).total_seconds()
        log(f&#39;    &gt; Done in {duration:.3f}s&#39;)

        # ------------------------- Rewards -------------------------
        self.immediate_reward_table = None
        self.immediate_reward_function = None
        if rewards == -1: # If -1 is set, it means the rewards are defined in the superclass POMDP
            pass
        elif rewards is None:
            # If no reward matrix given, generate random one
            self.immediate_reward_table = np.random.rand(self.state_count, self.action_count, self.state_count)
        elif callable(rewards):
            # Rewards is a function
            self.immediate_reward_function = rewards
            assert len(signature(rewards).parameters) == 3, &#34;Reward function should accept 3 parameters: s, a, sn...&#34;
        else:
            # Array like
            self.immediate_reward_table = np.array(rewards)
            r_shape = self.immediate_reward_table.shape
            exp_shape = (self.state_count, self.action_count, self.state_count)
            assert r_shape == exp_shape, f&#34;Rewards table doesnt have the right shape, it should be SxAxS (expected: {exp_shape}, received {r_shape})&#34;

        # ------------------------- Expected rewards -------------------------
        self.expected_rewards_table = None
        if rewards != -1:
            log(&#39;- Starting generation of expected rewards table&#39;)
            start_ts = datetime.now()

            reachable_rewards = None
            if self.immediate_reward_table is not None:
                reachable_rewards = self.immediate_reward_table[self.states[:,None,None], self.actions[None,:,None], self.reachable_states]
            else:
                def reach_reward_func(s,a,ri):
                    s = s.astype(int)
                    a = a.astype(int)
                    ri = ri.astype(int)
                    return self.immediate_reward_function(s,a,self.reachable_states[s,a,ri])
                
                reachable_rewards = np.fromfunction(reach_reward_func, self.reachable_states.shape)
                
            self.expected_rewards_table = np.einsum(&#39;sar,sar-&gt;sa&#39;, self.reachable_probabilities, reachable_rewards)

            duration = (datetime.now() - start_ts).total_seconds()
            log(f&#39;    &gt; Done in {duration:.3f}s&#39;)

    
    def transition(self, s:int, a:int) -&gt; int:
        &#39;&#39;&#39;
        Returns a random posterior state knowing we take action a in state t and weighted on the transition probabilities.

        Parameters
        ----------
        s : int 
            The current state
        a : int
            The action to take

        Returns
        -------
        s_p : int
            The posterior state
        &#39;&#39;&#39;
        xp = cp if self.is_on_gpu else np
        s_p = int(xp.random.choice(a=self.reachable_states[s,a], size=1, p=self.reachable_probabilities[s,a])[0])
        return s_p
    

    def reward(self, s:int, a:int, s_p:int) -&gt; Union[int,float]:
        &#39;&#39;&#39;
        Returns the rewards of playing action a when in state s and landing in state s_p.
        If the rewards are probabilistic, it will return 0 or 1.

        Parameters
        ----------
        s : int
            The current state
        a : int
            The action taking in state s
        s_p : int
            The state landing in after taking action a in state s

        Returns
        -------
        reward : int or float
            The reward received.
        &#39;&#39;&#39;
        reward = self.immediate_reward_table[s,a,s_p] if self.immediate_reward_table is not None else self.immediate_reward_function(s,a,s_p)
        if self.rewards_are_probabilistic:
            rnd = random.random()
            return 1 if rnd &lt; reward else 0
        else:
            return reward
    

    def save(self, file_name:str, path:str=&#39;./Models&#39;) -&gt; None:
        &#39;&#39;&#39;
        Function to save the current model in a json file.
        By default, the model will be saved in &#39;Models&#39; directory in the current working directory but this can be changed using the &#39;path&#39; parameter.

        Parameters
        ----------
        file_name : str
            The name of the json file the model will be saved in.
        path : str, default=&#39;./Models&#39;
            The path at which the model will be saved.
        &#39;&#39;&#39;
        if not os.path.exists(path):
            print(&#39;Folder does not exist yet, creating it...&#39;)
            os.makedirs(path)

        if not file_name.endswith(&#39;.json&#39;):
            file_name += &#39;.json&#39;

        # Taking the arguments of the CPU version of the model
        argument_dict = self.cpu_model.__dict__

        # Converting the numpy array to lists
        for k,v in argument_dict.items():
            argument_dict.__setattr__(k, v.tolist() if isinstance(v, np.ndarray) else v)

        json_object = json.dumps(argument_dict, indent=4)
        with open(path + &#39;/&#39; + file_name, &#39;w&#39;) as outfile:
            outfile.write(json_object)


    @classmethod
    def load_from_json(cls, file:str) -&gt; Self:
        &#39;&#39;&#39;
        Function to load a MDP model from a json file. The json structure must contain the same items as in the constructor of this class.

        Parameters
        ----------
        file : str
            The file and path of the model to be loaded.
                
        Returns
        -------
        loaded_model : mdp.Model
            An instance of the loaded model.
        &#39;&#39;&#39;
        with open(file, &#39;r&#39;) as openfile:
            json_model = json.load(openfile)

        loaded_model = super().__new__(cls)
        for k,v in json_model.items():
            loaded_model.__setattr__(k, np.array(v) if isinstance(v, list) else v)

        if &#39;grid_states&#39; in json_model:
            loaded_model.convert_to_grid(json_model[&#39;grid_states&#39;])

        return loaded_model


    @property
    def gpu_model(self) -&gt; Self:
        &#39;&#39;&#39;
        The same model but on the GPU instead of the CPU. If already on the GPU, the current model object is returned.
        &#39;&#39;&#39;
        if self.is_on_gpu:
            return self
        
        assert gpu_support, &#34;GPU Support is not available, try installing cupy...&#34;
        
        if self._alt_model is None:
            log(&#39;Sending Model to GPU...&#39;)
            start = datetime.now()

            # Setting all the arguments of the new class and convert to cupy if numpy array
            new_model = super().__new__(self.__class__)
            for arg, val in self.__dict__.items():
                new_model.__setattr__(arg, cp.array(val) if isinstance(val, np.ndarray) else val)

            # GPU/CPU variables
            new_model.is_on_gpu = True
            new_model._alt_model = self
            self._alt_model = new_model
            
            duration = (datetime.now() - start).total_seconds()
            log(f&#39;    &gt; Done in {duration:.3f}s&#39;)

        return self._alt_model


    @property
    def cpu_model(self) -&gt; Self:
        &#39;&#39;&#39;
        The same model but on the CPU instead of the GPU. If already on the CPU, the current model object is returned.
        &#39;&#39;&#39;
        if not self.is_on_gpu:
            return self
        
        assert gpu_support, &#34;GPU Support is not available, try installing cupy...&#34;

        if self._alt_model is None:
            log(&#39;Sending Model to CPU...&#39;)
            start = datetime.now()

            # Setting all the arguments of the new class and convert to numpy if cupy array
            new_model = super().__new__(self.__class__)
            for arg, val in self.__dict__.items():
                new_model.__setattr__(arg, cp.asnumpy(val) if isinstance(val, cp.ndarray) else val)
            
            # GPU/CPU variables
            new_model.is_on_gpu = False
            new_model._alt_model = self
            self._alt_model = new_model
            
            duration = (datetime.now() - start).total_seconds()
            log(f&#39;    &gt; Done in {duration:.3f}s&#39;)

        return self._alt_model</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="src.pomdp.Model" href="pomdp.html#src.pomdp.Model">Model</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="src.mdp.Model.load_from_json"><code class="name flex">
<span>def <span class="ident">load_from_json</span></span>(<span>file:str) >Self</span>
</code></dt>
<dd>
<div class="desc"><p>Function to load a MDP model from a json file. The json structure must contain the same items as in the constructor of this class.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file</code></strong> :&ensp;<code>str</code></dt>
<dd>The file and path of the model to be loaded.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loaded_model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>An instance of the loaded model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load_from_json(cls, file:str) -&gt; Self:
    &#39;&#39;&#39;
    Function to load a MDP model from a json file. The json structure must contain the same items as in the constructor of this class.

    Parameters
    ----------
    file : str
        The file and path of the model to be loaded.
            
    Returns
    -------
    loaded_model : mdp.Model
        An instance of the loaded model.
    &#39;&#39;&#39;
    with open(file, &#39;r&#39;) as openfile:
        json_model = json.load(openfile)

    loaded_model = super().__new__(cls)
    for k,v in json_model.items():
        loaded_model.__setattr__(k, np.array(v) if isinstance(v, list) else v)

    if &#39;grid_states&#39; in json_model:
        loaded_model.convert_to_grid(json_model[&#39;grid_states&#39;])

    return loaded_model</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="src.mdp.Model.cpu_model"><code class="name">var <span class="ident">cpu_model</span> :Self</code></dt>
<dd>
<div class="desc"><p>The same model but on the CPU instead of the GPU. If already on the CPU, the current model object is returned.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def cpu_model(self) -&gt; Self:
    &#39;&#39;&#39;
    The same model but on the CPU instead of the GPU. If already on the CPU, the current model object is returned.
    &#39;&#39;&#39;
    if not self.is_on_gpu:
        return self
    
    assert gpu_support, &#34;GPU Support is not available, try installing cupy...&#34;

    if self._alt_model is None:
        log(&#39;Sending Model to CPU...&#39;)
        start = datetime.now()

        # Setting all the arguments of the new class and convert to numpy if cupy array
        new_model = super().__new__(self.__class__)
        for arg, val in self.__dict__.items():
            new_model.__setattr__(arg, cp.asnumpy(val) if isinstance(val, cp.ndarray) else val)
        
        # GPU/CPU variables
        new_model.is_on_gpu = False
        new_model._alt_model = self
        self._alt_model = new_model
        
        duration = (datetime.now() - start).total_seconds()
        log(f&#39;    &gt; Done in {duration:.3f}s&#39;)

    return self._alt_model</code></pre>
</details>
</dd>
<dt id="src.mdp.Model.gpu_model"><code class="name">var <span class="ident">gpu_model</span> :Self</code></dt>
<dd>
<div class="desc"><p>The same model but on the GPU instead of the CPU. If already on the GPU, the current model object is returned.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def gpu_model(self) -&gt; Self:
    &#39;&#39;&#39;
    The same model but on the GPU instead of the CPU. If already on the GPU, the current model object is returned.
    &#39;&#39;&#39;
    if self.is_on_gpu:
        return self
    
    assert gpu_support, &#34;GPU Support is not available, try installing cupy...&#34;
    
    if self._alt_model is None:
        log(&#39;Sending Model to GPU...&#39;)
        start = datetime.now()

        # Setting all the arguments of the new class and convert to cupy if numpy array
        new_model = super().__new__(self.__class__)
        for arg, val in self.__dict__.items():
            new_model.__setattr__(arg, cp.array(val) if isinstance(val, np.ndarray) else val)

        # GPU/CPU variables
        new_model.is_on_gpu = True
        new_model._alt_model = self
        self._alt_model = new_model
        
        duration = (datetime.now() - start).total_seconds()
        log(f&#39;    &gt; Done in {duration:.3f}s&#39;)

    return self._alt_model</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.mdp.Model.reward"><code class="name flex">
<span>def <span class="ident">reward</span></span>(<span>self, s:int, a:int, s_p:int) >Union[int,float]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the rewards of playing action a when in state s and landing in state s_p.
If the rewards are probabilistic, it will return 0 or 1.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>s</code></strong> :&ensp;<code>int</code></dt>
<dd>The current state</dd>
<dt><strong><code>a</code></strong> :&ensp;<code>int</code></dt>
<dd>The action taking in state s</dd>
<dt><strong><code>s_p</code></strong> :&ensp;<code>int</code></dt>
<dd>The state landing in after taking action a in state s</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>reward</code></strong> :&ensp;<code>int</code> or <code>float</code></dt>
<dd>The reward received.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reward(self, s:int, a:int, s_p:int) -&gt; Union[int,float]:
    &#39;&#39;&#39;
    Returns the rewards of playing action a when in state s and landing in state s_p.
    If the rewards are probabilistic, it will return 0 or 1.

    Parameters
    ----------
    s : int
        The current state
    a : int
        The action taking in state s
    s_p : int
        The state landing in after taking action a in state s

    Returns
    -------
    reward : int or float
        The reward received.
    &#39;&#39;&#39;
    reward = self.immediate_reward_table[s,a,s_p] if self.immediate_reward_table is not None else self.immediate_reward_function(s,a,s_p)
    if self.rewards_are_probabilistic:
        rnd = random.random()
        return 1 if rnd &lt; reward else 0
    else:
        return reward</code></pre>
</details>
</dd>
<dt id="src.mdp.Model.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, file_name:str, path:str='./Models') >None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to save the current model in a json file.
By default, the model will be saved in 'Models' directory in the current working directory but this can be changed using the 'path' parameter.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the json file the model will be saved in.</dd>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code>, default=<code>'./Models'</code></dt>
<dd>The path at which the model will be saved.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, file_name:str, path:str=&#39;./Models&#39;) -&gt; None:
    &#39;&#39;&#39;
    Function to save the current model in a json file.
    By default, the model will be saved in &#39;Models&#39; directory in the current working directory but this can be changed using the &#39;path&#39; parameter.

    Parameters
    ----------
    file_name : str
        The name of the json file the model will be saved in.
    path : str, default=&#39;./Models&#39;
        The path at which the model will be saved.
    &#39;&#39;&#39;
    if not os.path.exists(path):
        print(&#39;Folder does not exist yet, creating it...&#39;)
        os.makedirs(path)

    if not file_name.endswith(&#39;.json&#39;):
        file_name += &#39;.json&#39;

    # Taking the arguments of the CPU version of the model
    argument_dict = self.cpu_model.__dict__

    # Converting the numpy array to lists
    for k,v in argument_dict.items():
        argument_dict.__setattr__(k, v.tolist() if isinstance(v, np.ndarray) else v)

    json_object = json.dumps(argument_dict, indent=4)
    with open(path + &#39;/&#39; + file_name, &#39;w&#39;) as outfile:
        outfile.write(json_object)</code></pre>
</details>
</dd>
<dt id="src.mdp.Model.transition"><code class="name flex">
<span>def <span class="ident">transition</span></span>(<span>self, s:int, a:int) >int</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a random posterior state knowing we take action a in state t and weighted on the transition probabilities.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>s</code></strong> :&ensp;<code>int </code></dt>
<dd>The current state</dd>
<dt><strong><code>a</code></strong> :&ensp;<code>int</code></dt>
<dd>The action to take</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>s_p</code></strong> :&ensp;<code>int</code></dt>
<dd>The posterior state</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transition(self, s:int, a:int) -&gt; int:
    &#39;&#39;&#39;
    Returns a random posterior state knowing we take action a in state t and weighted on the transition probabilities.

    Parameters
    ----------
    s : int 
        The current state
    a : int
        The action to take

    Returns
    -------
    s_p : int
        The posterior state
    &#39;&#39;&#39;
    xp = cp if self.is_on_gpu else np
    s_p = int(xp.random.choice(a=self.reachable_states[s,a], size=1, p=self.reachable_probabilities[s,a])[0])
    return s_p</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.mdp.RewardSet"><code class="flex name class">
<span>class <span class="ident">RewardSet</span></span>
<span>(</span><span>items:list=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Class to represent a list of rewards with some functionality to plot them. Plotting options:
- Totals: to plot a graph of the accumulated rewards over time.
- Moving average: to plot the moving average of the rewards received over time.
- Histogram: to plot a histogram of the various rewards received.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>items</code></strong> :&ensp;<code>list</code>, default=<code>[]</code></dt>
<dd>The rewards in the set.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RewardSet(list):
    &#39;&#39;&#39;
    Class to represent a list of rewards with some functionality to plot them. Plotting options:
        - Totals: to plot a graph of the accumulated rewards over time.
        - Moving average: to plot the moving average of the rewards received over time.
        - Histogram: to plot a histogram of the various rewards received.

    ...

    Parameters
    ----------
    items : list, default=[]
        The rewards in the set.
    &#39;&#39;&#39;
    def __init__(self, items:list=[]):
        self.extend(items)


    def plot(self,
             type:str=&#39;total&#39;,
             size:int=5,
             max_reward=None,
             compare_with:Union[Self, list[Self]]=[],
             graph_names:list[str]=[]
             ) -&gt; None:
        &#39;&#39;&#39;
        The method to plot summaries of the rewards received over time.
        The plots available:
            - Total (&#39;total&#39; or &#39;t&#39;): to plot the total reward as a cummulative sum over time.
            - Moving average (&#39;moving_average&#39; or &#39;ma&#39;): to plot the moving average of the rewards
            - Hisotgram (&#39;histogram&#39; or &#39;h&#39;): to plot the various reward in bins to plot a histogram of what was received

        Parameters
        ----------
        type : str, default=&#39;total&#39;
            The type of plot to generate.
        size : int, default=5
            The plot scale.
        max_reward : optional
            An upper bound to rewards that can be received at each timestep.
        compare_with : RewardSet or list[RewardSet], default=[]
            One or more RewardSets to plot onlonside this one for comparison.
        graph_names : list[str], default=[]
            A list of the names of the comparison graphs.
        &#39;&#39;&#39;
        plt.figure(figsize=(size*2,size))

        # Histories
        reward_sets = [self]
        if isinstance(compare_with, RewardSet):
            reward_sets.append(compare_with)
        else:
            reward_sets += compare_with
        
        assert len(reward_sets) &lt; len(COLOR_LIST), &#34;Not enough colors to plot all the comparisson graphs&#34;

        # Names
        names = []
        if len(graph_names) == 0:
            names.append(&#39;Main graph&#39;)
            for i in range(1, len(reward_sets)):
                names.append(f&#39;Comparisson {i}&#39;)
        else:
            assert len(graph_names) == len(reward_sets), &#34;Names for the graphs are provided but not enough&#34;
            names = copy.deepcopy(graph_names)

        # Actual plot
        if type in [&#39;total&#39;, &#39;t&#39;]:
            plt.title(&#39;Cummulative reward received of time&#39;)
            self._plot_total(reward_sets, names, max_reward)

        elif type in [&#39;moving_average&#39;, &#39;ma&#39;]:
            plt.title(&#39;Average rewards received of time&#39;)
            self._plot_moving_average(reward_sets, names, max_reward)

        elif type in [&#39;histogram&#39;, &#39;h&#39;]:
            plt.title(&#39;Histogram of rewards received&#39;)
            self._plot_histogram(reward_sets, names, max_reward)

        # Finalization
        plt.legend(loc=&#39;upper left&#39;)
        plt.show()


    def _plot_total(self, reward_sets, names, max_reward=None):
        x = np.arange(len(reward_sets[0]))

        # If given plot upper bound
        if max_reward is not None:
            y_best = max_reward * x
            plt.plot(x, y_best, color=&#39;red&#39;, linestyle=&#39;--&#39;, label=&#39;Max rewards&#39;)

        # Plot rewards
        for i, (rh, name) in enumerate(zip(reward_sets, names)):
            cum_rewards = np.cumsum([r for r in rh])
            plt.plot(x, cum_rewards, label=name, c=COLOR_LIST[i][&#39;id&#39;])
    

    def _plot_moving_average(self, reward_sets, names, max_reward=None):
        x = np.arange(len(reward_sets[0]))

        # If given plot upper bound
        if max_reward is not None:
            y_best = np.ones(len(reward_sets[0])) * max_reward
            plt.plot(x, y_best, color=&#39;red&#39;, linestyle=&#39;--&#39;, label=&#39;Max rewards&#39;)

        # Plot rewards
        for i, (rh, name) in enumerate(zip(reward_sets, names)):
            moving_avg = np.divide(np.cumsum([r for r in rh]), (x+1))
            plt.plot(x, moving_avg, label=name, c=COLOR_LIST[i][&#39;id&#39;])


    def _plot_histogram(self, reward_sets, names, max_rewards=None):
        max_unique = -np.inf
        for rh in reward_sets:
            unique_count = np.unique([r for r in rh]).shape[0]
            if max_unique &lt; unique_count:
                max_unique = unique_count

        bin_count = int(max_unique) if max_unique &lt; 10 else 10

        # Plot rewards
        for i, (rh, name) in enumerate(zip(reward_sets, names)):
            plt.hist([r for r in rh], bin_count, label=name, color=COLOR_LIST[i][&#39;id&#39;])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.list</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.mdp.RewardSet.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, type:str='total', size:int=5, max_reward=None, compare_with:Union[Self,list[Self]]=[], graph_names:list[str]=[]) >None</span>
</code></dt>
<dd>
<div class="desc"><p>The method to plot summaries of the rewards received over time.
The plots available:
- Total ('total' or 't'): to plot the total reward as a cummulative sum over time.
- Moving average ('moving_average' or 'ma'): to plot the moving average of the rewards
- Hisotgram ('histogram' or 'h'): to plot the various reward in bins to plot a histogram of what was received</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>type</code></strong> :&ensp;<code>str</code>, default=<code>'total'</code></dt>
<dd>The type of plot to generate.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code>, default=<code>5</code></dt>
<dd>The plot scale.</dd>
<dt><strong><code>max_reward</code></strong> :&ensp;<code>optional</code></dt>
<dd>An upper bound to rewards that can be received at each timestep.</dd>
<dt><strong><code>compare_with</code></strong> :&ensp;<code><a title="src.mdp.RewardSet" href="#src.mdp.RewardSet">RewardSet</a></code> or <code>list[<a title="src.mdp.RewardSet" href="#src.mdp.RewardSet">RewardSet</a>]</code>, default=<code>[]</code></dt>
<dd>One or more RewardSets to plot onlonside this one for comparison.</dd>
<dt><strong><code>graph_names</code></strong> :&ensp;<code>list[str]</code>, default=<code>[]</code></dt>
<dd>A list of the names of the comparison graphs.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self,
         type:str=&#39;total&#39;,
         size:int=5,
         max_reward=None,
         compare_with:Union[Self, list[Self]]=[],
         graph_names:list[str]=[]
         ) -&gt; None:
    &#39;&#39;&#39;
    The method to plot summaries of the rewards received over time.
    The plots available:
        - Total (&#39;total&#39; or &#39;t&#39;): to plot the total reward as a cummulative sum over time.
        - Moving average (&#39;moving_average&#39; or &#39;ma&#39;): to plot the moving average of the rewards
        - Hisotgram (&#39;histogram&#39; or &#39;h&#39;): to plot the various reward in bins to plot a histogram of what was received

    Parameters
    ----------
    type : str, default=&#39;total&#39;
        The type of plot to generate.
    size : int, default=5
        The plot scale.
    max_reward : optional
        An upper bound to rewards that can be received at each timestep.
    compare_with : RewardSet or list[RewardSet], default=[]
        One or more RewardSets to plot onlonside this one for comparison.
    graph_names : list[str], default=[]
        A list of the names of the comparison graphs.
    &#39;&#39;&#39;
    plt.figure(figsize=(size*2,size))

    # Histories
    reward_sets = [self]
    if isinstance(compare_with, RewardSet):
        reward_sets.append(compare_with)
    else:
        reward_sets += compare_with
    
    assert len(reward_sets) &lt; len(COLOR_LIST), &#34;Not enough colors to plot all the comparisson graphs&#34;

    # Names
    names = []
    if len(graph_names) == 0:
        names.append(&#39;Main graph&#39;)
        for i in range(1, len(reward_sets)):
            names.append(f&#39;Comparisson {i}&#39;)
    else:
        assert len(graph_names) == len(reward_sets), &#34;Names for the graphs are provided but not enough&#34;
        names = copy.deepcopy(graph_names)

    # Actual plot
    if type in [&#39;total&#39;, &#39;t&#39;]:
        plt.title(&#39;Cummulative reward received of time&#39;)
        self._plot_total(reward_sets, names, max_reward)

    elif type in [&#39;moving_average&#39;, &#39;ma&#39;]:
        plt.title(&#39;Average rewards received of time&#39;)
        self._plot_moving_average(reward_sets, names, max_reward)

    elif type in [&#39;histogram&#39;, &#39;h&#39;]:
        plt.title(&#39;Histogram of rewards received&#39;)
        self._plot_histogram(reward_sets, names, max_reward)

    # Finalization
    plt.legend(loc=&#39;upper left&#39;)
    plt.show()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.mdp.Simulation"><code class="flex name class">
<span>class <span class="ident">Simulation</span></span>
<span>(</span><span>model:<a title="src.mdp.Model" href="#src.mdp.Model">Model</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to reprensent a simulation process for a POMDP model.
An initial random state is given and action can be applied to the model that impact the actual state of the agent along with returning a reward and an observation.</p>
<p>Can be overwritten to be fit simulation needs of particular problems.</p>
<p>&hellip;
Parameters</p>
<hr>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>The MDP model the simulation will be applied on.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>agent_state</code></strong> :&ensp;<code>int</code></dt>
<dd>The agent's state in the running simulation</dd>
<dt><strong><code>is_done</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether or not the agent has reached an end state or performed an ending action.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Simulation:
    &#39;&#39;&#39;
    Class to reprensent a simulation process for a POMDP model.
    An initial random state is given and action can be applied to the model that impact the actual state of the agent along with returning a reward and an observation.

    Can be overwritten to be fit simulation needs of particular problems.

    ...
    Parameters
    ----------
    model: mdp.Model
        The MDP model the simulation will be applied on.

    Attributes
    ----------
    model: mdp.Model
    agent_state : int
        The agent&#39;s state in the running simulation
    is_done : bool
        Whether or not the agent has reached an end state or performed an ending action.
    &#39;&#39;&#39;
    def __init__(self, model:Model) -&gt; None:
        self.model = model
        
        # Simulation variables
        self.agent_state = -1
        self.is_done = True # Need to run initialization first

        self.initialize_simulation()


    def initialize_simulation(self, start_state:Union[int, None]=None) -&gt; int:
        &#39;&#39;&#39;
        Function to initialize the simulation by setting a random start state (according to the start probabilities) to the agent.

        Parameters
        ----------
        start_state : int, optional
            The state the agent should start in. (Default: randomly over model&#39;s start probabilities)

        Returns
        -------
        state : int
            The state the agent will start in.
        &#39;&#39;&#39;
        if start_state is None:
            self.agent_state = int(np.random.choice(a=self.model.states, size=1, p=self.model.start_probabilities)[0])
        else:
            self.agent_state = start_state
        
        self.is_done = False
        return self.agent_state

    
    def run_action(self, a:int) -&gt; Tuple[Union[int, float], int]:
        &#39;&#39;&#39;
        Run one step of simulation with action a.

        Parameters
        ----------
        a : int
            The action to take in the simulation.

        Returns
        -------
        r
            The reward given when doing action a in state s.
        s_p : int
            The state the agent lands in.
        &#39;&#39;&#39;
        assert not self.is_done, &#34;Action run when simulation is done.&#34;

        s = self.agent_state
        s_p = self.model.transition(s,a)
        r = self.model.reward(s,a,s_p)

        # Update agent state
        self.agent_state = s_p

        # State Done check
        if s_p in self.model.end_states:
            self.is_done = True

        # Action Done check
        if a in self.model.end_actions:
            self.is_done = True

        return r, s_p</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="src.pomdp.Simulation" href="pomdp.html#src.pomdp.Simulation">Simulation</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.mdp.Simulation.initialize_simulation"><code class="name flex">
<span>def <span class="ident">initialize_simulation</span></span>(<span>self, start_state:Optional[int]=None) >int</span>
</code></dt>
<dd>
<div class="desc"><p>Function to initialize the simulation by setting a random start state (according to the start probabilities) to the agent.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>start_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The state the agent should start in. (Default: randomly over model's start probabilities)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>int</code></dt>
<dd>The state the agent will start in.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_simulation(self, start_state:Union[int, None]=None) -&gt; int:
    &#39;&#39;&#39;
    Function to initialize the simulation by setting a random start state (according to the start probabilities) to the agent.

    Parameters
    ----------
    start_state : int, optional
        The state the agent should start in. (Default: randomly over model&#39;s start probabilities)

    Returns
    -------
    state : int
        The state the agent will start in.
    &#39;&#39;&#39;
    if start_state is None:
        self.agent_state = int(np.random.choice(a=self.model.states, size=1, p=self.model.start_probabilities)[0])
    else:
        self.agent_state = start_state
    
    self.is_done = False
    return self.agent_state</code></pre>
</details>
</dd>
<dt id="src.mdp.Simulation.run_action"><code class="name flex">
<span>def <span class="ident">run_action</span></span>(<span>self, a:int) >Tuple[Union[int,float],int]</span>
</code></dt>
<dd>
<div class="desc"><p>Run one step of simulation with action a.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>int</code></dt>
<dd>The action to take in the simulation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>r</code></dt>
<dd>The reward given when doing action a in state s.</dd>
<dt><strong><code>s_p</code></strong> :&ensp;<code>int</code></dt>
<dd>The state the agent lands in.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_action(self, a:int) -&gt; Tuple[Union[int, float], int]:
    &#39;&#39;&#39;
    Run one step of simulation with action a.

    Parameters
    ----------
    a : int
        The action to take in the simulation.

    Returns
    -------
    r
        The reward given when doing action a in state s.
    s_p : int
        The state the agent lands in.
    &#39;&#39;&#39;
    assert not self.is_done, &#34;Action run when simulation is done.&#34;

    s = self.agent_state
    s_p = self.model.transition(s,a)
    r = self.model.reward(s,a,s_p)

    # Update agent state
    self.agent_state = s_p

    # State Done check
    if s_p in self.model.end_states:
        self.is_done = True

    # Action Done check
    if a in self.model.end_actions:
        self.is_done = True

    return r, s_p</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.mdp.SimulationHistory"><code class="flex name class">
<span>class <span class="ident">SimulationHistory</span></span>
<span>(</span><span>model:<a title="src.mdp.Model" href="#src.mdp.Model">Model</a>, start_state:int)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to represent a list of the steps that happened during a Simulation with:
- the state the agent passes by ('state')
- the action the agent takes ('action')
- the state the agent lands in ('next_state)
- the reward received ('reward')</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>The model on which the simulation happened on.</dd>
<dt><strong><code>start_state</code></strong> :&ensp;<code>int</code></dt>
<dd>The initial state in the simulation.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>states</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>A list of recorded states through which the agent passed by during the simulation process.</dd>
<dt><strong><code>grid_point_sequence</code></strong> :&ensp;<code>list[list[int]]</code></dt>
<dd>A list of 2D points of the grid state through which the agent passed by during the simulation process.</dd>
<dt><strong><code>actions</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>A list of recorded actions the agent took during the simulation process.</dd>
<dt><strong><code>rewards</code></strong> :&ensp;<code><a title="src.mdp.RewardSet" href="#src.mdp.RewardSet">RewardSet</a></code></dt>
<dd>The set of rewards received by the agent throughout the simulation process.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SimulationHistory:
    &#39;&#39;&#39;
    Class to represent a list of the steps that happened during a Simulation with:
        - the state the agent passes by (&#39;state&#39;)
        - the action the agent takes (&#39;action&#39;)
        - the state the agent lands in (&#39;next_state)
        - the reward received (&#39;reward&#39;)

    ...

    Parameters
    ----------
    model : mdp.Model
        The model on which the simulation happened on.
    start_state : int
        The initial state in the simulation.
        
    Attributes
    ----------
    model : mdp.Model
    states : list[int]
        A list of recorded states through which the agent passed by during the simulation process.
    grid_point_sequence : list[list[int]]
        A list of 2D points of the grid state through which the agent passed by during the simulation process.
    actions : list[int]
        A list of recorded actions the agent took during the simulation process.
    rewards: RewardSet
        The set of rewards received by the agent throughout the simulation process.
    &#39;&#39;&#39;

    def __init__(self, model:Model, start_state:int):
        self.model = model

        self.states = [start_state]
        self.grid_point_sequence = [[i[0] for i in np.where(self.model.state_grid == start_state)]]
        self.actions = []
        self.rewards = RewardSet()


    def add(self, action:int, reward, next_state:int) -&gt; None:
        &#39;&#39;&#39;
        Function to add a step in the simulation history

        Parameters
        ----------
        action : int
            The action that was taken by the agent.
        reward
            The reward received by the agent after having taken action.
        next_state : int
            The state that was reached by the agent after having taken action.
        &#39;&#39;&#39;
        self.actions.append(action)
        self.rewards.append(reward)
        self.states.append(next_state)
        self.grid_point_sequence.append([i[0] for i in np.where(self.model.state_grid == next_state)])
    

    def plot_simulation_steps(self, size:int=5):
        &#39;&#39;&#39;
        Plotting the path that was taken during the simulation.

        Parameters
        ----------
        size : int, default=5
            The scale of the plot.
        &#39;&#39;&#39;
        plt.figure(figsize=(size,size))

        # Ticks
        dimensions = self.model.state_grid.shape
        plt.xticks([i for i in range(dimensions[1])])
        plt.yticks([i for i in range(dimensions[0])])

        ax = plt.gca()
        ax.invert_yaxis()

        # Actual plotting
        data = np.array(self.grid_point_sequence)
        plt.plot(data[:,1], data[:,0], color=&#39;red&#39;)
        plt.scatter(data[:,1], data[:,0], color=&#39;red&#39;)
        plt.show()


    def _plot_to_frame_on_ax(self, frame_i, ax):
        # Data
        data = np.array(self.grid_point_sequence)[:(frame_i+1),:]

        # Ticks
        dimensions = self.model.state_grid.shape
        x_ticks = [i for i in range(dimensions[1])]
        y_ticks = [i for i in range(dimensions[0])]

        # Plotting
        ax.clear()
        ax.set_title(f&#39;Simulation (Frame {frame_i})&#39;)

        ax.plot(data[:,1], data[:,0], color=&#39;red&#39;)
        ax.scatter(data[:,1], data[:,0], color=&#39;red&#39;)

        ax.set_xticks(x_ticks)
        ax.set_yticks(y_ticks)
        ax.invert_yaxis()


    def save_simulation_video(self, custom_name:Union[str,None]=None, fps:int=1) -&gt; None:
        &#39;&#39;&#39;
        Function to save a video of the simulation history with all the states it passes through.

        Parameters
        ----------
        custom_name : str, optional
            By default, the file name will be a combination of the state count, the action count and the run timestamp. If a custom name is provided, it will be prepended to the rest of the info.
        fps : int, default=1
            The amount of steps per second appearing in the video.
        &#39;&#39;&#39;
        fig = plt.figure()
        ax = plt.gca()
        steps = len(self.states)

        ani = animation.FuncAnimation(fig, (lambda frame_i: self._plot_to_frame_on_ax(frame_i, ax)), frames=steps, interval=500, repeat=False)
        
        # File Title
        solved_time = datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)

        video_title = f&#39;{custom_name}-&#39; if custom_name is not None else &#39;&#39; # Base
        video_title += f&#39;s{self.model.state_count}-a{self.model.action_count}-&#39; # Model params
        video_title += f&#39;{solved_time}.mp4&#39;

        # Video saving
        if not os.path.exists(&#39;./Sim Videos&#39;):
            print(&#39;Folder does not exist yet, creating it...&#39;)
            os.makedirs(&#39;./Sim Videos&#39;)

        writervideo = animation.FFMpegWriter(fps=fps)
        ani.save(&#39;./Sim Videos/&#39; + video_title, writer=writervideo)
        print(f&#39;Video saved at \&#39;Sim Videos/{video_title}\&#39;...&#39;)
        plt.close()</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="src.pomdp.SimulationHistory" href="pomdp.html#src.pomdp.SimulationHistory">SimulationHistory</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.mdp.SimulationHistory.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, action:int, reward, next_state:int) >None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to add a step in the simulation history</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>action</code></strong> :&ensp;<code>int</code></dt>
<dd>The action that was taken by the agent.</dd>
<dt><strong><code>reward</code></strong></dt>
<dd>The reward received by the agent after having taken action.</dd>
<dt><strong><code>next_state</code></strong> :&ensp;<code>int</code></dt>
<dd>The state that was reached by the agent after having taken action.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(self, action:int, reward, next_state:int) -&gt; None:
    &#39;&#39;&#39;
    Function to add a step in the simulation history

    Parameters
    ----------
    action : int
        The action that was taken by the agent.
    reward
        The reward received by the agent after having taken action.
    next_state : int
        The state that was reached by the agent after having taken action.
    &#39;&#39;&#39;
    self.actions.append(action)
    self.rewards.append(reward)
    self.states.append(next_state)
    self.grid_point_sequence.append([i[0] for i in np.where(self.model.state_grid == next_state)])</code></pre>
</details>
</dd>
<dt id="src.mdp.SimulationHistory.plot_simulation_steps"><code class="name flex">
<span>def <span class="ident">plot_simulation_steps</span></span>(<span>self, size:int=5)</span>
</code></dt>
<dd>
<div class="desc"><p>Plotting the path that was taken during the simulation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code>, default=<code>5</code></dt>
<dd>The scale of the plot.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_simulation_steps(self, size:int=5):
    &#39;&#39;&#39;
    Plotting the path that was taken during the simulation.

    Parameters
    ----------
    size : int, default=5
        The scale of the plot.
    &#39;&#39;&#39;
    plt.figure(figsize=(size,size))

    # Ticks
    dimensions = self.model.state_grid.shape
    plt.xticks([i for i in range(dimensions[1])])
    plt.yticks([i for i in range(dimensions[0])])

    ax = plt.gca()
    ax.invert_yaxis()

    # Actual plotting
    data = np.array(self.grid_point_sequence)
    plt.plot(data[:,1], data[:,0], color=&#39;red&#39;)
    plt.scatter(data[:,1], data[:,0], color=&#39;red&#39;)
    plt.show()</code></pre>
</details>
</dd>
<dt id="src.mdp.SimulationHistory.save_simulation_video"><code class="name flex">
<span>def <span class="ident">save_simulation_video</span></span>(<span>self, custom_name:Optional[str]=None, fps:int=1) >None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to save a video of the simulation history with all the states it passes through.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>custom_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>By default, the file name will be a combination of the state count, the action count and the run timestamp. If a custom name is provided, it will be prepended to the rest of the info.</dd>
<dt><strong><code>fps</code></strong> :&ensp;<code>int</code>, default=<code>1</code></dt>
<dd>The amount of steps per second appearing in the video.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_simulation_video(self, custom_name:Union[str,None]=None, fps:int=1) -&gt; None:
    &#39;&#39;&#39;
    Function to save a video of the simulation history with all the states it passes through.

    Parameters
    ----------
    custom_name : str, optional
        By default, the file name will be a combination of the state count, the action count and the run timestamp. If a custom name is provided, it will be prepended to the rest of the info.
    fps : int, default=1
        The amount of steps per second appearing in the video.
    &#39;&#39;&#39;
    fig = plt.figure()
    ax = plt.gca()
    steps = len(self.states)

    ani = animation.FuncAnimation(fig, (lambda frame_i: self._plot_to_frame_on_ax(frame_i, ax)), frames=steps, interval=500, repeat=False)
    
    # File Title
    solved_time = datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)

    video_title = f&#39;{custom_name}-&#39; if custom_name is not None else &#39;&#39; # Base
    video_title += f&#39;s{self.model.state_count}-a{self.model.action_count}-&#39; # Model params
    video_title += f&#39;{solved_time}.mp4&#39;

    # Video saving
    if not os.path.exists(&#39;./Sim Videos&#39;):
        print(&#39;Folder does not exist yet, creating it...&#39;)
        os.makedirs(&#39;./Sim Videos&#39;)

    writervideo = animation.FFMpegWriter(fps=fps)
    ani.save(&#39;./Sim Videos/&#39; + video_title, writer=writervideo)
    print(f&#39;Video saved at \&#39;Sim Videos/{video_title}\&#39;...&#39;)
    plt.close()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.mdp.Solver"><code class="flex name class">
<span>class <span class="ident">Solver</span></span>
</code></dt>
<dd>
<div class="desc"><p>MDP Model Solver - Abstract class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Solver:
    &#39;&#39;&#39;
    MDP Model Solver - Abstract class.
    &#39;&#39;&#39;
    def __init__(self) -&gt; None:
        raise Exception(&#34;Not an implementable class, please use a subclass...&#34;)
    
    def solve(self, model: Model) -&gt; tuple[ValueFunction, SolverHistory]:
        raise Exception(&#34;Method has to be implemented by subclass...&#34;)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="src.mdp.VI_Solver" href="#src.mdp.VI_Solver">VI_Solver</a></li>
<li><a title="src.pomdp.Solver" href="pomdp.html#src.pomdp.Solver">Solver</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.mdp.Solver.solve"><code class="name flex">
<span>def <span class="ident">solve</span></span>(<span>self, model:<a title="src.mdp.Model" href="#src.mdp.Model">Model</a>) >tuple[<a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a>,<a title="src.mdp.SolverHistory" href="#src.mdp.SolverHistory">SolverHistory</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve(self, model: Model) -&gt; tuple[ValueFunction, SolverHistory]:
    raise Exception(&#34;Method has to be implemented by subclass...&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.mdp.SolverHistory"><code class="flex name class">
<span>class <span class="ident">SolverHistory</span></span>
<span>(</span><span>tracking_level:int, model:<a title="src.mdp.Model" href="#src.mdp.Model">Model</a>, gamma:float, eps:float, initial_value_function:Optional[<a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a>]=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to represent the solving history of a solver.
The purpose of this class is to allow plotting of the solution and plotting the evolution of the value function over the training process.
This class is not meant to be instanciated manually, it meant to be used when returned by the solve() method of a Solver object.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tracking_level</code></strong> :&ensp;<code>int</code></dt>
<dd>The tracking level of the solver.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>The model that has been solved by the Solver.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>The gamma parameter used by the solver (learning rate).</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code></dt>
<dd>The epsilon parameter used by the solver (covergence bound).</dd>
<dt><strong><code>initial_value_function</code></strong> :&ensp;<code><a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a></code>, optional</dt>
<dd>The initial value function the solver will use to start the solving process.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>tracking_level</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>run_ts</code></strong> :&ensp;<code>datetime</code></dt>
<dd>The time at which the SolverHistory object was instantiated which is assumed to be the start of the solving run.</dd>
<dt><strong><code>iteration_times</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>A list of recorded iteration times.</dd>
<dt><strong><code>value_function_changes</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>A list of recorded value function changes (the maximum changed value between 2 value functions).</dd>
<dt><strong><code>value_functions</code></strong> :&ensp;<code>list[<a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a>]</code></dt>
<dd>A list of recorded value functions.</dd>
<dt><strong><code>solution</code></strong> :&ensp;<code><a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a></code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>summary</code></strong> :&ensp;<code>str</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SolverHistory:
    &#39;&#39;&#39;
    Class to represent the solving history of a solver.
    The purpose of this class is to allow plotting of the solution and plotting the evolution of the value function over the training process.
    This class is not meant to be instanciated manually, it meant to be used when returned by the solve() method of a Solver object.

    ...

    Parameters
    ----------
    tracking_level : int
        The tracking level of the solver.
    model : mdp.Model
        The model that has been solved by the Solver.
    gamma : float
        The gamma parameter used by the solver (learning rate).
    eps : float
        The epsilon parameter used by the solver (covergence bound).
    initial_value_function : ValueFunction, optional
        The initial value function the solver will use to start the solving process.
    
    Attributes
    ----------
    tracking_level : int
    model : mdp.Model
    gamma : float
    eps : float
    run_ts : datetime
        The time at which the SolverHistory object was instantiated which is assumed to be the start of the solving run.
    iteration_times : list[float]
        A list of recorded iteration times.
    value_function_changes : list[float]
        A list of recorded value function changes (the maximum changed value between 2 value functions).
    value_functions : list[ValueFunction]
        A list of recorded value functions.
    solution : ValueFunction
    summary : str
    &#39;&#39;&#39;
    def __init__(self,
                 tracking_level:int,
                 model:Model,
                 gamma:float,
                 eps:float,
                 initial_value_function:Union[ValueFunction,None]=None
                 ):
        self.tracking_level = tracking_level
        self.model = model
        self.gamma = gamma
        self.eps = eps
        self.run_ts = datetime.now()

        # Tracking metrics
        self.iteration_times = []
        self.value_function_changes = []

        self.value_functions = []
        if self.tracking_level &gt;= 2:
            self.value_functions.append(initial_value_function)


    @property
    def solution(self) -&gt; ValueFunction:
        &#39;&#39;&#39;
        The last value function of the solving process.
        &#39;&#39;&#39;
        assert self.tracking_level &gt;= 2, &#34;Tracking level is set too low, increase it to 2 if you want to have value function tracking as well.&#34;
        return self.value_functions[-1]
    

    def add(self,
            iteration_time:float,
            value_function_change:float,
            value_function:ValueFunction
            ) -&gt; None:
        &#39;&#39;&#39;
        Function to add a step in the simulation history.

        Parameters
        ----------
        iteration_time : float
            The time it took to run the iteration.
        value_function_change : float
            The change between the value function of this iteration and of the previous iteration.
        value_function : ValueFunction
            The value function resulting after a step of the solving process.
        &#39;&#39;&#39;
        if self.tracking_level &gt;= 1:
            self.iteration_times.append(float(iteration_time))
            self.value_function_changes.append(float(value_function_change))

        if self.tracking_level &gt;= 2:
            self.value_functions.append(value_function if not value_function.is_on_gpu else value_function.to_cpu())
    

    @property
    def summary(self) -&gt; str:
        &#39;&#39;&#39;
        A summary as a string of the information recorded.
        &#39;&#39;&#39;
        summary_str =  f&#39;Summary of Value Iteration run&#39;
        summary_str += f&#39;\n  - Model: {self.model.state_count}-state, {self.model.action_count}-action&#39;
        summary_str += f&#39;\n  - Converged in {len(self.iteration_times)} iterations and {sum(self.iteration_times):.4f} seconds&#39;
        
        if self.tracking_level &gt;= 1:
            summary_str += f&#39;\n  - Took on average {sum(self.iteration_times) / len(self.iteration_times):.4f}s per iteration&#39;
        
        return summary_str
    

    def plot_changes(self) -&gt; None:
        &#39;&#39;&#39;
        Function to plot the value function changes over the solving process.
        &#39;&#39;&#39;
        assert self.tracking_level &gt;= 1, &#34;To plot the change of the value function over time, use tracking level 1 or higher.&#34;
        plt.plot(np.arange(len(self.value_function_changes)), self.value_function_changes)
        plt.show()</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="src.mdp.SolverHistory.solution"><code class="name">var <span class="ident">solution</span> :<a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a></code></dt>
<dd>
<div class="desc"><p>The last value function of the solving process.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def solution(self) -&gt; ValueFunction:
    &#39;&#39;&#39;
    The last value function of the solving process.
    &#39;&#39;&#39;
    assert self.tracking_level &gt;= 2, &#34;Tracking level is set too low, increase it to 2 if you want to have value function tracking as well.&#34;
    return self.value_functions[-1]</code></pre>
</details>
</dd>
<dt id="src.mdp.SolverHistory.summary"><code class="name">var <span class="ident">summary</span> :str</code></dt>
<dd>
<div class="desc"><p>A summary as a string of the information recorded.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def summary(self) -&gt; str:
    &#39;&#39;&#39;
    A summary as a string of the information recorded.
    &#39;&#39;&#39;
    summary_str =  f&#39;Summary of Value Iteration run&#39;
    summary_str += f&#39;\n  - Model: {self.model.state_count}-state, {self.model.action_count}-action&#39;
    summary_str += f&#39;\n  - Converged in {len(self.iteration_times)} iterations and {sum(self.iteration_times):.4f} seconds&#39;
    
    if self.tracking_level &gt;= 1:
        summary_str += f&#39;\n  - Took on average {sum(self.iteration_times) / len(self.iteration_times):.4f}s per iteration&#39;
    
    return summary_str</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.mdp.SolverHistory.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, iteration_time:float, value_function_change:float, value_function:<a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a>) >None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to add a step in the simulation history.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>iteration_time</code></strong> :&ensp;<code>float</code></dt>
<dd>The time it took to run the iteration.</dd>
<dt><strong><code>value_function_change</code></strong> :&ensp;<code>float</code></dt>
<dd>The change between the value function of this iteration and of the previous iteration.</dd>
<dt><strong><code>value_function</code></strong> :&ensp;<code><a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a></code></dt>
<dd>The value function resulting after a step of the solving process.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(self,
        iteration_time:float,
        value_function_change:float,
        value_function:ValueFunction
        ) -&gt; None:
    &#39;&#39;&#39;
    Function to add a step in the simulation history.

    Parameters
    ----------
    iteration_time : float
        The time it took to run the iteration.
    value_function_change : float
        The change between the value function of this iteration and of the previous iteration.
    value_function : ValueFunction
        The value function resulting after a step of the solving process.
    &#39;&#39;&#39;
    if self.tracking_level &gt;= 1:
        self.iteration_times.append(float(iteration_time))
        self.value_function_changes.append(float(value_function_change))

    if self.tracking_level &gt;= 2:
        self.value_functions.append(value_function if not value_function.is_on_gpu else value_function.to_cpu())</code></pre>
</details>
</dd>
<dt id="src.mdp.SolverHistory.plot_changes"><code class="name flex">
<span>def <span class="ident">plot_changes</span></span>(<span>self) >None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to plot the value function changes over the solving process.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_changes(self) -&gt; None:
    &#39;&#39;&#39;
    Function to plot the value function changes over the solving process.
    &#39;&#39;&#39;
    assert self.tracking_level &gt;= 1, &#34;To plot the change of the value function over time, use tracking level 1 or higher.&#34;
    plt.plot(np.arange(len(self.value_function_changes)), self.value_function_changes)
    plt.show()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.mdp.VI_Solver"><code class="flex name class">
<span>class <span class="ident">VI_Solver</span></span>
<span>(</span><span>horizon:int=10000, gamma:float=0.99, eps:float=0.001)</span>
</code></dt>
<dd>
<div class="desc"><p>Solver for MDP Models. This solver implements Value Iteration.
It works by iteratively updating the value function that maps states to actions.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>horizon</code></strong> :&ensp;<code>int</code>, default=<code>1000</code></dt>
<dd>Controls for how many epochs the learning can run for (works as an infinite loop safety).</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code>, default=<code>0.99</code></dt>
<dd>Controls the learning rate, how fast the rewards are discounted at each epoch.</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, default=<code>0.001</code></dt>
<dd>Controls the threshold to determine whether the value functions has settled. If the max change of value for a state is lower than eps, then it has converged.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>horizon</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VI_Solver(Solver):
    &#39;&#39;&#39;
    Solver for MDP Models. This solver implements Value Iteration.
    It works by iteratively updating the value function that maps states to actions.
    
    ...

    Parameters
    ----------
    horizon : int, default=1000
        Controls for how many epochs the learning can run for (works as an infinite loop safety).
    gamma : float, default=0.99
        Controls the learning rate, how fast the rewards are discounted at each epoch.
    eps : float, default=0.001
        Controls the threshold to determine whether the value functions has settled. If the max change of value for a state is lower than eps, then it has converged.
        
    Attributes
    ----------
    horizon : int
    gamma : float
    eps : float
    &#39;&#39;&#39;
    def __init__(self, horizon:int=10000, gamma:float=0.99, eps:float=0.001):
        self.horizon = horizon
        self.gamma = gamma
        self.eps = eps


    def solve(self, 
              model: Model,
              initial_value_function:Union[ValueFunction,None]=None,
              use_gpu:bool=False,
              history_tracking_level:int=1,
              print_progress:bool=True
              ) -&gt; tuple[ValueFunction, SolverHistory]:
        &#39;&#39;&#39;
        Function to solve an MDP model using Value Iteration.
        If an initial value function is not provided, the value function will be initiated with the expected rewards.

        Parameters
        ----------
        model : mdp.Model
            The model on which to run value iteration.
        initial_value_function : ValueFunction, optional
            An optional initial value function to kick-start the value iteration process.
        use_gpu : bool, default=False
            Whether to use the GPU with cupy array to accelerate solving.
        history_tracking_level : int, default=1
            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)
        print_progress : bool, default=True
            Whether or not to print out the progress of the value iteration process.

        Returns
        -------
        value_function: ValueFunction
            The resulting value function solution to the model.
        history : SolverHistory
            The tracking of the solution over time.
        &#39;&#39;&#39;
        # numpy or cupy module
        xp = np

        # If GPU usage
        if use_gpu:
            assert gpu_support, &#34;GPU support is not enabled, Cupy might need to be installed...&#34;
            model = model.gpu_model

            # Replace numpy module by cupy for computations
            xp = cp

        # Value function initialization
        if initial_value_function is None:
            V = ValueFunction(model, model.expected_rewards_table.T, model.actions)
        else:
            V = initial_value_function.to_gpu() if use_gpu else initial_value_function
        V_opt = xp.max(V.alpha_vector_array, axis=0)

        # History tracking setup
        solve_history = SolverHistory(tracking_level=history_tracking_level,
                                      model=model,
                                      gamma=self.gamma,
                                      eps=self.eps,
                                      initial_value_function=V)

        # Computing max allowed change from epsilon and gamma parameters
        max_allowed_change = self.eps * (self.gamma / (1-self.gamma))

        for _ in trange(self.horizon) if print_progress else range(self.horizon):
            old_V_opt = V_opt
            
            start = datetime.now()

            # Computing the new alpha vectors
            alpha_vectors = model.expected_rewards_table.T + (self.gamma * xp.einsum(&#39;sar,sar-&gt;as&#39;, model.reachable_probabilities, V_opt[model.reachable_states]))
            V = ValueFunction(model, alpha_vectors, model.actions)

            V_opt = xp.max(V.alpha_vector_array, axis=0)
            
            # Change computation
            max_change = xp.max(xp.abs(V_opt - old_V_opt))

            # Tracking the history
            iteration_time = (datetime.now() - start).total_seconds()
            solve_history.add(iteration_time=iteration_time,
                              value_function_change=max_change,
                              value_function=V)

            # Convergence check
            if max_change &lt; max_allowed_change:
                break

        return V, solve_history</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="src.mdp.Solver" href="#src.mdp.Solver">Solver</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.mdp.VI_Solver.solve"><code class="name flex">
<span>def <span class="ident">solve</span></span>(<span>self, model:<a title="src.mdp.Model" href="#src.mdp.Model">Model</a>, initial_value_function:Optional[<a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a>]=None, use_gpu:bool=False, history_tracking_level:int=1, print_progress:bool=True) >tuple[<a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a>,<a title="src.mdp.SolverHistory" href="#src.mdp.SolverHistory">SolverHistory</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Function to solve an MDP model using Value Iteration.
If an initial value function is not provided, the value function will be initiated with the expected rewards.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>The model on which to run value iteration.</dd>
<dt><strong><code>initial_value_function</code></strong> :&ensp;<code><a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a></code>, optional</dt>
<dd>An optional initial value function to kick-start the value iteration process.</dd>
<dt><strong><code>use_gpu</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to use the GPU with cupy array to accelerate solving.</dd>
<dt><strong><code>history_tracking_level</code></strong> :&ensp;<code>int</code>, default=<code>1</code></dt>
<dd>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</dd>
<dt><strong><code>print_progress</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether or not to print out the progress of the value iteration process.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>value_function</code></strong> :&ensp;<code><a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a></code></dt>
<dd>The resulting value function solution to the model.</dd>
<dt><strong><code>history</code></strong> :&ensp;<code><a title="src.mdp.SolverHistory" href="#src.mdp.SolverHistory">SolverHistory</a></code></dt>
<dd>The tracking of the solution over time.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve(self, 
          model: Model,
          initial_value_function:Union[ValueFunction,None]=None,
          use_gpu:bool=False,
          history_tracking_level:int=1,
          print_progress:bool=True
          ) -&gt; tuple[ValueFunction, SolverHistory]:
    &#39;&#39;&#39;
    Function to solve an MDP model using Value Iteration.
    If an initial value function is not provided, the value function will be initiated with the expected rewards.

    Parameters
    ----------
    model : mdp.Model
        The model on which to run value iteration.
    initial_value_function : ValueFunction, optional
        An optional initial value function to kick-start the value iteration process.
    use_gpu : bool, default=False
        Whether to use the GPU with cupy array to accelerate solving.
    history_tracking_level : int, default=1
        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)
    print_progress : bool, default=True
        Whether or not to print out the progress of the value iteration process.

    Returns
    -------
    value_function: ValueFunction
        The resulting value function solution to the model.
    history : SolverHistory
        The tracking of the solution over time.
    &#39;&#39;&#39;
    # numpy or cupy module
    xp = np

    # If GPU usage
    if use_gpu:
        assert gpu_support, &#34;GPU support is not enabled, Cupy might need to be installed...&#34;
        model = model.gpu_model

        # Replace numpy module by cupy for computations
        xp = cp

    # Value function initialization
    if initial_value_function is None:
        V = ValueFunction(model, model.expected_rewards_table.T, model.actions)
    else:
        V = initial_value_function.to_gpu() if use_gpu else initial_value_function
    V_opt = xp.max(V.alpha_vector_array, axis=0)

    # History tracking setup
    solve_history = SolverHistory(tracking_level=history_tracking_level,
                                  model=model,
                                  gamma=self.gamma,
                                  eps=self.eps,
                                  initial_value_function=V)

    # Computing max allowed change from epsilon and gamma parameters
    max_allowed_change = self.eps * (self.gamma / (1-self.gamma))

    for _ in trange(self.horizon) if print_progress else range(self.horizon):
        old_V_opt = V_opt
        
        start = datetime.now()

        # Computing the new alpha vectors
        alpha_vectors = model.expected_rewards_table.T + (self.gamma * xp.einsum(&#39;sar,sar-&gt;as&#39;, model.reachable_probabilities, V_opt[model.reachable_states]))
        V = ValueFunction(model, alpha_vectors, model.actions)

        V_opt = xp.max(V.alpha_vector_array, axis=0)
        
        # Change computation
        max_change = xp.max(xp.abs(V_opt - old_V_opt))

        # Tracking the history
        iteration_time = (datetime.now() - start).total_seconds()
        solve_history.add(iteration_time=iteration_time,
                          value_function_change=max_change,
                          value_function=V)

        # Convergence check
        if max_change &lt; max_allowed_change:
            break

    return V, solve_history</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.mdp.ValueFunction"><code class="flex name class">
<span>class <span class="ident">ValueFunction</span></span>
<span>(</span><span>model:<a title="src.mdp.Model" href="#src.mdp.Model">Model</a>, alpha_vectors:Union[list[<a title="src.mdp.AlphaVector" href="#src.mdp.AlphaVector">AlphaVector</a>],numpy.ndarray]=[], action_list:list[int]=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Class representing a set of AlphaVectors. One such set approximates the value function of the MDP model.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>The model the value function is associated with.</dd>
<dt><strong><code>alpha_vectors</code></strong> :&ensp;<code>list[<a title="src.mdp.AlphaVector" href="#src.mdp.AlphaVector">AlphaVector</a>]</code> or <code>np.ndarray</code>, optional</dt>
<dd>The alpha vectors composing the value function, if none are provided, it will be empty to start with and AlphaVectors can be appended.</dd>
<dt><strong><code>action_list</code></strong> :&ensp;<code>list[int]</code>, optional</dt>
<dd>The actions associated with alpha vectors in the case the alpha vectors are provided as an numpy array.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>The model the value function is associated with.</dd>
<dt><strong><code>alpha_vector_list</code></strong> :&ensp;<code>list[<a title="src.mdp.AlphaVector" href="#src.mdp.AlphaVector">AlphaVector</a>]</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>alpha_vector_array</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>actions</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ValueFunction:
    &#39;&#39;&#39;
    Class representing a set of AlphaVectors. One such set approximates the value function of the MDP model.

    ...

    Parameters
    ----------
    model : mdp.Model
        The model the value function is associated with.
    alpha_vectors : list[AlphaVector] or np.ndarray, optional
        The alpha vectors composing the value function, if none are provided, it will be empty to start with and AlphaVectors can be appended.
    action_list : list[int], optional
        The actions associated with alpha vectors in the case the alpha vectors are provided as an numpy array.
    
    Attributes
    ----------
    model : mdp.Model
        The model the value function is associated with.
    alpha_vector_list : list[AlphaVector]
    alpha_vector_array : np.ndarray
    actions : list[int]
    &#39;&#39;&#39;
    def __init__(self, model:Model, alpha_vectors:Union[list[AlphaVector], np.ndarray]=[], action_list:list[int]=[]):
        self.model = model

        self._vector_list = None
        self._vector_array = None
        self._actions = None

        self.is_on_gpu = False

        # List of alpha vectors
        if isinstance(alpha_vectors, list):
            assert all(v.values.shape[0] == model.state_count for v in alpha_vectors), f&#34;Some or all alpha vectors in the list provided dont have the right size, they should be of shape: {model.state_count}&#34;
            self._vector_list = alpha_vectors
            
            # Check if on gpu and make sure all vectors are also on the gpu
            if (len(alpha_vectors) &gt; 0) and gpu_support and cp.get_array_module(alpha_vectors[0].values) == cp:
                assert all(cp.get_array_module(v.values) == cp for v in alpha_vectors), &#34;Either all or none of the alpha vectors should be on the GPU, not just some.&#34;
                self.is_on_gpu = True
        
        # As numpy array
        else:
            av_shape = alpha_vectors.shape
            exp_shape = (len(action_list), model.state_count)
            assert av_shape == exp_shape, f&#34;Alpha vector array does not have the right shape (received: {av_shape}; expected: {exp_shape})&#34;

            self._vector_array = alpha_vectors
            self._actions = action_list

            # Check if array is on gpu
            if gpu_support and cp.get_array_module(alpha_vectors) == cp:
                self.is_on_gpu = True


    @property
    def alpha_vector_list(self) -&gt; list[AlphaVector]:
        &#39;&#39;&#39;
        A list of AlphaVector objects. If the value function is defined as an matrix of vectors and a list of actions, the list of AlphaVectors will be generated from them.
        &#39;&#39;&#39;
        if self._vector_list is None:
            self._vector_list = []
            for alpha_vect, action in zip(self._vector_array, self._actions):
                self._vector_list.append(AlphaVector(alpha_vect, action))
        return self._vector_list
    

    @property
    def alpha_vector_array(self) -&gt; np.ndarray:
        &#39;&#39;&#39;
        A matrix of size N x S, containing all the alpha vectors making up the value function. (N is the number of alpha vectors and S the amount of states in the model)
        If the value function is defined as a list of AlphaVector objects, the matrix will the generated from them.
        &#39;&#39;&#39;
        xp = cp if (gpu_support and self.is_on_gpu) else np

        if self._vector_array is None:
            self._vector_array = xp.array([v.values for v in self._vector_list])
            self._actions = [v.action for v in self._vector_list]
        return self._vector_array
    

    @property
    def actions(self) -&gt; list[int]:
        &#39;&#39;&#39;
        A list of N actions corresponding to the N alpha vectors making up the value function.
        If the value function is defined as a list of AlphaVector objects, the list will the generated from the actions of those alpha vector objects.
        &#39;&#39;&#39;
        xp = cp if (gpu_support and self.is_on_gpu) else np

        if self._actions is None:
            self._vector_array = xp.array(self._vector_list)
            self._actions = [v.action for v in self._vector_list]
        return self._actions
    

    def __len__(self) -&gt; int:
        return len(self._vector_list) if self._vector_list is not None else self._vector_array.shape[0]
    

    def append(self, alpha_vector:AlphaVector) -&gt; None:
        &#39;&#39;&#39;
        Function to add an alpha vector to the value function.

        Parameters
        ----------
        alpha_vector : AlphaVector
            The alpha vector to be added to the value function.
        &#39;&#39;&#39;
        # Make sure size is correct
        assert alpha_vector.values.shape[0] == self.model.state_count, f&#34;Vector to add to value function doesn&#39;t have the right size (received: {alpha_vector.values.shape[0]}, expected: {self.model.state_count})&#34;
        
        # GPU support check
        xp = cp if (gpu_support and self.is_on_gpu) else np
        assert gpu_support and cp.get_array_module(alpha_vector.values) == xp, f&#34;Vector is{&#39; not&#39; if self.is_on_gpu else &#39;&#39;} on GPU while value function is{&#39;&#39; if self.is_on_gpu else &#39; not&#39;}.&#34;

        if self._vector_array is not None:
            self._vector_array = xp.append(self._vector_array, alpha_vector[None,:], axis=0)
            self._actions.append(alpha_vector.action)
        
        if self._vector_list is not None:
            self._vector_list.append(alpha_vector)


    def to_gpu(self) -&gt; Self:
        &#39;&#39;&#39;
        Function returning an equivalent value function object with the arrays stored on GPU instead of CPU.

        Returns
        -------
        gpu_value_function : ValueFunction
            A new value function with arrays on GPU.
        &#39;&#39;&#39;
        assert gpu_support, &#34;GPU support is not enabled, unable to execute this function&#34;

        gpu_model = self.model.gpu_model

        gpu_value_function = None
        if self._vector_array is not None:
            gpu_vector_array = cp.array(self._vector_array)
            gpu_actions = self._actions if isinstance(self._actions, list) else cp.array(self._actions)
            gpu_value_function = ValueFunction(gpu_model, gpu_vector_array, gpu_actions)
        
        else:
            gpu_alpha_vectors = [AlphaVector(cp.array(av.values), av.action) for av in self._vector_list]
            gpu_value_function = ValueFunction(gpu_model, gpu_alpha_vectors)

        return gpu_value_function
    

    def to_cpu(self) -&gt; Self:
        &#39;&#39;&#39;
        Function returning an equivalent value function object with the arrays stored on CPU instead of GPU.

        Returns
        -------
        cpu_value_function : ValueFunction
            A new value function with arrays on CPU.
        &#39;&#39;&#39;
        assert gpu_support, &#34;GPU support is not enabled, unable to execute this function&#34;

        cpu_model = self.model.cpu_model

        cpu_value_function = None
        if self._vector_array is not None:
            cpu_vector_array = cp.asnumpy(self._vector_array)
            cpu_actions = self._actions if isinstance(self._actions, list) else cp.asnumpy(self._actions)
            cpu_value_function = ValueFunction(cpu_model, cpu_vector_array, cpu_actions)
        
        else:
            cpu_alpha_vectors = [AlphaVector(cp.asnumpy(av.values), av.action) for av in self._vector_list]
            cpu_value_function = ValueFunction(cpu_model, cpu_alpha_vectors)

        return cpu_value_function


    def prune(self, level:int=1) -&gt; Self:
        &#39;&#39;&#39;
        Function returning a new value function with the set of alpha vector composing it being it pruned.
        The pruning is as thorough as the level:
            - 0: No pruning, returns a value function with the alpha vector set being an exact copy of the current one.
            - 1: Simple deduplication of the alpha vectors.
            - 2: 1+ Check of absolute domination (check if dominated at each state).
            - 3: 2+ Solves Linear Programming problem for each alpha vector to see if it is dominated by combinations of other vectors.
        
        Note that the higher the level, the heavier the time impact will be.

        Parameters
        ----------
        level : int, default=1
            Between 0 and 3, how thorough the alpha vector pruning should be.
            
        Returns
        -------
        new_value_function : ValueFunction
            A new value function with a pruned set of alpha vectors.
        &#39;&#39;&#39;
        # GPU support check
        xp = cp if (gpu_support and self.is_on_gpu) else np

        if level &lt; 1:
            return ValueFunction(self.model, xp.copy(self))
        
        # Level 1 pruning: Check for duplicates - works equally for cupy array (on gpu)
        L = {alpha_vector.values.tobytes(): alpha_vector for alpha_vector in self.alpha_vector_list}
        pruned_alpha_set = ValueFunction(self.model, list(L.values()))

        # Level 2 pruning: Check for absolute domination
        if level &gt;= 2:
            # Beyond this point, gpu can&#39;t be used due to the functions used so if on gpu, converting it back to cpu
            if pruned_alpha_set.is_on_gpu:
                pruned_alpha_set = pruned_alpha_set.to_cpu()

            alpha_vector_array = pruned_alpha_set.alpha_vector_array
            X = cdist(alpha_vector_array, alpha_vector_array, metric=(lambda a,b:(a &lt;= b).all() and not (a == b).all())).astype(bool)
            non_dominated_vector_indices = np.invert(X).all(axis=1)

            non_dominated_vectors = alpha_vector_array[non_dominated_vector_indices]
            non_dominated_actions = np.array(pruned_alpha_set.actions)[non_dominated_vector_indices].tolist()

            pruned_alpha_set = ValueFunction(self.model, non_dominated_vectors, non_dominated_actions)

        # Level 3 pruning: LP to check for more complex domination
        if level &gt;= 3:
            alpha_set = pruned_alpha_set.alpha_vector_list
            pruned_alpha_set = ValueFunction(self.model)

            for i, alpha_vect in enumerate(alpha_set):
                other_alphas = alpha_set[:i] + alpha_set[(i+1):]

                # Objective function
                c = np.concatenate([np.array([1]), -1*alpha_vect])

                # Alpha vector contraints
                other_count = len(other_alphas)
                A = np.c_[np.ones(other_count), np.multiply(np.array(other_alphas), -1)]
                alpha_constraints = LinearConstraint(A, 0, np.inf)

                # Constraints that sum of beliefs is 1
                belief_constraint = LinearConstraint(np.array([0] + ([1]*self.model.state_count)), 1, 1)

                # Solve problem
                res = milp(c=c, constraints=[alpha_constraints, belief_constraint])

                # Check if dominated
                is_dominated = (res.x[0] - np.dot(res.x[1:], alpha_vect)) &gt;= 0
                if is_dominated:
                    print(alpha_vect)
                    print(&#39; -&gt; Dominated\n&#39;)
                else:
                    pruned_alpha_set.append(alpha_vect)
        
        # If initial value function was on gpu, and intermediate array was converted to cpu, convert it back to gpu
        if self.is_on_gpu and not pruned_alpha_set.is_on_gpu:
            pruned_alpha_set = pruned_alpha_set.to_cpu()

        return pruned_alpha_set
    

    def save(self, path:str=&#39;./ValueFunctions&#39;, file_name:Union[str,None]=None) -&gt; None:
        &#39;&#39;&#39;
        Function to save the save function in a file at a given path. If no path is provided, it will be saved in a subfolder (ValueFunctions) inside the current working directory.
        If no file_name is provided, it be saved as &#39;&lt;current_timestamp&gt;_value_function.csv&#39;.

        Parameters
        ----------
        path : str, default=&#39;./ValueFunctions&#39;
            The path at which the csv will be saved.
        file_name : str, default=&#39;&lt;current_timestamp&gt;_value_function.csv&#39;
            The file name used to save in.
        &#39;&#39;&#39;
        if not os.path.exists(path):
            print(&#39;Folder does not exist yet, creating it...&#39;)
            os.makedirs(path)
            
        if file_name is None:
            timestamp = datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)
            file_name = timestamp + &#39;_value_function.csv&#39;

        vector_array = self.alpha_vector_array
        actions = self.actions

        # Convert arrays to numpy if on gpu
        if self.is_on_gpu:
            vector_array = cp.asnumpy(vector_array)
            actions = cp.asnumpy(actions)

        data = np.concatenate((np.array(self.actions)[:,None], self.alpha_vector_array), axis=1)
        columns = [&#39;action&#39;, *self.model.state_labels]

        df = pd.DataFrame(data)
        df.to_csv(path + &#39;/&#39; + file_name, index=False, header=columns)


    @classmethod
    def load_from_file(cls, file:str, model:Model) -&gt; Self:
        &#39;&#39;&#39;
        Function to load the value function from a csv file.

        Parameters
        ----------
        file : str
            The path and file_name of the value function to be loaded.
        model : mdp.Model
            The model the value function is linked to.
            
        Returns
        -------
        loaded_value_function : ValueFunction
            The loaded value function.
        &#39;&#39;&#39;
        df = pd.read_csv(file, header=0, index_col=False)
        alpha_vectors = df.to_numpy()

        return ValueFunction(model, alpha_vectors[:,1:], alpha_vectors[:,0].astype(int))


    def plot(self,
             as_grid:bool=False,
             size:int=5,
             belief_set:np.ndarray=None
             ) -&gt; None:
        &#39;&#39;&#39;
        Function to plot out the value function in 2 or 3 dimensions if possible and the as_grid parameter is kept to false. Else, the value function is plot as a grid.
        If a belief set array is provided and the model is a 2- or 3-model, it will be plot alongside the value function.

        Parameters
        ----------
        as_grid : bool, default=False
            Forces the plot to be plot as a grid.
        size : int, default=5
            The actual plot scale.
        belief_set : np.ndarray, optional
            A set of belief to plot the belief points that were explored.
        &#39;&#39;&#39;
        assert len(self) &gt; 0, &#34;Value function is empty, plotting is impossible...&#34;
        
        # If on GPU, convert to CPU and plot that one
        if self.is_on_gpu:
            print(&#39;[Warning] Value function on GPU, converting to numpy before plotting...&#39;)
            cpu_value_function = self.to_cpu()
            cpu_value_function.plot(as_grid, size, belief_set)
            return

        func = None
        if as_grid:
            func = self._plot_grid
        elif self.model.state_count == 2:
            func = self._plot_2D
        elif self.model.state_count == 3:
            func = self._plot_3D
        else:
            print(&#39;[Warning] \&#39;as_grid\&#39; parameter set to False but state count is &gt;3 so it will be plotted as a grid&#39;)
            func = self._plot_grid

        func(size, belief_set)


    def _plot_2D(self, size, belief_set=None):
        x = np.linspace(0, 1, 100)

        plt.figure(figsize=(int(size*1.5),size))
        grid_spec = {&#39;height_ratios&#39;: ([1] if belief_set is None else [19,1])}
        _, ax = plt.subplots((2 if belief_set is not None else 1),1,sharex=True,gridspec_kw=grid_spec)

        # Vector plotting
        alpha_vects = self.alpha_vector_array

        m = alpha_vects[:,1] - alpha_vects[:,0] # type: ignore
        m = m.reshape(m.shape[0],1)

        x = x.reshape((1,x.shape[0])).repeat(m.shape[0],axis=0)
        y = (m*x) + alpha_vects[:,0].reshape(m.shape[0],1)

        ax1 = ax[0] if belief_set is not None else ax
        for i, alpha in enumerate(self.alpha_vector_list):
            ax1.plot(x[i,:], y[i,:], color=COLOR_LIST[alpha.action][&#39;id&#39;]) # type: ignore

        # X-axis setting
        ticks = [0,0.25,0.5,0.75,1]
        x_ticks = [str(t) for t in ticks]
        x_ticks[0] = self.model.state_labels[0]
        x_ticks[-1] = self.model.state_labels[1]

        ax1.set_xticks(ticks, x_ticks) # type: ignore

        # Action legend
        proxy = [patches.Rectangle((0,0),1,1,fc = COLOR_LIST[a][&#39;id&#39;]) for a in self.model.actions]
        ax1.legend(proxy, self.model.action_labels) # type: ignore

        # Belief plotting
        if belief_set is not None:
            beliefs_x = belief_set.belief_array[:,1]
            ax[1].scatter(beliefs_x, np.zeros(beliefs_x.shape[0]), c=&#39;red&#39;)
            ax[1].get_yaxis().set_visible(False)
            ax[1].axhline(0, color=&#39;black&#39;)


    def _plot_3D(self, size, belief_set=None):

        def get_alpha_vect_z(xx, yy, alpha_vect):
            x0, y0, z0 = [0, 0, alpha_vect[0]]
            x1, y1, z1 = [1, 0, alpha_vect[1]]
            x2, y2, z2 = [0, 1, alpha_vect[2]]

            ux, uy, uz = u = [x1-x0, y1-y0, z1-z0]
            vx, vy, vz = v = [x2-x0, y2-y0, z2-z0]

            u_cross_v = [uy*vz-uz*vy, uz*vx-ux*vz, ux*vy-uy*vx]

            point  = np.array([0, 0, alpha_vect[0]])
            normal = np.array(u_cross_v)

            d = -point.dot(normal)

            z = (-normal[0] * xx - normal[1] * yy - d) * 1. / normal[2]
            
            return z

        def get_plane_gradient(alpha_vect):
        
            x0, y0, z0 = [0, 0, alpha_vect[0]]
            x1, y1, z1 = [1, 0, alpha_vect[1]]
            x2, y2, z2 = [0, 1, alpha_vect[2]]

            ux, uy, uz = u = [x1-x0, y1-y0, z1-z0]
            vx, vy, vz = v = [x2-x0, y2-y0, z2-z0]

            u_cross_v = [uy*vz-uz*vy, uz*vx-ux*vz, ux*vy-uy*vx]
            
            normal_vector = np.array(u_cross_v)
            normal_vector_norm = float(np.linalg.norm(normal_vector))
            normal_vector = np.divide(normal_vector, normal_vector_norm)
            normal_vector[2] = 0
            
            return np.linalg.norm(normal_vector)

        # Actual plotting
        x = np.linspace(0, 1, 1000)
        y = np.linspace(0, 1, 1000)

        xx, yy = np.meshgrid(x, y)

        max_z = np.zeros((xx.shape[0], yy.shape[0]))
        best_a = (np.zeros((xx.shape[0], yy.shape[0])))
        plane = (np.zeros((xx.shape[0], yy.shape[0])))
        gradients = (np.zeros((xx.shape[0], yy.shape[0])))

        for alpha in self.alpha_vector_list:

            z = get_alpha_vect_z(xx, yy, alpha)

            # Action array update
            new_a_mask = np.argmax(np.array([max_z, z]), axis=0)

            best_a[new_a_mask == 1] = alpha.action
            
            plane[new_a_mask == 1] = random.randrange(100)
            
            alpha_gradient = get_plane_gradient(alpha)
            gradients[new_a_mask == 1] = alpha_gradient

            # Max z update
            max_z = np.max(np.array([max_z, z]), axis=0)
            
        for x_i, x_val in enumerate(x):
            for y_i, y_val in enumerate(y):
                if (x_val+y_val) &gt; 1:
                    max_z[x_i, y_i] = np.nan
                    plane[x_i, y_i] = np.nan
                    gradients[x_i, y_i] = np.nan
                    best_a[x_i, y_i] = np.nan

        belief_points = None
        if belief_set is not None:
            belief_points = np.array(belief_set)[:,1:]
                    
        fig, ((ax1, ax2),(ax3,ax4)) = plt.subplots(2, 2, figsize=(size*4,size*3.5), sharex=True, sharey=True)

        # Set ticks
        ticks = [0,0.25,0.5,0.75,1]
        x_ticks = [str(t) for t in ticks]
        x_ticks[0] = self.model.state_labels[0]
        x_ticks[-1] = self.model.state_labels[1]
        
        y_ticks = [str(t) for t in ticks]
        y_ticks[0] = self.model.state_labels[0]
        y_ticks[-1] = self.model.state_labels[2]

        plt.setp([ax1,ax2,ax3,ax4], xticks=ticks, xticklabels=x_ticks, yticks=ticks, yticklabels=y_ticks)

        # Value function ax
        ax1.set_title(&#34;Value function&#34;)
        ax1_plot = ax1.contourf(x, y, max_z, 100, cmap=&#34;viridis&#34;)
        plt.colorbar(ax1_plot, ax=ax1)

        # Alpha planes ax
        ax2.set_title(&#34;Alpha planes&#34;)
        ax2_plot = ax2.contourf(x, y, plane, 100, cmap=&#34;viridis&#34;)
        plt.colorbar(ax2_plot, ax=ax2)
        
        # Gradient of planes ax
        ax3.set_title(&#34;Gradients of planes&#34;)
        ax3_plot = ax3.contourf(x, y, gradients, 100, cmap=&#34;Blues&#34;)
        plt.colorbar(ax3_plot, ax=ax3)

        # Action policy ax
        ax4.set_title(&#34;Action policy&#34;)
        ax4.contourf(x, y, best_a, 1, colors=[c[&#39;id&#39;] for c in COLOR_LIST])
        proxy = [patches.Rectangle((0,0),1,1,fc = COLOR_LIST[int(a)][&#39;id&#39;]) for a in self.model.actions]
        ax4.legend(proxy, self.model.action_labels)

        if belief_points is not None:
            for ax in [ax1,ax2,ax3,ax4]:
                ax.scatter(belief_points[:,0], belief_points[:,1], s=1, c=&#39;black&#39;)


    def _plot_grid(self, size=5, belief_set=None):
        value_table = np.max(self.alpha_vector_array, axis=0)[self.model.state_grid]
        best_action_table = np.array(self.actions)[np.argmax(self.alpha_vector_array, axis=0)][self.model.state_grid]
        best_action_colors = COLOR_ARRAY[best_action_table]

        dimensions = self.model.state_grid.shape

        fig, (ax1,ax2) = plt.subplots(1,2, figsize=(size*2, size), width_ratios=(0.55,0.45))

        # Ticks
        x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))
        y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))

        ax1.set_title(&#39;Value function&#39;)
        ax1_plot = ax1.imshow(value_table)
        plt.colorbar(ax1_plot, ax=ax1)
        ax1.set_xticks(x_ticks)
        ax1.set_yticks(y_ticks)

        ax2.set_title(&#39;Action policy&#39;)
        ax2.imshow(best_action_colors)
        p = [ patches.Patch(color=COLOR_LIST[int(i)][&#39;id&#39;], label=str(self.model.action_labels[int(i)])) for i in self.model.actions]
        ax2.legend(handles=p, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
        ax2.set_xticks(x_ticks)
        ax2.set_yticks(y_ticks)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="src.mdp.ValueFunction.load_from_file"><code class="name flex">
<span>def <span class="ident">load_from_file</span></span>(<span>file:str, model:<a title="src.mdp.Model" href="#src.mdp.Model">Model</a>) >Self</span>
</code></dt>
<dd>
<div class="desc"><p>Function to load the value function from a csv file.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file</code></strong> :&ensp;<code>str</code></dt>
<dd>The path and file_name of the value function to be loaded.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>The model the value function is linked to.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loaded_value_function</code></strong> :&ensp;<code><a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a></code></dt>
<dd>The loaded value function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def load_from_file(cls, file:str, model:Model) -&gt; Self:
    &#39;&#39;&#39;
    Function to load the value function from a csv file.

    Parameters
    ----------
    file : str
        The path and file_name of the value function to be loaded.
    model : mdp.Model
        The model the value function is linked to.
        
    Returns
    -------
    loaded_value_function : ValueFunction
        The loaded value function.
    &#39;&#39;&#39;
    df = pd.read_csv(file, header=0, index_col=False)
    alpha_vectors = df.to_numpy()

    return ValueFunction(model, alpha_vectors[:,1:], alpha_vectors[:,0].astype(int))</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="src.mdp.ValueFunction.actions"><code class="name">var <span class="ident">actions</span> :list[int]</code></dt>
<dd>
<div class="desc"><p>A list of N actions corresponding to the N alpha vectors making up the value function.
If the value function is defined as a list of AlphaVector objects, the list will the generated from the actions of those alpha vector objects.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def actions(self) -&gt; list[int]:
    &#39;&#39;&#39;
    A list of N actions corresponding to the N alpha vectors making up the value function.
    If the value function is defined as a list of AlphaVector objects, the list will the generated from the actions of those alpha vector objects.
    &#39;&#39;&#39;
    xp = cp if (gpu_support and self.is_on_gpu) else np

    if self._actions is None:
        self._vector_array = xp.array(self._vector_list)
        self._actions = [v.action for v in self._vector_list]
    return self._actions</code></pre>
</details>
</dd>
<dt id="src.mdp.ValueFunction.alpha_vector_array"><code class="name">var <span class="ident">alpha_vector_array</span> :numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>A matrix of size N x S, containing all the alpha vectors making up the value function. (N is the number of alpha vectors and S the amount of states in the model)
If the value function is defined as a list of AlphaVector objects, the matrix will the generated from them.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def alpha_vector_array(self) -&gt; np.ndarray:
    &#39;&#39;&#39;
    A matrix of size N x S, containing all the alpha vectors making up the value function. (N is the number of alpha vectors and S the amount of states in the model)
    If the value function is defined as a list of AlphaVector objects, the matrix will the generated from them.
    &#39;&#39;&#39;
    xp = cp if (gpu_support and self.is_on_gpu) else np

    if self._vector_array is None:
        self._vector_array = xp.array([v.values for v in self._vector_list])
        self._actions = [v.action for v in self._vector_list]
    return self._vector_array</code></pre>
</details>
</dd>
<dt id="src.mdp.ValueFunction.alpha_vector_list"><code class="name">var <span class="ident">alpha_vector_list</span> :list[<a title="src.mdp.AlphaVector" href="#src.mdp.AlphaVector">AlphaVector</a>]</code></dt>
<dd>
<div class="desc"><p>A list of AlphaVector objects. If the value function is defined as an matrix of vectors and a list of actions, the list of AlphaVectors will be generated from them.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def alpha_vector_list(self) -&gt; list[AlphaVector]:
    &#39;&#39;&#39;
    A list of AlphaVector objects. If the value function is defined as an matrix of vectors and a list of actions, the list of AlphaVectors will be generated from them.
    &#39;&#39;&#39;
    if self._vector_list is None:
        self._vector_list = []
        for alpha_vect, action in zip(self._vector_array, self._actions):
            self._vector_list.append(AlphaVector(alpha_vect, action))
    return self._vector_list</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.mdp.ValueFunction.append"><code class="name flex">
<span>def <span class="ident">append</span></span>(<span>self, alpha_vector:<a title="src.mdp.AlphaVector" href="#src.mdp.AlphaVector">AlphaVector</a>) >None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to add an alpha vector to the value function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>alpha_vector</code></strong> :&ensp;<code><a title="src.mdp.AlphaVector" href="#src.mdp.AlphaVector">AlphaVector</a></code></dt>
<dd>The alpha vector to be added to the value function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append(self, alpha_vector:AlphaVector) -&gt; None:
    &#39;&#39;&#39;
    Function to add an alpha vector to the value function.

    Parameters
    ----------
    alpha_vector : AlphaVector
        The alpha vector to be added to the value function.
    &#39;&#39;&#39;
    # Make sure size is correct
    assert alpha_vector.values.shape[0] == self.model.state_count, f&#34;Vector to add to value function doesn&#39;t have the right size (received: {alpha_vector.values.shape[0]}, expected: {self.model.state_count})&#34;
    
    # GPU support check
    xp = cp if (gpu_support and self.is_on_gpu) else np
    assert gpu_support and cp.get_array_module(alpha_vector.values) == xp, f&#34;Vector is{&#39; not&#39; if self.is_on_gpu else &#39;&#39;} on GPU while value function is{&#39;&#39; if self.is_on_gpu else &#39; not&#39;}.&#34;

    if self._vector_array is not None:
        self._vector_array = xp.append(self._vector_array, alpha_vector[None,:], axis=0)
        self._actions.append(alpha_vector.action)
    
    if self._vector_list is not None:
        self._vector_list.append(alpha_vector)</code></pre>
</details>
</dd>
<dt id="src.mdp.ValueFunction.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, as_grid:bool=False, size:int=5, belief_set:numpy.ndarray=None) >None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to plot out the value function in 2 or 3 dimensions if possible and the as_grid parameter is kept to false. Else, the value function is plot as a grid.
If a belief set array is provided and the model is a 2- or 3-model, it will be plot alongside the value function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>as_grid</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Forces the plot to be plot as a grid.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code>, default=<code>5</code></dt>
<dd>The actual plot scale.</dd>
<dt><strong><code>belief_set</code></strong> :&ensp;<code>np.ndarray</code>, optional</dt>
<dd>A set of belief to plot the belief points that were explored.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self,
         as_grid:bool=False,
         size:int=5,
         belief_set:np.ndarray=None
         ) -&gt; None:
    &#39;&#39;&#39;
    Function to plot out the value function in 2 or 3 dimensions if possible and the as_grid parameter is kept to false. Else, the value function is plot as a grid.
    If a belief set array is provided and the model is a 2- or 3-model, it will be plot alongside the value function.

    Parameters
    ----------
    as_grid : bool, default=False
        Forces the plot to be plot as a grid.
    size : int, default=5
        The actual plot scale.
    belief_set : np.ndarray, optional
        A set of belief to plot the belief points that were explored.
    &#39;&#39;&#39;
    assert len(self) &gt; 0, &#34;Value function is empty, plotting is impossible...&#34;
    
    # If on GPU, convert to CPU and plot that one
    if self.is_on_gpu:
        print(&#39;[Warning] Value function on GPU, converting to numpy before plotting...&#39;)
        cpu_value_function = self.to_cpu()
        cpu_value_function.plot(as_grid, size, belief_set)
        return

    func = None
    if as_grid:
        func = self._plot_grid
    elif self.model.state_count == 2:
        func = self._plot_2D
    elif self.model.state_count == 3:
        func = self._plot_3D
    else:
        print(&#39;[Warning] \&#39;as_grid\&#39; parameter set to False but state count is &gt;3 so it will be plotted as a grid&#39;)
        func = self._plot_grid

    func(size, belief_set)</code></pre>
</details>
</dd>
<dt id="src.mdp.ValueFunction.prune"><code class="name flex">
<span>def <span class="ident">prune</span></span>(<span>self, level:int=1) >Self</span>
</code></dt>
<dd>
<div class="desc"><p>Function returning a new value function with the set of alpha vector composing it being it pruned.
The pruning is as thorough as the level:
- 0: No pruning, returns a value function with the alpha vector set being an exact copy of the current one.
- 1: Simple deduplication of the alpha vectors.
- 2: 1+ Check of absolute domination (check if dominated at each state).
- 3: 2+ Solves Linear Programming problem for each alpha vector to see if it is dominated by combinations of other vectors.</p>
<p>Note that the higher the level, the heavier the time impact will be.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>level</code></strong> :&ensp;<code>int</code>, default=<code>1</code></dt>
<dd>Between 0 and 3, how thorough the alpha vector pruning should be.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>new_value_function</code></strong> :&ensp;<code><a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a></code></dt>
<dd>A new value function with a pruned set of alpha vectors.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prune(self, level:int=1) -&gt; Self:
    &#39;&#39;&#39;
    Function returning a new value function with the set of alpha vector composing it being it pruned.
    The pruning is as thorough as the level:
        - 0: No pruning, returns a value function with the alpha vector set being an exact copy of the current one.
        - 1: Simple deduplication of the alpha vectors.
        - 2: 1+ Check of absolute domination (check if dominated at each state).
        - 3: 2+ Solves Linear Programming problem for each alpha vector to see if it is dominated by combinations of other vectors.
    
    Note that the higher the level, the heavier the time impact will be.

    Parameters
    ----------
    level : int, default=1
        Between 0 and 3, how thorough the alpha vector pruning should be.
        
    Returns
    -------
    new_value_function : ValueFunction
        A new value function with a pruned set of alpha vectors.
    &#39;&#39;&#39;
    # GPU support check
    xp = cp if (gpu_support and self.is_on_gpu) else np

    if level &lt; 1:
        return ValueFunction(self.model, xp.copy(self))
    
    # Level 1 pruning: Check for duplicates - works equally for cupy array (on gpu)
    L = {alpha_vector.values.tobytes(): alpha_vector for alpha_vector in self.alpha_vector_list}
    pruned_alpha_set = ValueFunction(self.model, list(L.values()))

    # Level 2 pruning: Check for absolute domination
    if level &gt;= 2:
        # Beyond this point, gpu can&#39;t be used due to the functions used so if on gpu, converting it back to cpu
        if pruned_alpha_set.is_on_gpu:
            pruned_alpha_set = pruned_alpha_set.to_cpu()

        alpha_vector_array = pruned_alpha_set.alpha_vector_array
        X = cdist(alpha_vector_array, alpha_vector_array, metric=(lambda a,b:(a &lt;= b).all() and not (a == b).all())).astype(bool)
        non_dominated_vector_indices = np.invert(X).all(axis=1)

        non_dominated_vectors = alpha_vector_array[non_dominated_vector_indices]
        non_dominated_actions = np.array(pruned_alpha_set.actions)[non_dominated_vector_indices].tolist()

        pruned_alpha_set = ValueFunction(self.model, non_dominated_vectors, non_dominated_actions)

    # Level 3 pruning: LP to check for more complex domination
    if level &gt;= 3:
        alpha_set = pruned_alpha_set.alpha_vector_list
        pruned_alpha_set = ValueFunction(self.model)

        for i, alpha_vect in enumerate(alpha_set):
            other_alphas = alpha_set[:i] + alpha_set[(i+1):]

            # Objective function
            c = np.concatenate([np.array([1]), -1*alpha_vect])

            # Alpha vector contraints
            other_count = len(other_alphas)
            A = np.c_[np.ones(other_count), np.multiply(np.array(other_alphas), -1)]
            alpha_constraints = LinearConstraint(A, 0, np.inf)

            # Constraints that sum of beliefs is 1
            belief_constraint = LinearConstraint(np.array([0] + ([1]*self.model.state_count)), 1, 1)

            # Solve problem
            res = milp(c=c, constraints=[alpha_constraints, belief_constraint])

            # Check if dominated
            is_dominated = (res.x[0] - np.dot(res.x[1:], alpha_vect)) &gt;= 0
            if is_dominated:
                print(alpha_vect)
                print(&#39; -&gt; Dominated\n&#39;)
            else:
                pruned_alpha_set.append(alpha_vect)
    
    # If initial value function was on gpu, and intermediate array was converted to cpu, convert it back to gpu
    if self.is_on_gpu and not pruned_alpha_set.is_on_gpu:
        pruned_alpha_set = pruned_alpha_set.to_cpu()

    return pruned_alpha_set</code></pre>
</details>
</dd>
<dt id="src.mdp.ValueFunction.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path:str='./ValueFunctions', file_name:Optional[str]=None) >None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to save the save function in a file at a given path. If no path is provided, it will be saved in a subfolder (ValueFunctions) inside the current working directory.
If no file_name is provided, it be saved as '<current_timestamp>_value_function.csv'.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code>, default=<code>'./ValueFunctions'</code></dt>
<dd>The path at which the csv will be saved.</dd>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code>, default=<code>'&lt;current_timestamp&gt;_value_function.csv'</code></dt>
<dd>The file name used to save in.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path:str=&#39;./ValueFunctions&#39;, file_name:Union[str,None]=None) -&gt; None:
    &#39;&#39;&#39;
    Function to save the save function in a file at a given path. If no path is provided, it will be saved in a subfolder (ValueFunctions) inside the current working directory.
    If no file_name is provided, it be saved as &#39;&lt;current_timestamp&gt;_value_function.csv&#39;.

    Parameters
    ----------
    path : str, default=&#39;./ValueFunctions&#39;
        The path at which the csv will be saved.
    file_name : str, default=&#39;&lt;current_timestamp&gt;_value_function.csv&#39;
        The file name used to save in.
    &#39;&#39;&#39;
    if not os.path.exists(path):
        print(&#39;Folder does not exist yet, creating it...&#39;)
        os.makedirs(path)
        
    if file_name is None:
        timestamp = datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)
        file_name = timestamp + &#39;_value_function.csv&#39;

    vector_array = self.alpha_vector_array
    actions = self.actions

    # Convert arrays to numpy if on gpu
    if self.is_on_gpu:
        vector_array = cp.asnumpy(vector_array)
        actions = cp.asnumpy(actions)

    data = np.concatenate((np.array(self.actions)[:,None], self.alpha_vector_array), axis=1)
    columns = [&#39;action&#39;, *self.model.state_labels]

    df = pd.DataFrame(data)
    df.to_csv(path + &#39;/&#39; + file_name, index=False, header=columns)</code></pre>
</details>
</dd>
<dt id="src.mdp.ValueFunction.to_cpu"><code class="name flex">
<span>def <span class="ident">to_cpu</span></span>(<span>self) >Self</span>
</code></dt>
<dd>
<div class="desc"><p>Function returning an equivalent value function object with the arrays stored on CPU instead of GPU.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>cpu_value_function</code></strong> :&ensp;<code><a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a></code></dt>
<dd>A new value function with arrays on CPU.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_cpu(self) -&gt; Self:
    &#39;&#39;&#39;
    Function returning an equivalent value function object with the arrays stored on CPU instead of GPU.

    Returns
    -------
    cpu_value_function : ValueFunction
        A new value function with arrays on CPU.
    &#39;&#39;&#39;
    assert gpu_support, &#34;GPU support is not enabled, unable to execute this function&#34;

    cpu_model = self.model.cpu_model

    cpu_value_function = None
    if self._vector_array is not None:
        cpu_vector_array = cp.asnumpy(self._vector_array)
        cpu_actions = self._actions if isinstance(self._actions, list) else cp.asnumpy(self._actions)
        cpu_value_function = ValueFunction(cpu_model, cpu_vector_array, cpu_actions)
    
    else:
        cpu_alpha_vectors = [AlphaVector(cp.asnumpy(av.values), av.action) for av in self._vector_list]
        cpu_value_function = ValueFunction(cpu_model, cpu_alpha_vectors)

    return cpu_value_function</code></pre>
</details>
</dd>
<dt id="src.mdp.ValueFunction.to_gpu"><code class="name flex">
<span>def <span class="ident">to_gpu</span></span>(<span>self) >Self</span>
</code></dt>
<dd>
<div class="desc"><p>Function returning an equivalent value function object with the arrays stored on GPU instead of CPU.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>gpu_value_function</code></strong> :&ensp;<code><a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a></code></dt>
<dd>A new value function with arrays on GPU.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_gpu(self) -&gt; Self:
    &#39;&#39;&#39;
    Function returning an equivalent value function object with the arrays stored on GPU instead of CPU.

    Returns
    -------
    gpu_value_function : ValueFunction
        A new value function with arrays on GPU.
    &#39;&#39;&#39;
    assert gpu_support, &#34;GPU support is not enabled, unable to execute this function&#34;

    gpu_model = self.model.gpu_model

    gpu_value_function = None
    if self._vector_array is not None:
        gpu_vector_array = cp.array(self._vector_array)
        gpu_actions = self._actions if isinstance(self._actions, list) else cp.array(self._actions)
        gpu_value_function = ValueFunction(gpu_model, gpu_vector_array, gpu_actions)
    
    else:
        gpu_alpha_vectors = [AlphaVector(cp.array(av.values), av.action) for av in self._vector_list]
        gpu_value_function = ValueFunction(gpu_model, gpu_alpha_vectors)

    return gpu_value_function</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.mdp.log" href="#src.mdp.log">log</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.mdp.Agent" href="#src.mdp.Agent">Agent</a></code></h4>
<ul class="">
<li><code><a title="src.mdp.Agent.get_best_action" href="#src.mdp.Agent.get_best_action">get_best_action</a></code></li>
<li><code><a title="src.mdp.Agent.run_n_simulations" href="#src.mdp.Agent.run_n_simulations">run_n_simulations</a></code></li>
<li><code><a title="src.mdp.Agent.simulate" href="#src.mdp.Agent.simulate">simulate</a></code></li>
<li><code><a title="src.mdp.Agent.train" href="#src.mdp.Agent.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.mdp.AlphaVector" href="#src.mdp.AlphaVector">AlphaVector</a></code></h4>
</li>
<li>
<h4><code><a title="src.mdp.Model" href="#src.mdp.Model">Model</a></code></h4>
<ul class="two-column">
<li><code><a title="src.mdp.Model.cpu_model" href="#src.mdp.Model.cpu_model">cpu_model</a></code></li>
<li><code><a title="src.mdp.Model.gpu_model" href="#src.mdp.Model.gpu_model">gpu_model</a></code></li>
<li><code><a title="src.mdp.Model.load_from_json" href="#src.mdp.Model.load_from_json">load_from_json</a></code></li>
<li><code><a title="src.mdp.Model.reward" href="#src.mdp.Model.reward">reward</a></code></li>
<li><code><a title="src.mdp.Model.save" href="#src.mdp.Model.save">save</a></code></li>
<li><code><a title="src.mdp.Model.transition" href="#src.mdp.Model.transition">transition</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.mdp.RewardSet" href="#src.mdp.RewardSet">RewardSet</a></code></h4>
<ul class="">
<li><code><a title="src.mdp.RewardSet.plot" href="#src.mdp.RewardSet.plot">plot</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.mdp.Simulation" href="#src.mdp.Simulation">Simulation</a></code></h4>
<ul class="">
<li><code><a title="src.mdp.Simulation.initialize_simulation" href="#src.mdp.Simulation.initialize_simulation">initialize_simulation</a></code></li>
<li><code><a title="src.mdp.Simulation.run_action" href="#src.mdp.Simulation.run_action">run_action</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.mdp.SimulationHistory" href="#src.mdp.SimulationHistory">SimulationHistory</a></code></h4>
<ul class="">
<li><code><a title="src.mdp.SimulationHistory.add" href="#src.mdp.SimulationHistory.add">add</a></code></li>
<li><code><a title="src.mdp.SimulationHistory.plot_simulation_steps" href="#src.mdp.SimulationHistory.plot_simulation_steps">plot_simulation_steps</a></code></li>
<li><code><a title="src.mdp.SimulationHistory.save_simulation_video" href="#src.mdp.SimulationHistory.save_simulation_video">save_simulation_video</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.mdp.Solver" href="#src.mdp.Solver">Solver</a></code></h4>
<ul class="">
<li><code><a title="src.mdp.Solver.solve" href="#src.mdp.Solver.solve">solve</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.mdp.SolverHistory" href="#src.mdp.SolverHistory">SolverHistory</a></code></h4>
<ul class="">
<li><code><a title="src.mdp.SolverHistory.add" href="#src.mdp.SolverHistory.add">add</a></code></li>
<li><code><a title="src.mdp.SolverHistory.plot_changes" href="#src.mdp.SolverHistory.plot_changes">plot_changes</a></code></li>
<li><code><a title="src.mdp.SolverHistory.solution" href="#src.mdp.SolverHistory.solution">solution</a></code></li>
<li><code><a title="src.mdp.SolverHistory.summary" href="#src.mdp.SolverHistory.summary">summary</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.mdp.VI_Solver" href="#src.mdp.VI_Solver">VI_Solver</a></code></h4>
<ul class="">
<li><code><a title="src.mdp.VI_Solver.solve" href="#src.mdp.VI_Solver.solve">solve</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.mdp.ValueFunction" href="#src.mdp.ValueFunction">ValueFunction</a></code></h4>
<ul class="two-column">
<li><code><a title="src.mdp.ValueFunction.actions" href="#src.mdp.ValueFunction.actions">actions</a></code></li>
<li><code><a title="src.mdp.ValueFunction.alpha_vector_array" href="#src.mdp.ValueFunction.alpha_vector_array">alpha_vector_array</a></code></li>
<li><code><a title="src.mdp.ValueFunction.alpha_vector_list" href="#src.mdp.ValueFunction.alpha_vector_list">alpha_vector_list</a></code></li>
<li><code><a title="src.mdp.ValueFunction.append" href="#src.mdp.ValueFunction.append">append</a></code></li>
<li><code><a title="src.mdp.ValueFunction.load_from_file" href="#src.mdp.ValueFunction.load_from_file">load_from_file</a></code></li>
<li><code><a title="src.mdp.ValueFunction.plot" href="#src.mdp.ValueFunction.plot">plot</a></code></li>
<li><code><a title="src.mdp.ValueFunction.prune" href="#src.mdp.ValueFunction.prune">prune</a></code></li>
<li><code><a title="src.mdp.ValueFunction.save" href="#src.mdp.ValueFunction.save">save</a></code></li>
<li><code><a title="src.mdp.ValueFunction.to_cpu" href="#src.mdp.ValueFunction.to_cpu">to_cpu</a></code></li>
<li><code><a title="src.mdp.ValueFunction.to_gpu" href="#src.mdp.ValueFunction.to_gpu">to_gpu</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>