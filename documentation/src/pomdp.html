<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.pomdp API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.pomdp</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from datetime import datetime
from inspect import signature
from matplotlib import animation, cm, colors, ticker, patches
from matplotlib import pyplot as plt
from matplotlib.lines import Line2D
from matplotlib.patches import Rectangle
from tqdm.auto import trange
from typing import Tuple, Union

import copy
import pandas as pd
import os
import random

import numpy as np
gpu_support = False
try:
    import cupy as cp
    gpu_support = True
except:
    print(&#39;[Warning] Cupy could not be loaded: GPU support is not available.&#39;)

from src.mdp import log
from src.mdp import ValueFunction
from src.mdp import Model as MDP_Model
from src.mdp import RewardSet
from src.mdp import SimulationHistory as MDP_SimulationHistory
from src.mdp import SolverHistory as MDP_SolverHistory
from src.mdp import Solver as MDP_Solver
from src.mdp import VI_Solver
from src.mdp import Simulation as MDP_Simulation


COLOR_LIST = [{
    &#39;name&#39;: item.replace(&#39;tab:&#39;,&#39;&#39;),
    &#39;id&#39;: item,
    &#39;hex&#39;: value,
    &#39;rgb&#39;: [int(value.lstrip(&#39;#&#39;)[i:i + (len(value)-1) // 3], 16) for i in range(0, (len(value)-1), (len(value)-1) // 3)]
    } for item, value in colors.TABLEAU_COLORS.items()] # type: ignore

COLOR_ARRAY = np.array([c[&#39;rgb&#39;] for c in COLOR_LIST])


class Model(MDP_Model):
    &#39;&#39;&#39;
    POMDP Model class. Partially Observable Markov Decision Process Model.

    ...

    Parameters
    ----------
    states : int or list[str] or list[list[str]]
        A list of state labels or an amount of states to be used. Also allows to provide a matrix of states to define a grid model.
    actions : int or list
        A list of action labels or an amount of actions to be used.
    observations : int or list
        A list of observation labels or an amount of observations to be used
    transitions : array-like or function, optional
        The transitions between states, an array can be provided and has to be |S| x |A| x |S| or a function can be provided. 
        If a function is provided, it has be able to deal with np.array arguments.
        If none is provided, it will be randomly generated.
    reachable_states : array-like, optional
        A list of states that can be reached from each state and actions. It must be a matrix of size |S| x |A| x |R| where |R| is the max amount of states reachable from any given state and action pair.
        It is optional but useful for speedup purposes.
    rewards : array-like or function, optional
        The reward matrix, has to be |S| x |A| x |S|.
        A function can also be provided here but it has to be able to deal with np.array arguments.
        If provided, it will be use in combination with the transition matrix to fill to expected rewards.
    observation_table : array-like or function, optional
        The observation matrix, has to be |S| x |A| x |O|. If none is provided, it will be randomly generated.
    rewards_are_probabilistic: bool, default=False
        Whether the rewards provided are probabilistic or pure rewards. If probabilist 0 or 1 will be the reward with a certain probability.
    state_grid : array-like, optional
        If provided, the model will be converted to a grid model.
    start_probabilities : list, optional
        The distribution of chances to start in each state. If not provided, there will be an uniform chance for each state. It is also used to represent a belief of complete uncertainty.
    end_states : list, optional
        Entering either state in the list during a simulation will end the simulation.
    end_action : list, optional
        Playing action of the list during a simulation will end the simulation.

    Attributes
    ----------
    states : np.ndarray
        A 1D array of states indices. Used to loop over states.
    state_labels : list[str]
        A list of state labels. (To be mainly used for plotting)
    state_count : int
        How many states are in the Model.
    state_grid : np.ndarray
        The state indices organized as a 2D grid. (Used for plotting purposes)
    actions : np.ndarry
        A 1D array of action indices. Used to loop over actions.
    action_labels : list[str]
        A list of action labels. (To be mainly used for plotting)
    action_count : int
        How many action are in the Model.
    observations : np.ndarray
        A 1D array of observation indices. Used to loop over obervations.
    observation_labels : list[str]
        A list of observation labels. (To be mainly used for plotting)
    observation_count : int
        How many observations can be made in the Model.
    transition_table : np.ndarray
        A 3D matrix of the transition probabilities.
        Can be None in the case a transition function is provided instead.
        Note: When possible, use reachable states and reachable probabilities instead.
    transition_function : function
        A callable function taking 3 arguments: s, a, s_p; and returning a float between 0.0 and 1.0.
        Can be None in the case a transition table is provided instead.
        Note: When possible, use reachable states and reachable probabilities instead.
    observation_table : np.ndarray
        A 3D matrix of shape S x A x O representing the probabilies of obsevating o when taking action a and leading to state s_p.
    reachable_states : np.ndarray
        A 3D array of the shape S x A x R, where R is max amount to states that can be reached from any state-action pair.
    reachable_probabilities : np.ndarray
        A 3D array of the same shape as reachable_states, the array represent the probability of reaching the state pointed by the reachable_states matrix.
    reachable_state_count : int
        The maximum of states that can be reached from any state-action combination.
    reachable_transitional_observation_table : np.ndarray
        A 4D array of shape S x A x O x R, representing the probabiliies of landing if each reachable state r, while observing o after having taken action a from state s.
        Mainly used to speedup repeated operations in solver.
    immediate_reward_table : np.ndarray
        A 3D matrix of shape S x A x S x O of the reward that will received when taking action a, in state s, landing in state s_p, and observing o.
        Can be None in the case an immediate rewards function is provided instead.
    immediate_reward_function : function
        A callable function taking 4 argments: s, a, s_p, o and returning the immediate reward the agent will receive.
        Can be None in the case an immediate rewards function is provided instead.
    expected_reward_table : np.ndarray
        A 2D array of shape S x A. It represents the rewards that is expected to be received when taking action a from state s.
        It is made by taking the weighted average of immediate rewards with the transitions and the observation probabilities.
    start_probabilities : np.ndarray
        A 1D array of length |S| containing the probility distribution of the agent starting in each state.
    rewards_are_probabilisitic : bool
        Whether the immediate rewards are probabilitic, ie: returning a 0 or 1 based on the reward that is considered to be a probability.
    end_states : list[int]
        A list of states that, when reached, terminate a simulation.
    end_actions : list[int]
        A list of actions that, when taken, terminate a simulation.
    is_on_gpu : bool
        Whether the numpy array of the model are stored on the gpu or not.
    gpu_model : mdp.Model
        An equivalent model with the np.ndarray objects on GPU. (If already on GPU, returns self)
    cpu_model : mdp.Model
        An equivalent model with the np.ndarray objects on CPU. (If already on CPU, returns self)
    &#39;&#39;&#39;
    def __init__(self,
                 states:Union[int, list[str], list[list[str]]],
                 actions:Union[int, list],
                 observations:Union[int, list],
                 transitions=None,
                 reachable_states=None,
                 rewards=None,
                 observation_table=None,
                 rewards_are_probabilistic:bool=False,
                 state_grid=None,
                 start_probabilities:Union[list,None]=None,
                 end_states:list[int]=[],
                 end_actions:list[int]=[]
                 ):
        
        super().__init__(states=states,
                         actions=actions,
                         transitions=transitions,
                         reachable_states=reachable_states,
                         rewards=-1, # Defined here lower since immediate reward table has different shape for MDP is different than for POMDP
                         rewards_are_probabilistic=rewards_are_probabilistic,
                         state_grid=state_grid,
                         start_probabilities=start_probabilities,
                         end_states=end_states,
                         end_actions=end_actions)

        print()
        log(&#39;POMDP particular parameters:&#39;)

        def end_reward_function(s, a, sn, o):
            return (np.isin(sn, self.end_states) | np.isin(a, self.end_actions)).astype(int)

        # ------------------------- Observations -------------------------
        if isinstance(observations, int):
            self.observation_labels = [f&#39;o_{i}&#39; for i in range(observations)]
        else:
            self.observation_labels = observations
        self.observation_count = len(self.observation_labels)
        self.observations = np.arange(self.observation_count)

        if observation_table is None:
            # If no observation matrix given, generate random one
            random_probs = np.random.rand(self.state_count, self.action_count, self.observation_count)
            # Normalization to have s_p probabilies summing to 1
            self.observation_table = random_probs / np.sum(random_probs, axis=2, keepdims=True)
        else:
            self.observation_table = np.array(observation_table)
            o_shape = self.observation_table.shape
            exp_shape = (self.state_count, self.action_count, self.observation_count)
            assert o_shape == exp_shape, f&#34;Observations table doesnt have the right shape, it should be SxAxO (expected: {exp_shape}, received: {o_shape}).&#34;

        log(f&#39;- {self.observation_count} observations&#39;)

        # ------------------------- Reachable transitional observation probabilities -------------------------
        log(&#39;- Starting of transitional observations for reachable states table&#39;)
        start_ts = datetime.now()

        reachable_observations = self.observation_table[self.reachable_states[:,:,None,:], self.actions[None,:,None,None], self.observations[None,None,:,None]] # SAOR
        self.reachable_transitional_observation_table = np.einsum(&#39;sar,saor-&gt;saor&#39;, self.reachable_probabilities, reachable_observations)
        
        duration = (datetime.now() - start_ts).total_seconds()
        log(f&#39;    &gt; Done in {duration:.3f}s&#39;)

        # ------------------------- Rewards -------------------------
        self.immediate_reward_table = None
        self.immediate_reward_function = None
        
        if rewards is None:
            if (len(self.end_states) &gt; 0) or (len(self.end_actions) &gt; 0):
                log(&#39;- [Warning] Rewards are not define but end states/actions are, reaching an end state or doing an end action will give a reward of 1.&#39;)
                self.immediate_reward_function = self._end_reward_function
            else:
                # If no reward matrix given, generate random one
                self.immediate_reward_table = np.random.rand(self.state_count, self.action_count, self.state_count, self.observation_count)
        elif callable(rewards):
            # Rewards is a function
            log(&#39;- [Warning] The rewards are provided as a function, if the model is saved, the rewards will need to be defined before loading model.&#39;)
            log(&#39;    &gt; Alternative: Setting end states/actions and leaving the rewards can be done to make the end states/action giving a reward of 1 by default.&#39;)
            self.immediate_reward_function = rewards
            assert len(signature(rewards).parameters) == 4, &#34;Reward function should accept 4 parameters: s, a, sn, o...&#34;
        else:
            # Array like
            self.immediate_reward_table = np.array(rewards)
            r_shape = self.immediate_reward_table.shape
            exp_shape = (self.state_count, self.action_count, self.state_count, self.observation_count)
            assert r_shape == exp_shape, f&#34;Rewards table doesnt have the right shape, it should be SxAxSxO (expected: {exp_shape}, received {r_shape})&#34;
        
        # ------------------------- Expected rewards -------------------------
        log(&#39;- Starting generation of expected rewards table&#39;)
        start_ts = datetime.now()

        reachable_rewards = None
        if self.immediate_reward_table is not None:
            reachable_rewards = rewards[self.states[:,None,None,None], self.actions[None,:,None,None], self.reachable_states[:,:,:,None], self.observations[None,None,None,:]]
        else:
            def reach_reward_func(s,a,ri,o):
                s = s.astype(int)
                a = a.astype(int)
                ri = ri.astype(int)
                o = o.astype(int)
                return self.immediate_reward_function(s,a,self.reachable_states[s,a,ri],o)
            
            reachable_rewards = np.fromfunction(reach_reward_func, (*self.reachable_states.shape, self.observation_count))

        self._min_reward = float(np.min(reachable_rewards))
        self._max_reward = float(np.max(reachable_rewards))

        self.expected_rewards_table = np.einsum(&#39;saor,saro-&gt;sa&#39;, self.reachable_transitional_observation_table, reachable_rewards)

        duration = (datetime.now() - start_ts).total_seconds()
        log(f&#39;    &gt; Done in {duration:.3f}s&#39;)


    def _end_reward_function(self, s, a, sn, o):
        return (np.isin(sn, self.end_states) | np.isin(a, self.end_actions)).astype(int)
    

    def reward(self, s:int, a:int, s_p:int, o:int) -&gt; Union[int,float]:
        &#39;&#39;&#39;
        Returns the rewards of playing action a when in state s and landing in state s_p.
        If the rewards are probabilistic, it will return 0 or 1.

        Parameters
        ----------
        s : int
            The current state.
        a : int
            The action taking in state s.
        s_p : int
            The state landing in after taking action a in state s
        o : int
            The observation that is done after having played action a in state s and landing in s_p

        Returns
        -------
        reward : int or float
            The reward received.
        &#39;&#39;&#39;
        reward = float(self.immediate_reward_table[s,a,s_p,o] if self.immediate_reward_table is not None else self.immediate_reward_function(s,a,s_p,o))
        if self.rewards_are_probabilistic:
            rnd = random.random()
            return 1 if rnd &lt; reward else 0
        else:
            return reward
    

    def observe(self, s_p:int, a:int) -&gt; int:
        &#39;&#39;&#39;
        Returns a random observation knowing action a is taken from state s, it is weighted by the observation probabilities.

        Parameters
        ----------
        s_p : int
            The state landed on after having done action a.
        a : int
            The action to take.

        Returns
        -------
        o : int
            A random observation.
        &#39;&#39;&#39;
        xp = cp if self.is_on_gpu else np
        o = int(xp.random.choice(a=self.observations, size=1, p=self.observation_table[s_p,a])[0])
        return o


class Belief:
    &#39;&#39;&#39;
    A class representing a belief in the space of a given model. It is the belief to be in any combination of states:
    eg:
        - In a 2 state POMDP: a belief of (0.5, 0.5) represent the complete ignorance of which state we are in. Where a (1.0, 0.0) belief is the certainty to be in state 0.

    The belief update function has been implemented based on the belief update define in the paper of J. Pineau, G. Gordon, and S. Thrun, &#39;Point-based approximations for fast POMDP solving&#39;

    ...

    Parameters
    ----------
    model : pomdp.Model
        The model on which the belief applies on.
    values : np.ndarray, optional
        A vector of the probabilities to be in each state of the model. The sum of the probabilities must sum to 1.
        If not specified, it will be set as the start probabilities of the model.

    Attributes
    ----------
    model : pomdp.Model
    values : np.ndarray
    bytes_repr : bytes
        A representation in bytes of the value of the belief
    &#39;&#39;&#39;
    def __init__(self, model:Model, values:Union[np.ndarray,None]=None):
        assert model is not None
        self.model = model

        if values is not None:
            assert values.shape[0] == model.state_count, &#34;Belief must contain be of dimension |S|&#34;

            xp = np if not gpu_support else cp.get_array_module(values)

            prob_sum = xp.sum(values)
            rounded_sum = xp.round(prob_sum, decimals=3)
            assert rounded_sum == 1.0, f&#34;States probabilities in belief must sum to 1 (found: {prob_sum}; rounded {rounded_sum})&#34;

            self._values = values
        else:
            self._values = model.start_probabilities


    def __new__(cls, *args, **kwargs):
        instance = super().__new__(cls)

        instance._bytes_repr = None
        instance._successors = {}
        
        return instance


    @property
    def bytes_repr(self) -&gt; bytes:
        if self._bytes_repr is None:
            self._bytes_repr = self.values.tobytes()
        return self._bytes_repr


    def __eq__(self, other: object) -&gt; bool:
        return self.bytes_repr == other.bytes_repr

    
    @property
    def values(self) -&gt; np.ndarray:
        &#39;&#39;&#39;
        An array of the probability distribution to be in each state.
        &#39;&#39;&#39;
        return self._values
    

    def update(self, a:int, o:int) -&gt; &#39;Belief&#39;:
        &#39;&#39;&#39;
        Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).

        Parameters
        ----------
        a : int
            The most recent action.
        o : int
            The most recent observation.

        Returns
        -------
        new_belief : Belief
            An updated belief
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(self._values)

        succ_id = f&#39;{a}_{o}&#39;
        succ = self._successors.get(succ_id)
        if succ is not None:
            return succ

        reachable_state_probabilities = self.model.reachable_transitional_observation_table[:,a,o,:] * self.values[:,None]
        new_state_probabilities = xp.bincount(self.model.reachable_states[:,a,:].flatten(), weights=reachable_state_probabilities.flatten(), minlength=self.model.state_count)
        
        # Normalization
        new_state_probabilities /= xp.sum(new_state_probabilities)

        # Generation of new belief from new state probabilities
        new_belief = self.__new__(self.__class__)
        new_belief.model = self.model
        new_belief._values = new_state_probabilities

        # Remember generated successor
        self._successors[succ_id] = new_belief

        return new_belief
    

    def generate_successors(self) -&gt; list[&#39;Belief&#39;]:
        &#39;&#39;&#39;
        Function to generate a set of belief that can be reached for each actions and observations available in the model.

        Returns
        -------
        successor_beliefs : list[Belief]
            The successor beliefs.
        &#39;&#39;&#39;
        successor_beliefs = []
        for a in self.model.actions:
            for o in self.model.observations:
                successor_beliefs.append(self.update(a,o))

        return successor_beliefs


    def random_state(self) -&gt; int:
        &#39;&#39;&#39;
        Returns a random state of the model weighted by the belief probabily.

        Returns
        -------
        rand_s : int
            A random state.
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(self._values)

        rand_s = int(xp.random.choice(a=self.model.states, size=1, p=self._values)[0])
        return rand_s
    

    def plot(self, size:int=5) -&gt; None:
        &#39;&#39;&#39;
        Function to plot a heatmap of the belief distribution if the belief is of a grid model.

        Parameters
        ----------
        size : int, default=5
            The scale of the plot.
        &#39;&#39;&#39;
        # Plot setup
        plt.figure(figsize=(size*1.2,size))

        model = self.model.cpu_model

        # Ticks
        dimensions = model.state_grid.shape
        x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))
        y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))

        plt.xticks(x_ticks)
        plt.yticks(y_ticks)

        # Title
        plt.title(f&#39;Belief (probability distribution over states)&#39;)

        # Actual plot
        belief_values = self._values if (not gpu_support) or (cp.get_array_module(self._values) == np) else cp.asnumpy(self._values)
        grid_values = belief_values[model.state_grid]
        plt.imshow(grid_values,cmap=&#39;Blues&#39;)
        plt.colorbar()
        plt.show()


class BeliefSet:
    &#39;&#39;&#39;
    Class to represent a set of beliefs with regard to a POMDP model.
    It has the purpose to store the beliefs in numpy array format and be able to conver it to a list of Belief class objects.
    This class also provides the option to display the beliefs when operating on a 2 or 3d space with the plot() function.
    
    ...

    Parameters
    ----------
    model : pomdp.Model
        The model on which the beliefs apply.
    beliefs : list[Belief] | np.ndarray
        The actual set of beliefs.

    Attributes
    ----------
    model : pomdp.Model
    belief_array : np.ndarray
        A 2D array of shape N x S of N belief vectors.
    belief_list : list[Belief]
        A list of N Belief object.
    &#39;&#39;&#39;
    def __init__(self, model:Model, beliefs:Union[list[Belief],np.ndarray]) -&gt; None:
        self.model = model

        self._belief_list = None
        self._belief_array = None

        self.is_on_gpu = False

        if isinstance(beliefs, list):
            assert all(len(b.values) == model.state_count for b in beliefs), f&#34;Beliefs in belief list provided dont all have shape ({model.state_count},)&#34;
            self._belief_list = beliefs

            # Check if on gpu and make sure all beliefs are also on the gpu
            if (len(beliefs) &gt; 0) and gpu_support and cp.get_array_module(beliefs[0].values) == cp:
                assert all(cp.get_array_module(b.values) == cp for b in beliefs), &#34;Either all or none of the alpha vectors should be on the GPU, not just some.&#34;
                self.is_on_gpu = True
        else:
            assert beliefs.shape[1] == model.state_count, f&#34;Belief array provided doesnt have the right shape (expected (-,{model.state_count}), received {beliefs.shape})&#34;
            
            self._belief_list = []
            for belief_values in beliefs:
                self._belief_list.append(Belief(model, belief_values))

            # Check if array is on gpu
            if gpu_support and cp.get_array_module(beliefs) == cp:
                self.is_on_gpu = True

        # # Deduplication
        # self._uniqueness_dict = {belief.values.tobytes(): belief for belief in self._belief_list}
        # self._belief_list = list(self._uniqueness_dict.values())


    @property
    def belief_array(self) -&gt; np.ndarray:
        &#39;&#39;&#39;
        A matrix of size N x S containing N belief vectors. If belief set is stored as a list of Belief objects, the matrix of beliefs will be generated from them.
        &#39;&#39;&#39;
        xp = cp if (gpu_support and self.is_on_gpu) else np

        if self._belief_array is None:
            self._belief_array = xp.array([b.values for b in self._belief_list])
        return self._belief_array
    

    @property
    def belief_list(self) -&gt; list[Belief]:
        &#39;&#39;&#39;
        A list of Belief objects. If the belief set is represented as a matrix of Belief vectors, the list of Belief objects will be generated from it.
        &#39;&#39;&#39;
        if self._belief_list is None:
            self._belief_list = [Belief(self.model, belief_vector) for belief_vector in self._belief_array]
        return self._belief_list
    

    def generate_all_successors(self) -&gt; &#39;BeliefSet&#39;:
        &#39;&#39;&#39;
        Function to generate the successors beliefs of all the beliefs in the belief set.

        Returns
        -------
        all_successors : BeliefSet
            All successors of all beliefs in the belief set.
        &#39;&#39;&#39;
        all_successors = []
        for belief in self.belief_list:
            all_successors.extend(belief.generate_successors())
        return BeliefSet(self.model, all_successors)
    

    def __len__(self) -&gt; int:
        return len(self._belief_list) if self._belief_list is not None else self._belief_array.shape[0]
    

    def to_gpu(self) -&gt; &#39;BeliefSet&#39;:
        &#39;&#39;&#39;
        Function returning an equivalent belief set object with the array of values stored on GPU instead of CPU.

        Returns
        -------
        gpu_belief_set : BeliefSet
            A new belief set with array on GPU.
        &#39;&#39;&#39;
        assert gpu_support, &#34;GPU support is not enabled, unable to execute this function&#34;

        gpu_model = self.model.gpu_model

        gpu_belief_set = None
        if self._belief_array is not None:
            gpu_belief_array = cp.array(self._belief_array)
            gpu_belief_set = BeliefSet(gpu_model, gpu_belief_array)
        else:
            gpu_belief_list = [Belief(gpu_model, cp.array(b.values)) for b in self._belief_list]
            gpu_belief_set = BeliefSet(gpu_model, gpu_belief_list)

        return gpu_belief_set
    

    def to_cpu(self) -&gt; &#39;BeliefSet&#39;:
        &#39;&#39;&#39;
        Function returning an equivalent belief set object with the array of values stored on CPU instead of GPU.

        Returns
        -------
        cpu_belief_set : BeliefSet
            A new belief set with array on CPU.
        &#39;&#39;&#39;
        assert gpu_support, &#34;GPU support is not enabled, unable to execute this function&#34;

        cpu_model = self.model.cpu_model

        cpu_belief_set = None
        if self._belief_array is not None:
            cpu_belief_array = cp.asnumpy(self._belief_array)
            cpu_belief_set = BeliefSet(cpu_model, cpu_belief_array)
        
        else:
            cpu_belief_list = [Belief(cpu_model, cp.asnumpy(b.values)) for b in self._belief_list]
            cpu_belief_set = BeliefSet(cpu_model, cpu_belief_list)

        return cpu_belief_set
    
    
    def plot(self, size:int=15):
        &#39;&#39;&#39;
        Function to plot the beliefs in the belief set.
        Note: Only works for 2-state and 3-state believes.

        Parameters
        ----------
        size : int, default=15
            The figure size and general scaling factor
        &#39;&#39;&#39;
        assert self.model.state_count in [2,3], &#34;Can&#39;t plot for models with state count other than 2 or 3&#34;

        # If on GPU, convert to CPU and plot that one
        if self.is_on_gpu:
            print(&#39;[Warning] Value function on GPU, converting to numpy before plotting...&#39;)
            cpu_belief_set = self.to_cpu()
            cpu_belief_set.plot(size)
            return

        if self.model.state_count == 2:
            self._plot_2D(size)
        elif self.model.state_count == 3:
            self._plot_3D(size)


    def _plot_2D(self, size=15):
        beliefs_x = self.belief_array[:,1]

        plt.figure(figsize=(size, max([int(size/7),1])))
        plt.scatter(beliefs_x, np.zeros(beliefs_x.shape[0]), c=list(range(beliefs_x.shape[0])), cmap=&#39;Blues&#39;)
        ax = plt.gca()
        ax.get_yaxis().set_visible(False)

        # Set title and ax-label
        ax.set_title(&#39;Set of beliefs&#39;)
        ax.set_xlabel(&#39;Belief space&#39;)

        # X-axis setting
        ticks = [0,0.25,0.5,0.75,1]
        x_ticks = [str(t) for t in ticks]
        x_ticks[0] = self.model.state_labels[0]
        x_ticks[-1] = self.model.state_labels[1]

        plt.xticks(ticks, x_ticks)
        plt.show()


    def _plot_3D(self, size=15):
        # Function to project points to a simplex triangle
        def projectSimplex(points):
            &#34;&#34;&#34; 
            Project probabilities on the 3-simplex to a 2D triangle
            
            N points are given as N x 3 array
            &#34;&#34;&#34;
            # Convert points one at a time
            tripts = np.zeros((points.shape[0],2))
            for idx in range(points.shape[0]):
                # Init to triangle centroid
                x = 1.0 / 2
                y = 1.0 / (2 * np.sqrt(3))
                # Vector 1 - bisect out of lower left vertex 
                p1 = points[idx, 0]
                x = x - (1.0 / np.sqrt(3)) * p1 * np.cos(np.pi / 6)
                y = y - (1.0 / np.sqrt(3)) * p1 * np.sin(np.pi / 6)
                # Vector 2 - bisect out of lower right vertex  
                p2 = points[idx, 1]  
                x = x + (1.0 / np.sqrt(3)) * p2 * np.cos(np.pi / 6)
                y = y - (1.0 / np.sqrt(3)) * p2 * np.sin(np.pi / 6)        
                # Vector 3 - bisect out of top vertex
                p3 = points[idx, 2]
                y = y + (1.0 / np.sqrt(3) * p3)
            
                tripts[idx,:] = (x,y)

            return tripts
        
        # Plotting the simplex 
        def plotSimplex(points,
                        fig=None,
                        vertexlabels=[&#39;s_0&#39;,&#39;s_1&#39;,&#39;s_2&#39;],
                        **kwargs):
            &#34;&#34;&#34;
            Plot Nx3 points array on the 3-simplex 
            (with optionally labeled vertices) 
            
            kwargs will be passed along directly to matplotlib.pyplot.scatter    
            Returns Figure, caller must .show()
            &#34;&#34;&#34;
            if(fig == None):        
                fig = plt.figure()
            # Draw the triangle
            l1 = Line2D([0, 0.5, 1.0, 0], # xcoords
                        [0, np.sqrt(3) / 2, 0, 0], # ycoords
                        color=&#39;k&#39;)
            fig.gca().add_line(l1)
            fig.gca().xaxis.set_major_locator(ticker.NullLocator())
            fig.gca().yaxis.set_major_locator(ticker.NullLocator())
            # Draw vertex labels
            fig.gca().text(-0.05, -0.05, vertexlabels[0])
            fig.gca().text(1.05, -0.05, vertexlabels[1])
            fig.gca().text(0.5, np.sqrt(3) / 2 + 0.05, vertexlabels[2])
            # Project and draw the actual points
            projected = projectSimplex(points)
            plt.scatter(projected[:,0], projected[:,1], **kwargs)              
            # Leave some buffer around the triangle for vertex labels
            fig.gca().set_xlim(-0.2, 1.2)
            fig.gca().set_ylim(-0.2, 1.2)

            return fig

        # Actual plot
        fig = plt.figure(figsize=(size,size))
        plt.title(&#39;Set of Beliefs&#39;)

        cmap = cm.get_cmap(&#39;Blues&#39;)
        norm = colors.Normalize(vmin=0, vmax=self.belief_array.shape[0])
        c = range(self.belief_array.shape[0])
        # Do scatter plot
        fig = plotSimplex(self.belief_array, fig=fig, vertexlabels=self.model.state_labels, s=size, c=c, cmap=cmap, norm=norm)

        plt.show()


class BeliefValueMapping:
    &#39;&#39;&#39;
    Alternate representation of a value function, particularly for pomdp models.
    It works by adding adding belief and associated value to the object.
    To evaluate this version of the value function the sawtooth algorithm is used (described in Shani G. et al., &#34;A survey of point-based POMDP solvers&#34;)
    
    We can also compute the Q value for a particular belief b and action using the qva function.

    Parameters
    ----------
    model : pomdp.Model
        The model on which the value function applies on
    corner_belief_values : ValueFunction
        A general value function to define the value at corner points in belief space (ie: at certainty beliefs, or when beliefs have a probability of 1 for a given state).
        This is usually the solution of the MDP version of the problem.

    Attributes
    ----------
    model : pomdp.Model
    corner_belief_values : ValueFunction
    corner_values : np.ndarray
        Array of |S| shape, having the max value at each state based on the corner_belief_values.
    beliefs : Belief
        Beliefs contained in the belief-value mapping.
    belief_value_mapping : dict[bytes, float]
        Mapping of beliefs points with their associated value.
    
    &#39;&#39;&#39;
    def __init__(self, model, corner_belief_values:ValueFunction) -&gt; None:
        xp = np if not gpu_support else cp.get_array_module(corner_belief_values.alpha_vector_array)

        self.model = model
        self.corner_belief_values = corner_belief_values
        
        self.corner_values = xp.max(corner_belief_values.alpha_vector_array, axis=0)

        self.beliefs = []
        self.belief_value_mapping = {}

        self._belief_array = None
        self._value_array = None

    
    def add(self, b:Belief, v:float) -&gt; None:
        &#39;&#39;&#39;
        Function to a belief point and its associated value to the belief value mappings

        Parameters
        ----------
        b: Belief
        v: float
        &#39;&#39;&#39;
        if b not in self.beliefs:
            self.beliefs.append(b)
            self.belief_value_mapping[b.bytes_repr] = v


    @property
    def belief_array(self) -&gt; np.ndarray:
        xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)

        if self._belief_array is None:
            self._belief_array = xp.array([b.values for b in self.beliefs])

        return self._belief_array
    

    @property
    def value_array(self) -&gt; np.ndarray:
        xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)

        if self._value_array is None:
            self._value_array = xp.array(list(self.belief_value_mapping.values()))

        return self._value_array
    

    def update(self) -&gt; None:
        &#39;&#39;&#39;
        Function to update the belief and value arrays to speed up computation.
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)

        self._belief_array = xp.array([b.values for b in self.beliefs])
        self._value_array = xp.array(list(self.belief_value_mapping.values()))


    def evaluate(self, belief:Belief) -&gt; float:
        &#39;&#39;&#39;
        Runs the sawtooth algorithm to find the value at a given belief point.

        Parameters
        ----------
        belief: Belief
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(belief.values)

        # Shortcut if belief already exists in the mapping
        if belief in self.beliefs:
            return self.belief_value_mapping[belief.bytes_repr]

        v0 = xp.dot(belief.values, self.corner_values)

        if len(self.beliefs) == 0:
            return float(v0)

        with np.errstate(divide=&#39;ignore&#39;, invalid=&#39;ignore&#39;):
            vb = v0 + ((self.value_array - xp.dot(self.belief_array, self.corner_values)) * xp.min(belief.values / self.belief_array, axis=1))

        return float(xp.min(xp.append(vb, v0)))


class SolverHistory:
    &#39;&#39;&#39;
    Class to represent the history of a solver for a POMDP solver.
    It has mainly the purpose to have visualizations for the solution, belief set and the whole solving history.
    The visualizations available are:
        - Belief set plot
        - Solution plot
        - Video of value function and belief set evolution over training.

    ...

    Parameters
    ----------
    tracking_level : int
        The tracking level of the solver.
    model : pomdp.Model
        The model the solver has solved.
    gamma : float
        The gamma parameter used by the solver (learning rate).
    eps : float
        The epsilon parameter used by the solver (covergence bound).
    expand_function : str
        The expand (exploration) function used by the solver.
    expand_append : bool
        Whether the expand function appends new belief points to the belief set of reloads it all.
    initial_value_function : ValueFunction
        The initial value function the solver will use to start the solving process.
    initial_belief_set : BeliefSet
        The initial belief set the solver will use to start the solving process.

    Attributes
    ----------
    tracking_level : int
    model : pomdp.Model
    gamma : float
    eps : float
    expand_function : str
    expand_append : bool
    run_ts : datetime
        The time at which the SolverHistory object was instantiated which is assumed to be the start of the solving run.
    expansion_times : list[float]
        A list of recorded times of the expand function.
    backup_times : list[float]
        A list of recorded times of the backup function.
    alpha_vector_counts : list[int]
        A list of recorded alpha vector count making up the value function over the solving process.
    beliefs_counts : list[int]
        A list of recorded belief count making up the belief set over the solving process.
    value_function_changes : list[float]
        A list of recorded value function changes (the maximum changed value between 2 value functions).
    value_functions : list[ValueFunction]
        A list of recorded value functions.
    belief_sets : list[BeliefSet]
        A list of recorded belief sets.
    solution : ValueFunction
    explored_beliefs : BeliefSet
    &#39;&#39;&#39;
    def __init__(self,
                 tracking_level:int,
                 model:Model,
                 gamma:float,
                 eps:float,
                 expand_function:str,
                 expand_append:bool,
                 initial_value_function:ValueFunction,
                 initial_belief_set:BeliefSet
                 ):
        
        self.tracking_level = tracking_level
        self.model = model
        self.gamma = gamma
        self.eps = eps
        self.run_ts = datetime.now()
        
        self.expand_function = expand_function
        self.expand_append = expand_append

        # Time tracking
        self.expansion_times = []
        self.backup_times = []
        self.pruning_times = []

        # Value function and belief set sizes tracking
        self.alpha_vector_counts = []
        self.beliefs_counts = []
        self.prune_counts = []

        if self.tracking_level &gt;= 1:
            self.alpha_vector_counts.append(len(initial_value_function))
            self.beliefs_counts.append(len(initial_belief_set))

        # Value function and belief set tracking
        self.belief_sets = []
        self.value_functions = []
        self.value_function_changes = []

        if self.tracking_level &gt;= 2:
            self.belief_sets.append(initial_belief_set)
            self.value_functions.append(initial_value_function)


    @property
    def solution(self) -&gt; ValueFunction:
        &#39;&#39;&#39;
        The last value function of the solving process.
        &#39;&#39;&#39;
        assert self.tracking_level &gt;= 2, &#34;Tracking level is set too low, increase it to 2 if you want to have value function tracking as well.&#34;
        return self.value_functions[-1]
    

    @property
    def explored_beliefs(self) -&gt; BeliefSet:
        &#39;&#39;&#39;
        The final set of beliefs explored during the solving.
        &#39;&#39;&#39;
        assert self.tracking_level &gt;= 2, &#34;Tracking level is set too low, increase it to 2 if you want to have belief sets tracking as well.&#34;
        return self.belief_sets[-1]
    

    def add_expand_step(self,
                        expansion_time:float,
                        belief_set:BeliefSet
                        ) -&gt; None:
        &#39;&#39;&#39;
        Function to add an expansion step in the simulation history by the explored belief set the expand function generated.

        Parameters
        ----------
        expansion_time : float
            The time it took to run a step of expansion of the belief set. (Also known as the exploration step.)
        belief_set : BeliefSet
            The belief set used for the Update step of the solving process.
        &#39;&#39;&#39;
        if self.tracking_level &gt;= 1:
            self.expansion_times.append(float(expansion_time))
            self.beliefs_counts.append(len(belief_set))

        if self.tracking_level &gt;= 2:
            self.belief_sets.append(belief_set if not belief_set.is_on_gpu else belief_set.to_cpu())


    def add_backup_step(self,
                        backup_time:float,
                        value_function_change:float,
                        value_function:ValueFunction
                        ) -&gt; None:
        &#39;&#39;&#39;
        Function to add a backup step in the simulation history by recording the value function the backup function generated.

        Parameters
        ----------
        backup_time : float
            The time it took to run a step of backup of the value function. (Also known as the value function update.)
        value_function_change : float
            The change between the value function of this iteration and of the previous iteration.
        value_function : ValueFunction
            The value function resulting after a step of the solving process.
        &#39;&#39;&#39;
        if self.tracking_level &gt;= 1:
            self.backup_times.append(float(backup_time))
            self.alpha_vector_counts.append(len(value_function))
            self.value_function_changes.append(float(value_function_change))

        if self.tracking_level &gt;= 2:
            self.value_functions.append(value_function if not value_function.is_on_gpu else value_function.to_cpu())


    def add_prune_step(self,
                       prune_time:float,
                       alpha_vectors_pruned:int
                       ) -&gt; None:
        &#39;&#39;&#39;
        Function to add a prune step in the simulation history by recording the amount of alpha vectors that were pruned by the pruning function and how long it took.

        Parameters
        ----------
        prune_time : float
            The time it took to run the pruning step.
        alpha_vectors_pruned : int
            How many alpha vectors were pruned.
        &#39;&#39;&#39;
        if self.tracking_level &gt;= 1:
            self.pruning_times.append(prune_time)
            self.prune_counts.append(alpha_vectors_pruned)


    @property
    def summary(self) -&gt; str:
        &#39;&#39;&#39;
        A summary as a string of the information recorded.

        Returns
        -------
        summary_str : str
            The summary of the information.
        &#39;&#39;&#39;
        summary_str =  f&#39;Summary of Value Iteration run&#39;
        summary_str += f&#39;\n  - Model: {self.model.state_count} state, {self.model.action_count} action, {self.model.observation_count} observations&#39;
        summary_str += f&#39;\n  - Converged or stopped after {len(self.expansion_times)} expansion steps and {len(self.backup_times)} backup steps.&#39;

        if self.tracking_level &gt;= 1:
            summary_str += f&#39;\n  - Resulting value function has {self.alpha_vector_counts[-1]} alpha vectors.&#39;
            summary_str += f&#39;\n  - Converged in {(sum(self.expansion_times) + sum(self.backup_times)):.4f}s&#39;
            summary_str += f&#39;\n&#39;

            summary_str += f&#39;\n  - Expand function took on average {sum(self.expansion_times) / len(self.expansion_times):.4f}s &#39;
            if self.expand_append:
                summary_str += f&#39;and yielded on average {sum(np.diff(self.beliefs_counts)) / len(self.beliefs_counts[1:]):.2f} beliefs per iteration.&#39;
            else:
                summary_str += f&#39;and yielded on average {sum(self.beliefs_counts[1:]) / len(self.beliefs_counts[1:]):.2f} beliefs per iteration.&#39;
            summary_str += f&#39; ({np.sum(np.divide(self.expansion_times, self.beliefs_counts[1:])) / len(self.expansion_times):.4f}s/it/belief)&#39;
            
            summary_str += f&#39;\n  - Backup function took on average {sum(self.backup_times) /len(self.backup_times):.4f}s &#39;
            summary_str += f&#39;and yielded on average value functions of size {sum(self.alpha_vector_counts[1:]) / len(self.alpha_vector_counts[1:]):.2f} per iteration.&#39;
            summary_str += f&#39; ({np.sum(np.divide(self.backup_times, self.alpha_vector_counts[1:])) / len(self.backup_times):.4f}s/it/alpha)&#39;

            summary_str += f&#39;\n  - Pruning function took on average {sum(self.pruning_times) /len(self.pruning_times):.4f}s &#39;
            summary_str += f&#39;and yielded on average prunings of {sum(self.prune_counts) / len(self.prune_counts):.2f} alpha vectors per iteration.&#39;
        
        return summary_str
    

    def plot_belief_set(self, size:int=15) -&gt; None:
        &#39;&#39;&#39;
        Function to plot the last belief set explored during the solving process.

        Parameters
        ----------
        size : int, default=15
            The scale of the plot.
        &#39;&#39;&#39;
        self.explored_beliefs.plot(size=size)


    def plot_solution(self, size:int=5, plot_belief:bool=True) -&gt; None:
        &#39;&#39;&#39;
        Function to plot the value function of the solution.
        Note: only works for 2 and 3 states models

        Parameters
        ----------
        size : int, default=5
            The figure size and general scaling factor.
        plot_belief : bool, default=True
            Whether to plot the belief set along with the value function.
        &#39;&#39;&#39;
        self.solution.plot(size=size, belief_set=(self.explored_beliefs if plot_belief else None))


    def save_history_video(self,
                           custom_name:Union[str,None]=None,
                           compare_with:Union[list, ValueFunction, MDP_SolverHistory]=[],
                           graph_names:list[str]=[],
                           fps:int=10
                           ) -&gt; None:
        &#39;&#39;&#39;
        Function to generate a video of the training history. Another solved solver or list of solvers can be put in the &#39;compare_with&#39; parameter.
        These other solver&#39;s value function will be overlapped with the 1st value function.
        The explored beliefs of the main solver are also mapped out. (other solvers&#39;s explored beliefs will not be plotted)
        Also, a single value function or list of value functions can be sent in but they will be fixed in the video.

        Note: only works for 2-state models.

        Parameters
        ----------
        custom_name : str, optional
            The name the video will be saved with.
        compare_with : PBVI or ValueFunction or list, default=[]
            Value functions or other solvers to plot against the current solver&#39;s history.
        graph_names : list[str], default=[]
            Names of the graphs for the legend of which graph is being plot.
        fps : int, default=10
            How many frames per second should the saved video have.
        &#39;&#39;&#39;
        assert self.tracking_level &gt;= 2, &#34;Tracking level is set too low, increase it to 2 if you want to have value function and belief sets tracking as well.&#34;
        assert self.model.state_count in [2,3], &#34;Can&#39;t plot for models with state count other than 2 or 3&#34; # TODO Make support for gird videos

        if self.model.state_count == 2:
            self._save_history_video_2D(custom_name, compare_with, copy.copy(graph_names), fps)
        elif self.model.state_count == 3:
            raise Exception(&#39;Not implemented...&#39;)


    def _save_history_video_2D(self, custom_name=None, compare_with=[], graph_names=[], fps=10):
        # Figure definition
        grid_spec = {&#39;height_ratios&#39;: [19,1]}
        fig, (ax1,ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw=grid_spec)

        # Figure title
        fig.suptitle(f&#34;{self.model.state_count}-s {self.model.action_count}-a {self.model.observation_count}-o POMDP model solve history&#34;, fontsize=16)
        title = f&#39;{self.expand_function} expand strat, {self.gamma}-gamma, {self.eps}-eps &#39;

        # X-axis setting
        ticks = [0,0.25,0.5,0.75,1]
        x_ticks = [str(t) for t in ticks]
        x_ticks[0] = self.model.state_labels[0]
        x_ticks[-1] = self.model.state_labels[1]

        # Colors and lines
        line_types = [&#39;-&#39;, &#39;--&#39;, &#39;-.&#39;, &#39;:&#39;]

        proxy = [Rectangle((0,0),1,1,fc = COLOR_LIST[a][&#39;id&#39;]) for a in self.model.actions]

        # Solver list
        if isinstance(compare_with, ValueFunction) or isinstance(compare_with, MDP_SolverHistory):
            compare_with_list = [compare_with] # Single item
        else:
            compare_with_list = compare_with # Already group of items
        solver_histories = [self] + compare_with_list
        
        assert len(solver_histories) &lt;= len(line_types), f&#34;Plotting can only happen for up to {len(line_types)} solvers...&#34;
        line_types = line_types[:len(solver_histories)]

        assert len(graph_names) in [0, len(solver_histories)], &#34;Not enough graph names provided&#34;
        if len(graph_names) == 0:
            graph_names.append(&#39;Main graph&#39;)
            for i in range(1,len(solver_histories)):
                graph_names.append(f&#39;Comparison {i}&#39;)

        def plot_on_ax(history:Union[ValueFunction,&#39;SolverHistory&#39;], frame_i:int, ax, line_type:str):
            if isinstance(history, ValueFunction):
                value_function = history
            else:
                frame_i = frame_i if frame_i &lt; len(history.value_functions) else (len(history.value_functions) - 1)
                value_function = history.value_functions[frame_i]

            alpha_vects = value_function.alpha_vector_array
            m = np.subtract(alpha_vects[:,1], alpha_vects[:,0])
            m = m.reshape(m.shape[0],1)

            x = np.linspace(0, 1, 100)
            x = x.reshape((1,x.shape[0])).repeat(m.shape[0],axis=0)
            y = np.add((m*x), alpha_vects[:,0].reshape(m.shape[0],1))

            for i, alpha in enumerate(value_function.alpha_vector_list):
                ax.plot(x[i,:], y[i,:], line_type, color=COLOR_LIST[alpha.action][&#39;id&#39;])

        def plt_frame(frame_i):
            ax1.clear()
            ax2.clear()

            # Axes labels
            ax1.set_ylabel(&#39;V(b)&#39;)
            ax2.set_xlabel(&#39;Belief space&#39;)

            self_frame_i = frame_i if frame_i &lt; len(self.value_functions) else (len(self.value_functions) - 1)

            # Subtitle
            ax1.set_title(title + f&#39;(Frame {frame_i})&#39;)

            # Color legend
            leg1 = ax1.legend(proxy, self.model.action_labels, loc=&#39;upper center&#39;)
            ax1.set_xticks(ticks, x_ticks)
            ax1.add_artist(leg1)

            # Line legend
            lines = []
            point = self.value_functions[self_frame_i].alpha_vector_array[0,0]
            for l in line_types:
                lines.append(Line2D([0,point],[0,point],linestyle=l))
            ax1.legend(lines, graph_names, loc=&#39;lower center&#39;)

            # Alpha vector plotting
            for history, line_type in zip(solver_histories, line_types):
                plot_on_ax(history, frame_i, ax1, line_type)

            # Belief plotting
            beliefs_x = self.belief_sets[frame_i if frame_i &lt; len(self.belief_sets) else -1].belief_array[:,1]
            ax2.scatter(beliefs_x, np.zeros(beliefs_x.shape[0]), c=&#39;red&#39;)
            ax2.get_yaxis().set_visible(False)
            ax2.axhline(0, color=&#39;black&#39;)

        max_steps = max([len(history.value_functions) for history in solver_histories if not isinstance(history,ValueFunction)])
        ani = animation.FuncAnimation(fig, plt_frame, frames=max_steps, repeat=False)
        
        # File Title
        solved_time = self.run_ts.strftime(&#39;%Y%m%d_%H%M%S&#39;)

        video_title = f&#39;{custom_name}-&#39; if custom_name is not None else &#39;&#39; # Base
        video_title += f&#39;s{self.model.state_count}-a{self.model.action_count}-&#39; # Model params
        video_title += f&#39;{self.expand_function}-&#39; # Expand function used
        video_title += f&#39;g{self.gamma}-e{self.eps}-&#39; # Solving params
        video_title += f&#39;{solved_time}.mp4&#39;

        # Video saving
        if not os.path.exists(&#39;./Results&#39;):
            print(&#39;Folder does not exist yet, creating it...&#39;)
            os.makedirs(&#39;./Results&#39;)

        writervideo = animation.FFMpegWriter(fps=fps)
        ani.save(&#39;./Results/&#39; + video_title, writer=writervideo)
        print(f&#39;Video saved at \&#39;Results/{video_title}\&#39;...&#39;)
        plt.close()


class Solver(MDP_Solver):
    &#39;&#39;&#39;
    POMDP Model Solver - abstract class
    &#39;&#39;&#39;
    def solve(self, model: Model) -&gt; tuple[ValueFunction, SolverHistory]:
        raise Exception(&#34;Method has to be implemented by subclass...&#34;)


class PBVI_Solver(Solver):
    &#39;&#39;&#39;
    The Point-Based Value Iteration solver for POMDP Models. It works in two steps, first the backup step that updates the alpha vector set that approximates the value function.
    Then, the expand function that expands the belief set.

    The various expand functions and the backup function have been implemented based on the pseudocodes found the paper from J. Pineau, G. Gordon, and S. Thrun, &#39;Point-based approximations for fast POMDP solving&#39;

    ...
    Parameters
    ----------
    gamma : float, default=0.99
        The learning rate, used to control how fast the value function will change after the each iterations.
    eps : float, default=0.001
        The treshold for convergence. If the max change between value function is lower that eps, the algorithm is considered to have converged.
    expand_function : str, default=&#39;ssea&#39;
        The type of expand strategy to use to expand the belief set.
    expand_function_params : dict, optional
        Other required parameters to be sent to the expand function.

    Attributes
    ----------
    gamma : float
    eps : float
    expand_function : str
    expand_function_params : dict
    &#39;&#39;&#39;
    def __init__(self,
                 gamma:float=0.99,
                 eps:float=0.001,
                 expand_function:str=&#39;ssea&#39;,
                 **expand_function_params):
        self.gamma = gamma
        self.eps = eps
        self.expand_function = expand_function
        self.expand_function_params = expand_function_params


    def test_n_simulations(self, model:Model, value_function:ValueFunction, n:int=1000, horizon:int=300, print_progress:bool=False):
        &#39;&#39;&#39;
        Function that tests a value function with n simulations. It returns the start states, the amount of steps in which the simulation reached an end state, the rewards received and the discounted rewards received.

        Parameters
        ----------
        model : pomdp.Model
            The model on which to run the simulations.
        value_function : ValueFunction
            The value function that will be evaluated.
        n : int, default=1000
            The amount of simulations to run.
        horizon : int, default=300
            The maximum amount of steps the simulation can run for.
        print_progress : bool, default=False
            Whether to display a progress bar of how many simulation steps have been run so far. 
        &#39;&#39;&#39;
        # GPU support
        xp = np if not value_function.is_on_gpu else cp
        model = model.cpu_model if not value_function.is_on_gpu else model.gpu_model

        # Genetion of an array of n beliefs
        initial_beliefs = xp.repeat(Belief(model).values[None,:], n, axis=0)

        # Generating n initial positions
        start_states = xp.random.choice(model.states, size=n, p=model.start_probabilities)
        
        # Belief and state arrays
        beliefs = initial_beliefs
        new_beliefs = None
        states = start_states
        next_states = None

        # Tracking what simulations are done
        sim_is_done = xp.zeros(n, dtype=bool)
        done_at_step = xp.full(n, -1)

        # Speedup item
        simulations = xp.arange(n)
        flatten_offset = (simulations[:,None] * model.state_count)
        flat_shape = (n, (model.state_count * model.reachable_state_count))

        # 2D bincount for belief set update
        def bincount2D_vectorized(a, w):    
            a_offs = a + flatten_offset
            return xp.bincount(a_offs.ravel(), weights=w.ravel(), minlength=a.shape[0]*model.state_count).reshape(-1,model.state_count)

        # Results
        discount = self.gamma
        rewards = []
        discounted_rewards = []
        
        iterator = trange(horizon) if print_progress else range(horizon)
        for i in iterator:
            # Retrieving the top vectors according to the value function
            best_vectors = xp.argmax(xp.matmul(beliefs, value_function.alpha_vector_array.T), axis=1)

            # Retrieving the actions associated with the vectors chosen
            best_actions = value_function.actions[best_vectors]

            # Get each reachable next states for each action
            reachable_state_per_actions = model.reachable_states[:, best_actions, :]

            # Gathering new states based on the transition function and the chosen actions
            next_state_potentials = reachable_state_per_actions[states, simulations]
            if model.reachable_state_count == 1:
                next_states = next_state_potentials[:,0]
            else:
                potential_probabilities = model.reachable_probabilities[states[:,None], best_actions[:,None]][:,0,:]
                chosen_indices = xp.apply_along_axis(lambda x: xp.random.choice(len(x), size=1, p=x), axis=1, arr=potential_probabilities)
                next_states = next_state_potentials[chosen_indices][:,0,0]

            # Making observations based on the states landed in and the action that was taken
            observation_probabilities = model.observation_table[next_states[:,None], best_actions[:,None]][:,0,:]
            observations = xp.sum(xp.random.random(n)[:,None] &gt; xp.cumsum(observation_probabilities[:,:-1], axis=1), axis=1)

            # Belief set update
            reachable_probabilities = (model.reachable_transitional_observation_table[:,best_actions,observations,:] * beliefs.T[:,:,None])
            new_beliefs = bincount2D_vectorized(a=reachable_state_per_actions.swapaxes(0,1).reshape(flat_shape),
                                                w=reachable_probabilities.swapaxes(0,1).reshape(flat_shape))

            new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]

            # Rewards computation
            step_rewards = xp.array([model.immediate_reward_function(s,a,s_p,o) for s,a,s_p,o in zip(states, best_actions, next_states, observations)])
            rewards.append(xp.where(~sim_is_done, step_rewards, 0))
            discounted_rewards.append(xp.where(~sim_is_done, step_rewards * discount, 0))

            # Checking for done condition
            are_done = xp.isin(next_states, xp.array(model.end_states))
            done_at_step[sim_is_done ^ are_done] = i+1
            sim_is_done |= are_done

            # Update iterator postfix
            if print_progress:
                iterator.set_postfix({&#39;done&#39;: xp.sum(sim_is_done)})

            # Replacing old with new
            states = next_states
            beliefs = new_beliefs
            discount *= self.gamma

            # Early stopping
            if xp.all(sim_is_done):
                break

        return start_states, done_at_step, rewards, discounted_rewards


    def backup(self,
               model:Model,
               belief_set:BeliefSet,
               value_function:ValueFunction,
               append:bool=False,
               belief_dominance_prune:bool=True
               ) -&gt; ValueFunction:
        &#39;&#39;&#39;
        This function has purpose to update the set of alpha vectors. It does so in 3 steps:
        1. It creates projections from each alpha vector for each possible action and each possible observation
        2. It collapses this set of generated alpha vectors by taking the weighted sum of the alpha vectors weighted by the observation probability and this for each action and for each belief.
        3. Then it further collapses the set to take the best alpha vector and action per belief
        In the end we have a set of alpha vectors as large as the amount of beliefs.

        The alpha vectors are also pruned to avoid duplicates and remove dominated ones.

        Parameters
        ----------
        model : pomdp.Model
            The model on which to run the backup method on.
        belief_set : BeliefSet
            The belief set to use to generate the new alpha vectors with.
        value_function : ValueFunction
            The alpha vectors to generate the new set from.
        append : bool, default=False
            Whether to append the new alpha vectors generated to the old alpha vectors before pruning.
        belief_dominance_prune : bool, default=True
            Whether, before returning the new value function, checks what alpha vectors have a supperior value, if so it adds it.
            
        Returns
        -------
        new_alpha_set : ValueFunction
            A list of updated alpha vectors.
        &#39;&#39;&#39;
        # Get numpy corresponding to the arrays
        xp = np if not gpu_support else cp.get_array_module(value_function.alpha_vector_array)

        # Step 1
        vector_array = value_function.alpha_vector_array
        vectors_array_reachable_states = vector_array[xp.arange(vector_array.shape[0])[:,None,None,None], model.reachable_states[None,:,:,:]]
        
        gamma_a_o_t = self.gamma * xp.einsum(&#39;saor,vsar-&gt;aovs&#39;, model.reachable_transitional_observation_table, vectors_array_reachable_states)

        # Step 2
        belief_array = belief_set.belief_array # bs
        best_alpha_ind = xp.argmax(xp.tensordot(belief_array, gamma_a_o_t, (1,3)), axis=3) # argmax(bs,aovs-&gt;baov) -&gt; bao

        best_alphas_per_o = gamma_a_o_t[model.actions[None,:,None,None], model.observations[None,None,:,None], best_alpha_ind[:,:,:,None], model.states[None,None,None,:]] # baos

        alpha_a = model.expected_rewards_table.T + xp.sum(best_alphas_per_o, axis=2) # as + bas

        # Step 3
        best_actions = xp.argmax(xp.einsum(&#39;bas,bs-&gt;ba&#39;, alpha_a, belief_array), axis=1)
        alpha_vectors = xp.take_along_axis(alpha_a, best_actions[:,None,None],axis=1)[:,0,:]

        # Belief domination
        if belief_dominance_prune:
            best_value_per_belief = xp.sum((belief_array * alpha_vectors), axis=1)
            old_best_value_per_belief = xp.max(xp.matmul(belief_array, vector_array.T), axis=1)
            dominating_vectors = best_value_per_belief &gt; old_best_value_per_belief

            best_actions = best_actions[dominating_vectors]
            alpha_vectors = alpha_vectors[dominating_vectors]

        # Creation of value function
        new_value_function = ValueFunction(model, alpha_vectors, best_actions)

        # Union with previous value function
        if append:
            new_value_function.extend(value_function)
        
        return new_value_function
    

    def expand_ra(self, model:Model, belief_set:BeliefSet, max_generation:int=10) -&gt; BeliefSet:
        &#39;&#39;&#39;
        This expansion technique relies only randomness and will generate at most &#39;max_generation&#39; beliefs.

        Parameters
        model : pomdp.Model
            The POMDP model on which to expand the belief set on.
        belief_set : BeliefSet
            List of beliefs to expand on.
        max_generation : int, default=10
            The max amount of beliefs that can be added to the belief set at once.
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

        # How many new beliefs to add
        generation_count = min(belief_set.belief_array.shape[0], max_generation)

        # Generation of the new beliefs at random
        new_beliefs = xp.random.random((generation_count, model.state_count))
        new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]

        return BeliefSet(model, new_beliefs)

    
    def expand_ssra(self, model:Model, belief_set:BeliefSet, max_generation:int=10) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Stochastic Simulation with Random Action.
        Simulates running a single-step forward from the beliefs in the &#34;belief_set&#34;.
        The step forward is taking assuming we are in a random state (weighted by the belief) and taking a random action leading to a state s_p and a observation o.
        From this action a and observation o we can update our belief.

        Parameters
        ----------
        model : pomdp.Model
            The POMDP model on which to expand the belief set on.
        belief_set : BeliefSet
            List of beliefs to expand on.
        max_generation : int, default=10
            The max amount of beliefs that can be added to the belief set at once.

        Returns
        -------
        belief_set_new : BeliefSet
            Union of the belief_set and the expansions of the beliefs in the belief_set
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

        old_shape = belief_set.belief_array.shape
        to_generate = min(max_generation, old_shape[0])

        new_belief_array = xp.empty((to_generate, old_shape[1]))

        # Random previous beliefs
        rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)

        for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):
            b = Belief(model, belief_vector)
            s = b.random_state()
            a = random.choice(model.actions)
            s_p = model.transition(s, a)
            o = model.observe(s_p, a)
            b_new = b.update(a, o)
            
            new_belief_array[i] = b_new.values
            
        return BeliefSet(model, new_belief_array)
    

    def expand_ssga(self, model:Model, belief_set:BeliefSet, value_function:ValueFunction, epsilon:float=0.1, max_generation:int=10) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Stochastic Simulation with Greedy Action.
        Simulates running a single-step forward from the beliefs in the &#34;belief_set&#34;.
        The step forward is taking assuming we are in a random state s (weighted by the belief),
         then taking the best action a based on the belief with probability &#39;epsilon&#39;.
        These lead to a new state s_p and a observation o.
        From this action a and observation o we can update our belief. 

        Parameters
        ----------
        model : pomdp.Model
            The POMDP model on which to expand the belief set on.
        belief_set : BeliefSet
            List of beliefs to expand on.
        value_function : ValueFunction
            Used to find the best action knowing the belief.
        eps : float
            Parameter tuning how often we take a greedy approach and how often we move randomly.
        max_generation : int, default=10
            The max amount of beliefs that can be added to the belief set at once.

        Returns
        -------
        belief_set_new : BeliefSet
            Union of the belief_set and the expansions of the beliefs in the belief_set
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

        old_shape = belief_set.belief_array.shape
        to_generate = min(max_generation, old_shape[0])

        new_belief_array = xp.empty((to_generate, old_shape[1]))

        # Random previous beliefs
        rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)

        for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):
            b = Belief(model, belief_vector)
            s = b.random_state()
            
            if random.random() &lt; epsilon:
                a = random.choice(model.actions)
            else:
                best_alpha_index = xp.argmax(xp.dot(value_function.alpha_vector_array, b.values))
                a = value_function.actions[best_alpha_index]
            
            s_p = model.transition(s, a)
            o = model.observe(s_p, a)
            b_new = b.update(a, o)
            
            new_belief_array[i] = b_new.values
            
        return BeliefSet(model, new_belief_array)
    

    def expand_ssea(self, model:Model, belief_set:BeliefSet, max_generation:int=10) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Stochastic Simulation with Exploratory Action.
        Simulates running steps forward for each possible action knowing we are a state s, chosen randomly with according to the belief probability.
        These lead to a new state s_p and a observation o for each action.
        From all these and observation o we can generate updated beliefs. 
        Then it takes the belief that is furthest away from other beliefs, meaning it explores the most the belief space.

        Parameters
        ----------
        model : pomdp.Model
            The POMDP model on which to expand the belief set on.
        belief_set : BeliefSet
            List of beliefs to expand on.
        max_generation : int, default=10
            The max amount of beliefs that can be added to the belief set at once.

        Returns
        -------
        belief_set_new : BeliefSet
            Union of the belief_set and the expansions of the beliefs in the belief_set
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

        old_shape = belief_set.belief_array.shape
        to_generate = min(max_generation, old_shape[0])

        # Generation of successors
        successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])
        
        # Compute the distances between each pair and of successor are source beliefs
        diff = (belief_set.belief_array[:, None,None,None, :] - successor_beliefs)
        dist = xp.sqrt(xp.einsum(&#39;bnaos,bnaos-&gt;bnao&#39;, diff, diff))

        # Taking the min distance for each belief
        belief_min_dists = xp.min(dist,axis=0)

        # Taking the max distanced successors
        b_star, a_star, o_star = xp.unravel_index(xp.argsort(belief_min_dists, axis=None)[::-1][:to_generate], successor_beliefs.shape[:-1])

        # Selecting successor beliefs
        new_belief_array = successor_beliefs[b_star[:,None], a_star[:,None], o_star[:,None], model.states[None,:]]

        return BeliefSet(model, new_belief_array)
    

    def expand_ger(self, model:Model, belief_set:BeliefSet, value_function:ValueFunction, max_generation:int=10) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Greedy Error Reduction.
        It attempts to choose the believes that will maximize the improvement of the value function by minimizing the error.
        The error is computed by the sum of the change between two beliefs and their two corresponding alpha vectors.

        Parameters
        ----------
        model : pomdp.Model
            The POMDP model on which to expand the belief set on.
        belief_set : BeliefSet
            List of beliefs to expand on.
        value_function : ValueFunction
            Used to find the best action knowing the belief.
        max_generation : int, default=10
            The max amount of beliefs that can be added to the belief set at once.

        Returns
        -------
        belief_set_new : BeliefSet
            Union of the belief_set and the expansions of the beliefs in the belief_set
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

        old_shape = belief_set.belief_array.shape
        to_generate = min(max_generation, old_shape[0])

        new_belief_array = xp.empty((old_shape[0] + to_generate, old_shape[1]))
        new_belief_array[:old_shape[0]] = belief_set.belief_array

        # Finding the min and max rewards for computation of the epsilon
        r_min = model._min_reward / (1 - self.gamma)
        r_max = model._max_reward / (1 - self.gamma)

        # Generation of all potential successor beliefs
        successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])
        
        # Finding the alphas associated with each previous beliefs
        best_alpha = xp.argmax(xp.dot(belief_set.belief_array, value_function.alpha_vector_array.T), axis = 1)
        b_alphas = value_function.alpha_vector_array[best_alpha]

        # Difference between beliefs and their successors
        b_diffs = successor_beliefs - belief_set.belief_array[:,None,None,:]

        # Computing a &#39;next&#39; alpha vector made of the max and min
        alphas_p = xp.where(b_diffs &gt;= 0, r_max, r_min)

        # Difference between alpha vectors and their successors alpha vector
        alphas_diffs = alphas_p - b_alphas[:,None,None,:]

        # Computing epsilon for all successor beliefs
        eps = xp.einsum(&#39;baos,baos-&gt;bao&#39;, alphas_diffs, b_diffs)

        # Computing the probability of the b and doing action a and receiving observation o
        bao_probs = xp.einsum(&#39;bs,saor-&gt;bao&#39;, belief_set.belief_array, model.reachable_transitional_observation_table)

        # Taking the sumproduct of the probs with the epsilons
        res = xp.einsum(&#39;bao,bao-&gt;ba&#39;, bao_probs, eps)

        # Picking the correct amount of initial beliefs and ideal actions
        b_stars, a_stars = xp.unravel_index(xp.argsort(res, axis=None)[::-1][:to_generate], res.shape)

        # And picking the ideal observations
        o_star = xp.argmax(bao_probs[b_stars[:,None], a_stars[:,None], model.observations[None,:]] * eps[b_stars[:,None], a_stars[:,None], model.observations[None,:]], axis=1)

        # Selecting the successor beliefs
        new_belief_array = successor_beliefs[b_stars[:,None], a_stars[:,None], o_star[:,None], model.states[None,:]]

        return BeliefSet(model, new_belief_array)


    def expand_hsvi(self,
                    model:Model,
                    b:Belief,
                    value_function:ValueFunction,
                    upper_bound_belief_value_map:BeliefValueMapping,
                    conv_term:Union[float,None]=None,
                    max_generation:int=10
                    ) -&gt; BeliefSet:
        &#39;&#39;&#39;
        The expand function of the  Heruistic Search Value Iteration (HSVI) technique.
        It is a redursive function attempting to minimize the bound between the upper and lower estimations of the value function.

        It is developped by Smith T. and Simmons R. and described in the paper &#34;Heuristic Search Value Iteration for POMDPs&#34;.
        
        Parameters
        ----------
        model : pomdp.Model
            The model in which the exploration process will happen.
        b : Belief
            A belief to be added to the returned belief sequence and updated for the next step of the recursion.
        value_function : ValueFunction
            The lower bound of the value function.
        upper_bound_belief_value_map : BeliefValueMapping
            The upper bound of the value function.
            Initially it is define with the mdp policy of the model (run: &#34;BeliefValueMapping(model, mdp_policy)&#34;).
            It is then refined through the expansion process by adding newly found belief and value pairs.
        max_generation : int, default=10
            The maximum recursion depth that can be reached before the generated belief sequence is returned.
        
        Returns
        -------
        belief_set : BeliefSet
            A new sequence of beliefs.
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(b.values)

        if conv_term is None:
            conv_term = self.eps

        # Update convergence term
        conv_term /= self.gamma

        # Find best a based on upper bound v
        max_qv = -xp.inf
        best_a = -1
        for a in model.actions:
            b_probs = xp.einsum(&#39;sor,s-&gt;o&#39;, model.reachable_transitional_observation_table[:,a,:,:], b.values)

            b_prob_val = 0
            for o in model.observations:
                b_prob_val += (b_probs[o] * upper_bound_belief_value_map.evaluate(b.update(a,o)))
            
            qva = float(xp.dot(model.expected_rewards_table[:,a], b.values) + (self.gamma * b_prob_val))

            # qva = upper_bound_belief_value_map.qva(b, a, gamma=self.gamma)
            if qva &gt; max_qv:
                max_qv = qva
                best_a = a

        # Choose o that max gap between bounds
        b_probs = xp.einsum(&#39;sor,s-&gt;o&#39;, model.reachable_transitional_observation_table[:,best_a,:,:], b.values)

        max_o_val = -xp.inf
        best_v_diff = -xp.inf
        next_b = b

        for o in model.observations:
            bao = b.update(best_a, o)

            upper_v_bao = upper_bound_belief_value_map.evaluate(bao)
            lower_v_bao = xp.max(xp.dot(value_function.alpha_vector_array, bao.values))

            v_diff = (upper_v_bao - lower_v_bao)

            o_val = b_probs[o] * v_diff
            
            if o_val &gt; max_o_val:
                max_o_val = o_val
                best_v_diff = v_diff
                next_b = bao

        # if bounds_split &lt; conv_term or max_generation &lt;= 0:
        if best_v_diff &lt; conv_term or max_generation &lt;= 1:
            return BeliefSet(model, [next_b])
        
        # Add the belief point and associated value to the belief-value mapping
        upper_bound_belief_value_map.add(b, max_qv)

        # Go one step deeper in the recursion
        b_set = self.expand_hsvi(model=model,
                                 b=next_b,
                                 value_function=value_function,
                                 upper_bound_belief_value_map=upper_bound_belief_value_map,
                                 conv_term=conv_term,
                                 max_generation=max_generation-1)
        
        # Append the nex belief of this iteration to the deeper beliefs
        new_belief_list = b_set.belief_list
        new_belief_list.append(next_b)

        return BeliefSet(model, new_belief_list)


    def expand_fsvi(self,
                   model:Model,
                   b:Belief,
                   mdp_policy:ValueFunction,
                   s:Union[int, None]=None,
                   max_generation:int=10,
                   sequence_string:str=&#39;&#39;
                   ) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs following the the Forward Search Value Iteration principles.
        It is a recursive function that is started by a initial state &#39;s&#39; and using the MDP policy, chooses the best action to take.
        Following this, a random next state &#39;s_p&#39; is being sampled from the transition probabilities and a random observation &#39;o&#39; based on the observation probabilities.
        Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence.
        Once the state is a goal state, the recursion is done and the belief sequence is returned.

        Parameters
        ----------
        model : pomdp.Model
            The model in which the exploration process will happen.
        b : Belief
            A belief to be added to the returned belief sequence and updated for the next step of the recursion.
        mdp_policy : ValueFunction
            The mdp policy used to choose the action from with the given state &#39;s&#39;.
        s : int
            The state that starts the exploration sequence and based on which an action will be chosen.
        max_generation : int, default=10
            The maximum recursion depth that can be reached before the generated belief sequence is returned.
        sequence_string : str, default=&#39;&#39;
            The sequence of previously explored actions and observations.
        
        Returns
        -------
        belief_set : BeliefSet
            A new sequence of beliefs.
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(b.values)
        belief_list = [b]

        # If state not provided pick a random one
        if s is None:
            s = b.random_state()

        # If end is not reached
        if (s not in model.end_states) and (max_generation &gt; 0):
            # Choose action based on mdp value function
            a_star = xp.argmax(mdp_policy.alpha_vector_array[:,s])

            # Pick a random next state (weighted by transition probabilities)
            s_p = model.transition(s, a_star)

            # Pick a random observation weighted by observation probabilities in state s_p and after having done action a_star
            o = model.observe(s_p, a_star)

            # Update sequence string
            sequence_string += (&#39;-&#39; if len(sequence_string) &gt; 0 else &#39;&#39;) + f&#39;{a_star},{o}&#39;

            # Generate a new belief based on a_star and o
            b_p = b.update(a_star, o)

            # Recursive call to go closer to goal
            b_set = self.expand_fsvi(model=model,
                                     b=b_p,
                                     mdp_policy=mdp_policy,
                                     s=s_p,
                                     max_generation=max_generation-1,
                                     sequence_string=sequence_string)
            belief_list.extend(b_set.belief_list)
        
        return BeliefSet(model, belief_list)
    

    def expand_perseus(self,
                       model:Model,
                       b:Belief,
                       max_generation:int=10
                       ) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs.
        It is a recursive function that is started by a initial state &#39;s&#39; and using the MDP policy, chooses the best action to take.
        Following this, a random next state &#39;s_p&#39; is being sampled from the transition probabilities and a random observation &#39;o&#39; based on the observation probabilities.
        Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence.
        Once the state is a goal state, the recursion is done and the belief sequence is returned.

        Parameters
        ----------
        model : pomdp.Model
            The model in which the exploration process will happen.
        b : Belief
            A belief to be added to the returned belief sequence and updated for the next step of the recursion.
        max_generation : int, default=10
            The maximum recursion depth that can be reached before the generated belief sequence is returned.
        
        Returns
        -------
        belief_set : BeliefSet
            A new sequence of beliefs.
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(b.values)

        initial_belief = b
        belief_sequence = []

        for i in range(max_generation):
            # Choose random action
            a = int(xp.random.choice(model.actions, size=1)[0])

            # Choose random observation based on prob: P(o|b,a)
            obs_prob = xp.einsum(&#39;sor,s-&gt;o&#39;, model.reachable_transitional_observation_table[:,a,:,:], b.values)
            o = int(xp.random.choice(model.observations, size=1, p=obs_prob)[0])

            # Update belief
            bao = b.update(a,o)

            # Finalization
            belief_sequence.append(bao)
            b = bao

        return BeliefSet(model, belief_sequence)
    

    def expand(self, model:Model, belief_set:BeliefSet, max_generation:int, **function_specific_parameters) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Central method to call one of the functions for a particular expansion strategy:
            - Random selction (RA)
            - Stochastic Simulation with Random Action (ssra)
            - Stochastic Simulation with Greedy Action (ssga)
            - Stochastic Simulation with Exploratory Action (ssea)
            - Greedy Error Reduction (ger)
            - Heuristic Search Value Iteration (hsvi)
            - Forward Search Value Iteration (fsvi)
            - Perseus (perseus)
                
        Parameters
        ----------
        model : pomdp.Model
            The model on which to run the belief expansion on.
        belief_set : BeliefSet
            The set of beliefs to expand.
        max_generation : int
            The max amount of beliefs that can be added to the belief set at once.
        function_specific_parameters
            Potential additional parameters necessary for the specific expand function.

        Returns
        -------
        belief_set_new : BeliefSet
            The belief set the expansion function returns. 
        &#39;&#39;&#39;
        if self.expand_function in &#39;expand_ra&#39;:
            return self.expand_ra(model=model, belief_set=belief_set, max_generation=max_generation)

        elif self.expand_function in &#39;expand_ssra&#39;:
            return self.expand_ssra(model=model, belief_set=belief_set, max_generation=max_generation)
        
        elif self.expand_function in &#39;expand_ssga&#39;:
            args = {arg: function_specific_parameters[arg] for arg in [&#39;value_function&#39;, &#39;epsilon&#39;] if arg in function_specific_parameters}
            return self.expand_ssga(model=model, belief_set=belief_set, max_generation=max_generation, **args)
        
        elif self.expand_function in &#39;expand_ssea&#39;:
            return self.expand_ssea(model=model, belief_set=belief_set, max_generation=max_generation)
        
        elif self.expand_function in &#39;expand_ger&#39;:
            args = {arg: function_specific_parameters[arg] for arg in [&#39;value_function&#39;] if arg in function_specific_parameters}
            return self.expand_ger(model=model, belief_set=belief_set, max_generation=max_generation, **args)
        
        elif self.expand_function in &#39;expand_hsvi&#39;:
            args = {arg: function_specific_parameters[arg] for arg in [&#39;value_function&#39;, &#39;mdp_policy&#39;] if arg in function_specific_parameters}
            if not hasattr(self, &#39;_upper_bound&#39;):
                self._upper_bound = BeliefValueMapping(model, args[&#39;mdp_policy&#39;])
            else:
                self._upper_bound.update()
            return self.expand_hsvi(model=model, 
                                    b=belief_set.belief_list[0],
                                    value_function=args[&#39;value_function&#39;],
                                    upper_bound_belief_value_map=self._upper_bound,
                                    max_generation=max_generation)
        
        elif self.expand_function in &#39;expand_fsvi&#39;:
            args = {arg: function_specific_parameters[arg] for arg in [&#39;mdp_policy&#39;] if arg in function_specific_parameters}
            return self.expand_fsvi(model=model, 
                                    b=belief_set.belief_list[0],
                                    mdp_policy=args[&#39;mdp_policy&#39;],
                                    max_generation=max_generation)
        
        elif self.expand_function in &#39;expand_perseus&#39;:
            return self.expand_perseus(model=model, b=belief_set.belief_list[0], max_generation=max_generation)
        
        else:
            raise Exception(&#39;Not implemented&#39;)

        return []


    def compute_change(self, value_function:ValueFunction, new_value_function:ValueFunction, belief_set:BeliefSet) -&gt; float:
        &#39;&#39;&#39;
        Function to compute whether the change between two value functions can be considered as having converged based on the eps parameter of the Solver.
        It check for each belief, the maximum value and take the max change between believe&#39;s value functions.
        If this max change is lower than eps * (gamma / (1 - gamma)).

        Parameters
        ----------
        value_function : ValueFunction
            The first value function to compare.
        new_value_function : ValueFunction
            The second value function to compare.
        belief_set : BeliefSet
            The set of believes to check the values on to compute the max change on.

        Returns
        -------
        max_change : float
            The maximum change between value functions at belief points.
        &#39;&#39;&#39;
        # Get numpy corresponding to the arrays
        xp = np if not gpu_support else cp.get_array_module(value_function.alpha_vector_array)

        # Computing Delta for each beliefs
        max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, value_function.alpha_vector_array.T), axis=1)
        new_max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, new_value_function.alpha_vector_array.T), axis=1)
        max_change = xp.max(xp.abs(new_max_val_per_belief - max_val_per_belief))

        return max_change


    def solve(self,
              model:Model,
              expansions:int,
              full_backup:Union[bool,None]=None,
              update_passes:int=1,
              max_belief_growth:int=10,
              initial_belief:Union[BeliefSet, Belief, None]=None,
              initial_value_function:Union[ValueFunction,None]=None,
              prune_level:int=1,
              prune_interval:int=10,
              use_gpu:bool=False,
              history_tracking_level:int=1,
              print_progress:bool=True
              ) -&gt; tuple[ValueFunction, SolverHistory]:
        &#39;&#39;&#39;
        Main loop of the Point-Based Value Iteration algorithm.
        It consists in 2 steps, Backup and Expand.
        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function
        2. Backup: Updates the alpha vectors based on the current belief set

        Depending on the expand strategy chosen, various extra parameters are needed. List of the available expand strategies and their extra required parameters:
            - ssra: Stochastic Simulation with Random Action. Extra params: /
            - ssga: Stochastic Simulation with Greedy Action. Extra params: epsilon (float)
            - ssea: Stochastic Simulation with Exploratory Action. Extra params: /
            - ger: Greedy Error Reduction. Extra params: /
            - hsvi: Heuristic Search Value Iteration. Extra param: mdp_policy (ValueFunction)
            - fsvi: Forward Search Value Iteration: Extra param: mdp_policy (ValueFunction)
            - perseus: Perseus. Extra params: /

        Parameters
        ----------
        model : pomdp.Model
            The model to solve.
        expansions : int
            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)
        full_backup : bool, optional
            Whether to force the backup function has to be run on the full set beliefs uncovered since the beginning or only on the new points.
            By default, it will be determined by which expand function is chosen (False if: fsvi, hsvi, perseus; True otherwise)
        update_passes : int, default=1
            How many times the backup function has to be run every time the belief set is expanded.
        max_belief_growth : int, default=10
            How many beliefs can be added at every expansion step to the belief set.
        initial_belief : BeliefSet or Belief, optional
            An initial list of beliefs to start with.
        initial_value_function : ValueFunction, optional
            An initial value function to start the solving process with.
        prune_level : int, default=1
            Parameter to prune the value function further before the expand function.
        prune_interval : int, default=10
            How often to prune the value function. It is counted in number of backup iterations.
        use_gpu : bool, default=False
            Whether to use the GPU with cupy array to accelerate solving.
        history_tracking_level : int, default=1
            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)
        print_progress : bool, default=True
            Whether or not to print out the progress of the value iteration process.

        Returns
        -------
        value_function : ValueFunction
            The alpha vectors approximating the value function.
        solver_history : SolverHistory
            The history of the solving process with some plotting options.
        &#39;&#39;&#39;
        # numpy or cupy module
        xp = np

        # If GPU usage
        if use_gpu:
            assert gpu_support, &#34;GPU support is not enabled, Cupy might need to be installed...&#34;
            model = model.gpu_model

            # Replace numpy module by cupy for computations
            xp = cp

        # Initial belief
        if initial_belief is None:
            belief_set = BeliefSet(model, [Belief(model)])
        elif isinstance(initial_belief, BeliefSet):
            belief_set = initial_belief.to_gpu() if use_gpu else initial_belief 
        else:
            initial_belief = Belief(model, xp.array(initial_belief.values))
            belief_set = BeliefSet(model, [initial_belief])
        
        # Initial value function
        if initial_value_function is None:
            value_function = ValueFunction(model, model.expected_rewards_table.T, model.actions)
        else:
            value_function = initial_value_function.to_gpu() if use_gpu else initial_value_function

        # Full backup setter if not forced
        if full_backup is None:
            full_backup = any([self.expand_function in func for func in [&#39;expand_ra&#39;, &#39;expand_ssra&#39;, &#39;expand_ssga&#39;, &#39;expand_ssea&#39;, &#39;expand_ger&#39;]])

        # For hsvi of fsvi, mdp policy is required as upper bound, so if it is not required, generate it
        if (&#39;mdp_policy&#39; not in self.expand_function_params) or (self.expand_function_params[&#39;mdp_policy&#39;] is None):
            log(&#39;[Warning] MDP solution not provided, running value iteration on the problem to retrieve it...&#39;)
            vi_solver = VI_Solver(gamma=self.gamma, eps=self.eps)

            log(&#39;    &gt; Starting MDP Value Iteration...&#39;)
            mdp_solution, hist = vi_solver.solve(model,
                                                 use_gpu=use_gpu,
                                                 print_progress=False)
            
            log(f&#39;    &gt; Value Iteration stopped or converged in {sum(hist.iteration_times):.3f}s, and after {len(hist.iteration_times)} iteration.\n&#39;)

            self.expand_function_params[&#39;mdp_policy&#39;] = mdp_solution

        # Convergence check boundary
        max_allowed_change = self.eps * (self.gamma / (1-self.gamma))

        # History tracking
        solver_history = SolverHistory(tracking_level=history_tracking_level,
                                       model=model,
                                       gamma=self.gamma,
                                       eps=self.eps,
                                       expand_function=self.expand_function,
                                       expand_append=full_backup,
                                       initial_value_function=value_function,
                                       initial_belief_set=belief_set)

        # Loop
        iteration = 0
        expand_value_function = value_function
        old_value_function = value_function

        try:
            for expansion_i in range(expansions) if not print_progress else trange(expansions, desc=&#39;Expansions&#39;):

                # 1: Expand belief set
                start_ts = datetime.now()

                new_belief_set = self.expand(model=model,
                                             belief_set=belief_set,
                                             value_function=value_function,
                                             max_generation=max_belief_growth,
                                             **self.expand_function_params)

                # If full backup append the newly generated set to the old belief_set
                if full_backup:
                    belief_set = BeliefSet(model, xp.vstack((belief_set.belief_array, new_belief_set.belief_array)))
                else:
                    belief_set = new_belief_set

                expand_time = (datetime.now() - start_ts).total_seconds()
                solver_history.add_expand_step(expansion_time=expand_time, belief_set=belief_set)

                # 2: Backup, update value function (alpha vector set)
                for _ in range(update_passes) if (not print_progress or update_passes &lt;= 1) else trange(update_passes, desc=f&#39;Backups {expansion_i}&#39;):
                    start_ts = datetime.now()

                    # Backup step
                    value_function = self.backup(model,
                                                 belief_set,
                                                 value_function,
                                                 append=(not full_backup),
                                                 belief_dominance_prune=False)
                    backup_time = (datetime.now() - start_ts).total_seconds()

                    # Additional pruning
                    if (iteration % prune_interval) == 0 and iteration &gt; 0:
                        start_ts = datetime.now()
                        vf_len = len(value_function)

                        value_function.prune(prune_level)

                        prune_time = (datetime.now() - start_ts).total_seconds()
                        alpha_vectors_pruned = len(value_function) - vf_len
                        solver_history.add_prune_step(prune_time, alpha_vectors_pruned)
                    
                    # Compute the change between value functions
                    max_change = self.compute_change(value_function, old_value_function, belief_set)

                    # History tracking
                    solver_history.add_backup_step(backup_time, max_change, value_function)

                    # Convergence check
                    if max_change &lt; max_allowed_change:
                        break

                    old_value_function = value_function

                    # Update iteration counter
                    iteration += 1

                # Compute change with old expansion value function
                expand_max_change = self.compute_change(expand_value_function, value_function, belief_set)

                if expand_max_change &lt; max_allowed_change:
                    print(&#39;Converged!&#39;)
                    break

                expand_value_function = value_function
        except MemoryError as e:
            print(f&#39;Memory full: {e}&#39;)
            print(&#39;Returning value function and history as is...\n&#39;)

        # Final pruning
        start_ts = datetime.now()
        vf_len = len(value_function)

        value_function.prune(prune_level)

        prune_time = (datetime.now() - start_ts).total_seconds()
        alpha_vectors_pruned = len(value_function) - vf_len
        solver_history.add_prune_step(prune_time, alpha_vectors_pruned)

        return value_function, solver_history


class HSVI_Solver(PBVI_Solver):
    &#39;&#39;&#39;
    TODO
    &#39;&#39;&#39;
    def __init__(self,
                 gamma:float=0.99,
                 eps:float=0.001,
                 mdp_solution:Union[ValueFunction,None]=None):

        super().__init__(gamma=gamma,
                         eps=eps,
                         expand_function=&#39;hsvi&#39;,
                         mdp_policy=mdp_solution)
        
    def solve(self,
              model:Model,
              expansions:int,
              max_belief_growth:int=10,
              initial_belief:Union[BeliefSet,Belief,None]=None,
              initial_value_function:Union[ValueFunction,None]=None,
              prune_level:int=1,
              prune_interval:int=10,
              use_gpu:bool=False,
              history_tracking_level:int=1,
              print_progress:bool=True
              ) -&gt; tuple[ValueFunction, SolverHistory]:
        return super().solve(model=model,
                             expansions=expansions,
                             full_backup=False,
                             update_passes=1,
                             max_belief_growth=max_belief_growth,
                             initial_belief=initial_belief,
                             initial_value_function=initial_value_function,
                             prune_level=prune_level,
                             prune_interval=prune_interval,
                             use_gpu=use_gpu,
                             history_tracking_level=history_tracking_level,
                             print_progress=print_progress)


class FSVI_Solver(PBVI_Solver):
    &#39;&#39;&#39;
    Solver to solve a POMDP problem based on the Forward Search Value Iteration principle.
    It has been built based on the paper of G. Shani, R. I. Brafman, and S. I. Shimony: &#39;Forward Search Value Iteration for POMDPS&#39;.
    It works by utilizing the optimal MDP policy to generate paths to explore that lead to series of Beliefs
    that can then be used by the Backup function to update the value function.

    The solve function is the same as the pomdp.PBVI_Solver class with some parameters prefilled.

    Note:
    - The backup mode is set to new points only
    - Only one update/backup is run for each iteration.
    
    ...
    Parameters
    ----------
    gamma : float, default=0.9
        The learning rate, used to control how fast the value function will change after the each iterations.
    eps : float, default=0.001
        The treshold for convergence. If the max change between value function is lower that eps, the algorithm is considered to have converged.
    mdp_policy : ValueFunction, optional
        The value that will be used to decide actions during the MDP explore procedure. If not provided, it will be computed at solve time.

    Attributes
    ----------
    gamma : float
    eps : float
    &#39;&#39;&#39;
    def __init__(self,
                 gamma:float=0.9,
                 eps:float=0.001,
                 mdp_policy:Union[ValueFunction,None]=None):
        super().__init__(gamma=gamma,
                         eps=eps,
                         expand_function=&#39;fsvi&#39;,
                         mdp_policy=mdp_policy)
        

    def solve(self,
              model:Model,
              expansions:int,
              max_belief_growth:int=10,
              initial_belief:Union[BeliefSet, Belief, None]=None,
              initial_value_function:Union[ValueFunction, None]=None,
              prune_level:int=1,
              prune_interval:int=10,
              use_gpu:bool=False,
              history_tracking_level:int=1,
              print_progress:bool=True
              ) -&gt; tuple[ValueFunction, SolverHistory]:
        return super().solve(model=model,
                             expansions=expansions,
                             full_backup=False,
                             update_passes=1,
                             max_belief_growth=max_belief_growth,
                             initial_belief=initial_belief,
                             initial_value_function=initial_value_function,
                             prune_level=prune_level,
                             prune_interval=prune_interval,
                             use_gpu=use_gpu,
                             history_tracking_level=history_tracking_level,
                             print_progress=print_progress)


class SimulationHistory(MDP_SimulationHistory):
    &#39;&#39;&#39;
    Class to represent a list of rewards received during a Simulation.
    The main purpose of the class is to provide a set of visualization options of the rewards received.

    Multiple types of plots can be done:
        - Totals: to plot a graph of the accumulated rewards over time.
        - Moving average: to plot the moving average of the rewards received over time.
        - Histogram: to plot a histogram of the various rewards received.

    ...

    Parameters
    ----------
    model: mdp.Model
        The model on which the simulation happened on.
    start_state: int
        The initial state in the simulation.
    start_belief: Belief
        The initial belief the agent starts with during the simulation.

    Attributes
    ----------
    model : mdp.Model
    states : list[int]
        A list of recorded states through which the agent passed by during the simulation process.
    grid_point_sequence : list[list[int]]
        A list of 2D points of the grid state through which the agent passed by during the simulation process.
    actions : list[int]
        A list of recorded actions the agent took during the simulation process.
    rewards: RewardSet
        The set of rewards received by the agent throughout the simulation process.
    beliefs : list[Belief]
        A list of recorded beliefs the agent is in throughout the simulation process.
    observations : list[int]
        A list of recorded observations gotten by the agent during the simulation process.
    &#39;&#39;&#39;
    def __init__(self, model:Model, start_state:int, start_belief:Belief):
        super().__init__(model, start_state)
        self._beliefs = [start_belief]
        self.observations = []


    @property
    def beliefs(self) -&gt; list[Belief]:
        if len(self._beliefs) &lt; len(self):
            init_belief = self._beliefs[0]

            self._beliefs = [init_belief]

            # Generation of belief sequence based on actions and observations
            belief = init_belief
            for a, o in zip(self.actions, self.observations):
                new_belief = belief.update(a,o)
                self._beliefs.append(new_belief)
                belief = new_belief

        return self._beliefs


    def add(self, action:int, reward, next_state:int, next_belief:Belief, observation:int) -&gt; None:
        &#39;&#39;&#39;
        Function to add a step in the simulation history

        Parameters
        ----------
        action : int
            The action that was taken by the agent.
        reward
            The reward received by the agent after having taken action.
        next_state : int
            The state that was reached by the agent after having taken action.
        next_belief : Belief
            The new belief of the agent after having taken an action and received an observation.
        observation : int
            The observation the agent received after having made an action.
        &#39;&#39;&#39;
        super().add(action, reward, next_state)
        self._beliefs.append(next_belief)
        self.observations.append(observation)


    # Overwritten
    def to_dataframe(self, include_beliefs:bool=False) -&gt; pd.DataFrame:
        &#39;&#39;&#39;
        Returns a pandas dataframe representation of the simulation history.

        Note: Beliefs not saved as the sequence can be recreated from the sequence of action-observation pairs.

        Parameters
        ----------
        include_beliefs : bool, default=False # TODO: implement option
            Whether or not to include the beliefs in the dataframe or not. Doing so requires potential a large amount of memory.
        &#39;&#39;&#39;
        df = super().to_dataframe()
        df[&#39;Observations&#39;] = self.observations + [None]

        if include_beliefs:
            belief_array = np.array([b.values.tolist() for b in self.beliefs])
            belief_df = pd.DataFrame(belief_array, columns=[f&#39;B_{sl}&#39; for sl in self.model.state_labels])

            df = pd.concat([df, belief_df], axis=1)

        return df
    

    # Overwritten
    def save(self, path:str=&#39;./Simulations&#39;, file_name:Union[str,None]=None, include_beliefs:bool=False) -&gt; None:
        &#39;&#39;&#39;
        Function to save the simulation history in a file at a given path. If no path is provided, it will be saved in a subfolder (Simulations) inside the current working directory.
        If no file_name is provided, it be saved as &#39;&lt;current_timestamp&gt;_simulation.csv&#39;.

        Parameters
        ----------
        path : str, default=&#39;./Simulations&#39;
            The path at which the csv will be saved.
        file_name : str, default=&#39;&lt;current_timestamp&gt;_simulation.csv&#39;
            The file name used to save in.
        &#39;&#39;&#39;
        if not os.path.exists(path):
            print(&#39;Path &#34;{path}&#34; does not exist yet, creating it...&#39;)
            os.makedirs(path)
            
        if file_name is None:
            timestamp = datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)
            file_name = timestamp + &#39;_simulation.csv&#39;

        if not file_name.endswith(&#39;.csv&#39;):
            file_name += &#39;.csv&#39;
        
        if not include_beliefs:
            print(&#39;[Warning] Beliefs not saved with simulation history but the belief sequence can be recreated from the actions and observations.&#39;)

        df = self.to_dataframe(include_beliefs=include_beliefs)

        df.to_csv(path + &#39;/&#39; + file_name, index=False)
        print(f&#39;Saved to: {path}/{file_name}&#39;)
    

    # Overwritten
    def _plot_to_frame_on_ax(self, frame_i, ax):
        model = self.model.cpu_model

        # Data
        data = np.array(self.grid_point_sequence)[:(frame_i+1),:]
        belief = self.beliefs[frame_i]
        belief_values = belief.values if (not gpu_support) or (cp.get_array_module(belief.values) == np) else cp.asnumpy(belief.values)
        observations = self.observations[:(frame_i)]
        obs_colors = [&#39;#000000&#39;] + [COLOR_LIST[o][&#39;hex&#39;] for o in observations]

        # Ticks
        dimensions = model.state_grid.shape
        x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))
        y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))

        # Plotting
        ax.clear()
        ax.set_title(f&#39;Simulation (Frame {frame_i})&#39;)

        # Observation labels legend
        proxy = [patches.Rectangle((0,0),1,1,fc = COLOR_LIST[o][&#39;id&#39;]) for o in model.observations]
        ax.legend(proxy, model.observation_labels, title=&#39;Observations&#39;) # type: ignore

        grid_values = belief_values[model.state_grid]
        ax.imshow(grid_values, cmap=&#39;Blues&#39;)
        ax.plot(data[:,1], data[:,0], color=&#39;red&#39;, zorder=-1)
        ax.scatter(data[:,1], data[:,0], c=obs_colors)

        ax.set_xticks(x_ticks)
        ax.set_yticks(y_ticks)


class Simulation(MDP_Simulation):
    &#39;&#39;&#39;
    Class to reprensent a simulation process for a POMDP model.
    An initial random state is given and action can be applied to the model that impact the actual state of the agent along with returning a reward and an observation.

    ...

    Parameters
    ----------
    model: pomdp.Model
        The POMDP model the simulation will be applied on.

    Attributes
    ----------
    model: pomdp.Model
    agent_state : int
        The agent&#39;s state in the running simulation
    is_done : bool
        Whether or not the agent has reached an end state or performed an ending action.
    &#39;&#39;&#39;
    def __init__(self, model:Model) -&gt; None:
        super().__init__(model)
        self.model = model


    def run_action(self, a:int) -&gt; tuple[Union[int,float], int]:
        &#39;&#39;&#39;
        Run one step of simulation with action a.

        Parameters
        ----------
        a : int
            The action to take in the simulation.

        Returns
        -------
        r : int or float
            The reward given when doing action a in state s and landing in state s_p. (s and s_p are hidden from agent)
        o : int
            The observation following the action applied on the previous state.
        &#39;&#39;&#39;
        assert not self.is_done, &#34;Action run when simulation is done.&#34;

        s = self.agent_state
        s_p = self.model.transition(s, a)
        o = self.model.observe(s_p, a)
        r = self.model.reward(s, a, s_p, o)

        # Update agent state
        self.agent_state = s_p

        # State Done check
        if s_p in self.model.end_states:
            self.is_done = True

        # Action Done check
        if a in self.model.end_actions:
            self.is_done = True

        return (r, o)


class Agent:
    &#39;&#39;&#39;
    The class of an Agent running on a POMDP model.
    It has the ability to train using a given solver (here PBVI_Solver).
    Then, once trained, it can simulate actions with a given Simulation,
    either for a given amount of steps or until a single reward is received.

    ...

    Parameters
    ----------
    model: pomdp.Model
        The model in which the agent can run
    value_function : ValueFunction, optional
        A value function the agent can use to play a simulation, in the case the model has been solved beforehand.
    
    Attributes
    ----------
    model: pomdp.Model
    value_function : ValueFunction
        The value function the agent has come up to after training.
    &#39;&#39;&#39;
    def __init__(self, model:Model, value_function:Union[ValueFunction,None]=None) -&gt; None:
        self.model = model
        self.value_function = value_function


    def train(self, solver:PBVI_Solver, expansions:int, horizon:int) -&gt; SolverHistory:
        &#39;&#39;&#39;
        Method to train the agent using a given solver.
        The solver will provide a value function that will map beliefs in belief space to actions.

        Parameters
        ----------
        solver : PBVI_Solver
            The solver to run.
        expansions : int
            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)
        horizon : int
            How many times the alpha vector set must be updated every time the belief set is expanded.
        
        Returns
        -------
        solve_history : SolverHistory
            The history of the solving process.
        &#39;&#39;&#39;
        self.value_function, solve_history = solver.solve(self.model, expansions, horizon)
        return solve_history


    def get_best_action(self, belief:Belief) -&gt; int:
        &#39;&#39;&#39;
        Function to retrieve the best action for a given belief based on the value function retrieved from the training.

        Parameters
        ----------
        belief : Belief
            The belief to get the best action with.
                
        Returns
        -------
        best_action : int
            The best action found.
        &#39;&#39;&#39;
        assert self.value_function is not None, &#34;No value function, training probably has to be run...&#34;

        # GPU
        xp = np if not self.value_function.is_on_gpu else cp

        best_vector = xp.argmax(xp.dot(self.value_function.alpha_vector_array, belief.values))
        best_action = int(self.value_function.actions[best_vector])

        return best_action


    def simulate(self,
                 simulator:Union[Simulation,None]=None,
                 max_steps:int=1000,
                 start_state:Union[int,None]=None,
                 print_progress:bool=True,
                 print_stats:bool=True
                 ) -&gt; SimulationHistory:
        &#39;&#39;&#39;
        Function to run a simulation with the current agent for up to &#39;max_steps&#39; amount of steps using a Simulation simulator.

        Parameters
        ----------
        simulator : pomdp.Simulation, optional
            The simulation that will be used by the agent. If not provided, the default MDP simulator will be used.
        max_steps : int, default=1000
            The max amount of steps the simulation can run for.
        start_state : int, optional
            The state the agent should start in, if not provided, will be set at random based on start probabilities of the model.
        print_progress : bool, default=True
            Whether or not to print out the progress of the simulation.
        print_stats : bool, default=True
            Whether or not to print simulation statistics at the end of the simulation.

        Returns
        -------
        history : SimulationHistory
            A list of rewards with the additional functionality that the can be plot with the plot() function.
        &#39;&#39;&#39;
        assert self.value_function is not None, &#34;No value function, training probably has to be run...&#34;

        # GPU setup
        self.model = self.model.gpu_model if self.value_function.is_on_gpu else self.model.cpu_model

        # Get or generate a default simulator
        if simulator is None:
            simulator = Simulation(self.model)

        # reset
        s = simulator.initialize_simulation(start_state=start_state) # s is only used for the simulation history
        belief = Belief(self.model)

        history = SimulationHistory(self.model, start_state=s, start_belief=belief)

        sim_start_ts = datetime.now()

        # Simulation loop
        for _ in (trange(max_steps) if print_progress else range(max_steps)):
            # Play best action
            a = self.get_best_action(belief)
            r,o = simulator.run_action(a)

            # Update the belief
            new_belief = belief.update(a, o)

            # Post action history recording
            history.add(action=a, next_state=simulator.agent_state, next_belief=new_belief, reward=r, observation=o)

            # Replace belief
            belief = new_belief

            # If simulation is considered done, the rewards are simply returned
            if simulator.is_done:
                break
            
        if print_stats:
            sim_end_ts = datetime.now()
            print(&#39;Simulation done:&#39;)
            print(f&#39;\t- Runtime (s): {(sim_end_ts - sim_start_ts).total_seconds()}&#39;)
            print(f&#39;\t- Steps: {len(history.states)}&#39;)
            print(f&#39;\t- Total rewards: {sum(history.rewards)}&#39;)
            print(f&#39;\t- End state: {self.model.state_labels[history.states[-1]]}&#39;)

        return history


    def run_n_simulations(self,
                          simulator:Union[Simulation,None]=None,
                          n:int=1000,
                          max_steps:int=1000,
                          start_state:Union[int,None]=None,
                          reward_discount:float=0.99,
                          print_progress:bool=True,
                          print_stats:bool=True
                          ) -&gt; Tuple[RewardSet, list[SimulationHistory]]:
        &#39;&#39;&#39;
        Function to run a set of simulations in a row.
        This is useful when the simulation has a &#39;done&#39; condition.
        In this case, the rewards of individual simulations are summed together under a single number.

        Parameters
        ----------
        simulator : pomdp.Simulation, optional
            The simulation that will be used by the agent. If not provided, the default MDP simulator will be used. (Optional)
        n : int, default=1000
            The amount of simulations to run.
        max_steps : int, default=1000
            The max_steps to run per simulation.
        start_state : int, optional
            The state the agent should start in, if not provided, will be set at random based on start probabilities of the model (Default: random)
        reward_discount : float, default=0.99
            The reward discount to compute the Average Discounted Reward (ADR).
        print_progress : bool, default=True
            Whether or not to print out the progress of the simulation.
        print_stats : bool, default=True
            Whether or not to print simulation statistics at the end of the simulation.

        Returns
        -------
        all_histories : RewardSet
            A list of the final rewards after each simulation.
        all_histories : list[SimulationHistory]
            A list the simulation histories gathered at each simulation.
        &#39;&#39;&#39;
        if simulator is None:
            simulator = Simulation(self.model)

        sim_start_ts = datetime.now()

        all_histories = []
        all_final_rewards = RewardSet()
        all_discounted_rewards = []
        all_sim_length = []
        done_sim_count = 0
        for _ in (trange(n) if print_progress else range(n)):
            sim_history = self.simulate(simulator, max_steps, start_state, False, False)

            if simulator.is_done:
                done_sim_count += 1

            all_histories.append(sim_history)
            all_final_rewards.append(np.sum(sim_history.rewards))
            all_discounted_rewards.append(sim_history.rewards.get_total_discounted_reward(reward_discount))
            all_sim_length.append(len(sim_history))

        if print_stats:
            sim_end_ts = datetime.now()
            print(f&#39;All {n} simulations done:&#39;)
            print(f&#39;\t- Average runtime (s): {((sim_end_ts - sim_start_ts).total_seconds() / n)}&#39;)
            print(f&#39;\t- Simulations reached goal: {done_sim_count}/{n} ({n-done_sim_count} failures)&#39;)
            print(f&#39;\t- Average step count: {(sum(all_sim_length) / n)}&#39;)
            print(f&#39;\t- Average total rewards: {(sum(all_final_rewards) / n)}&#39;)
            print(f&#39;\t- Average discounted rewards (ADR): {(sum(all_discounted_rewards) / n)}&#39;)

        return all_final_rewards, all_histories


    def run_n_simulations_parallel(self,
                                   n:int=1000,
                                   max_steps:int=1000,
                                   start_state:Union[list[int],int,None]=None,
                                   reward_discount:float=0.99,
                                   print_progress:bool=True,
                                   print_stats:bool=True
                                   ) -&gt; Tuple[RewardSet, list[SimulationHistory]]:
        &#39;&#39;&#39;
        TODO
        &#39;&#39;&#39;
        # GPU support
        xp = np if not self.value_function.is_on_gpu else cp
        model = self.model.cpu_model if not self.value_function.is_on_gpu else self.model.gpu_model

        # Genetion of an array of n beliefs
        initial_beliefs = xp.repeat(Belief(model).values[None,:], n, axis=0)

        # Generating n initial positions
        start_state_array = xp.empty(n)
        if isinstance(start_state, int):
            start_state_array = start_state
        elif isinstance(start_state, list):
            repeated_list = xp.repeat(xp.array(start_state), int(np.ceil(n / len(start_state))))
            start_state_array = xp.resize(repeated_list, n)
        else:
            start_state_array = xp.random.choice(model.states, size=n, p=model.start_probabilities)
        
        # Belief and state arrays
        beliefs = initial_beliefs
        new_beliefs = None
        states = start_state_array
        next_states = None

        # Tracking what simulations are done
        sim_is_done = xp.zeros(n, dtype=bool)
        done_at_step = xp.full(n, -1)

        # Speedup item
        simulations = xp.arange(n)
        flatten_offset = (simulations[:,None] * model.state_count)
        flat_shape = (n, (model.state_count * model.reachable_state_count))

        # 2D bincount for belief set update
        def bincount2D_vectorized(a, w):    
            a_offs = a + flatten_offset
            return xp.bincount(a_offs.ravel(), weights=w.ravel(), minlength=a.shape[0]*model.state_count).reshape(-1,model.state_count)

        # Results
        discount = reward_discount
        rewards = []
        discounted_rewards = []

        # History items
        states_history = [states]
        actions_history = []
        observations_history = []

        sim_start_ts = datetime.now()
        
        iterator = trange(max_steps) if print_progress else range(max_steps)
        for i in iterator:
            # Retrieving the top vectors according to the value function
            best_vectors = xp.argmax(xp.matmul(beliefs, self.value_function.alpha_vector_array.T), axis=1)

            # Retrieving the actions associated with the vectors chosen
            best_actions = self.value_function.actions[best_vectors]

            # Get each reachable next states for each action
            reachable_state_per_actions = model.reachable_states[:, best_actions, :]

            # Gathering new states based on the transition function and the chosen actions
            next_state_potentials = reachable_state_per_actions[states, simulations]
            if model.reachable_state_count == 1:
                next_states = next_state_potentials[:,0]
            else:
                potential_probabilities = model.reachable_probabilities[states[:,None], best_actions[:,None]][:,0,:]
                chosen_indices = xp.apply_along_axis(lambda x: xp.random.choice(len(x), size=1, p=x), axis=1, arr=potential_probabilities)
                next_states = next_state_potentials[chosen_indices][:,0,0]

            # Making observations based on the states landed in and the action that was taken
            observation_probabilities = model.observation_table[next_states[:,None], best_actions[:,None]][:,0,:]
            observations = xp.sum(xp.random.random(n)[:,None] &gt; xp.cumsum(observation_probabilities[:,:-1], axis=1), axis=1)

            # Belief set update
            reachable_probabilities = (model.reachable_transitional_observation_table[:,best_actions,observations,:] * beliefs.T[:,:,None])
            new_beliefs = bincount2D_vectorized(a=reachable_state_per_actions.swapaxes(0,1).reshape(flat_shape),
                                                w=reachable_probabilities.swapaxes(0,1).reshape(flat_shape))

            new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]

            # Rewards computation
            step_rewards = xp.array([model.immediate_reward_function(int(s), int(a), int(s_p), int(o)) for s,a,s_p,o in zip(states, best_actions, next_states, observations)])
            rewards.append(xp.where(~sim_is_done, step_rewards, 0))
            discounted_rewards.append(xp.where(~sim_is_done, step_rewards * discount, 0))

            # Checking for done condition
            are_done = xp.isin(next_states, xp.array(model.end_states))
            done_at_step[sim_is_done ^ are_done] = i+1
            sim_is_done |= are_done

            # Update iterator postfix
            if print_progress:
                iterator.set_postfix({&#39;done&#39;: xp.sum(sim_is_done)})

            # Replacing old with new
            states = next_states
            beliefs = new_beliefs
            discount *= reward_discount

            # History tracking
            states_history.append(states)
            actions_history.append(best_actions)
            observations_history.append(observations)

            # Early stopping
            if xp.all(sim_is_done):
                break

        # Wrap up
        sim_hist_list = []
        b0 = Belief(model)

        states_history = xp.array(states_history).T
        actions_history = xp.array(actions_history).T
        observations_history = xp.array(observations_history).T
        rewards = xp.array(rewards).T
        discounted_rewards = xp.array(discounted_rewards).T

        done_at_step_sum = 0

        for i, s0 in enumerate(start_state_array):
            sim_hist = SimulationHistory(self.model, int(s0), b0)
            
            last_step = int(done_at_step[i]) if bool(sim_is_done[i]) else max_steps
            done_at_step_sum += last_step

            # Setting the elements
            sim_hist.states = states_history[i,:last_step+1].tolist()
            sim_hist.actions = actions_history[i,:last_step].tolist()
            sim_hist.observations = observations_history[i,:last_step].tolist()
            sim_hist.rewards = rewards[i,:last_step].tolist()

            # Adding the sim history to the list of histories
            sim_hist_list.append(sim_hist)

        done_sim_count = sum([1 if done else 0 for done in sim_is_done])

        if print_stats:
            sim_end_ts = datetime.now()
            print(f&#39;All {n} simulations done in {(sim_end_ts - sim_start_ts).total_seconds():.3f}s:&#39;)
            print(f&#39;\t- Simulations reached goal: {done_sim_count}/{n} ({n-done_sim_count} failures)&#39;)
            print(f&#39;\t- Average step count: {(done_at_step_sum / n)}&#39;)
            print(f&#39;\t- Average total rewards: {(xp.sum(rewards) / n)}&#39;)
            print(f&#39;\t- Average discounted rewards (ADR): {(xp.sum(discounted_rewards) / n)}&#39;)

        return RewardSet(xp.sum(rewards, axis=1).tolist()), sim_hist_list
    

def load_POMDP_file(file_name:str) -&gt; Tuple[Model, PBVI_Solver]:
    &#39;&#39;&#39;
    Function to load files of .POMDP format.
    This file format was implemented by Cassandra and the specifications of the format can be found here: https://pomdp.org/code/pomdp-file-spec.html
     
    Then, example models can be found on the following page: https://pomdp.org/examples/

    Parameters
    ----------
    file_name : str
        The name and path of the file to be loaded.

    Returns
    -------
    loaded_model : pomdp.Model
        A POMDP model with the parameters found in the POMDP file. 
    loaded_solver : PBVI_Solver
        A solver with the gamma parameter from the POMDP file.
    &#39;&#39;&#39;
    # Working params
    gamma_param = 1.0
    values_param = &#39;&#39; # To investigate
    state_count = -1
    action_count = -1
    observation_count = -1

    model_params = {}
    reading:str = &#39;&#39;
    read_lines = 0

    with open(file_name) as file:
        for line in file:
            if line.startswith((&#39;#&#39;, &#39;\n&#39;)):
                continue

            # Split line
            line_items = line.replace(&#39;\n&#39;,&#39;&#39;).strip().split()

            # Discount factor
            if line.startswith(&#39;discount&#39;):
                gamma_param = float(line_items[-1])
            
            # Value (either reward or cost)
            elif line.startswith(&#39;values&#39;):
                values_param = line_items[-1] # To investigate

            # States
            elif line.startswith(&#39;states&#39;):
                if line_items[-1].isnumeric():
                    state_count = int(line_items[-1])
                    model_params[&#39;states&#39;] = [f&#39;s{i}&#39; for i in range(state_count)]
                else:
                    model_params[&#39;states&#39;] = line_items[1:]
                    state_count = len(model_params[&#39;states&#39;])

            # Actions
            elif line.startswith(&#39;actions&#39;):
                if line_items[-1].isnumeric():
                    action_count = int(line_items[-1])
                    model_params[&#39;actions&#39;] = [f&#39;a{i}&#39; for i in range(action_count)]
                else:
                    model_params[&#39;actions&#39;] = line_items[1:]
                    action_count = len(model_params[&#39;actions&#39;])

            # Observations
            elif line.startswith(&#39;observations&#39;):
                if line_items[-1].isnumeric():
                    observation_count = int(line_items[-1])
                    model_params[&#39;observations&#39;] = [f&#39;o{i}&#39; for i in range(observation_count)]
                else:
                    model_params[&#39;observations&#39;] = line_items[1:]
                    observation_count = len(model_params[&#39;observations&#39;])
            
            # Start
            elif line.startswith(&#39;start&#39;):
                if len(line_items) == 1:
                    reading = &#39;start&#39;
                else:
                    assert len(line_items[1:]) == state_count, &#39;Not enough states in initial belief&#39;
                    model_params[&#39;start_probabilities&#39;] = np.array([float(item) for item in line_items[1:]])
            elif reading == &#39;start&#39;:
                assert len(line_items) == state_count, &#39;Not enough states in initial belief&#39;
                model_params[&#39;start_probabilities&#39;] = np.array([float(item) for item in line_items])
                reading = &#39;None&#39;

            # ----------------------------------------------------------------------------------------------
            # Transition table
            # ----------------------------------------------------------------------------------------------
            if (&#39;states&#39; in model_params) and (&#39;actions&#39; in model_params) and (&#39;observations&#39; in model_params) and not (&#39;transitions&#39; in model_params):
                model_params[&#39;transitions&#39;] = np.full((state_count, action_count, state_count), 0.0)
            
            if line.startswith(&#39;T&#39;):
                transition_params = line.replace(&#39;:&#39;,&#39; &#39;).split()[1:]
                transition_params = transition_params[:-1] if (line.count(&#39;:&#39;) == 3) else transition_params

                ids = []
                for i, param in enumerate(transition_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    elif i in [1,2]:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])
                    else:
                        raise Exception(&#39;Cant load more than 3 parameters for transitions&#39;)

                # single item
                if len(transition_params) == 3:
                    for s in ids[1]:
                        for a in ids[0]:
                            for s_p in ids[2]:
                                model_params[&#39;transitions&#39;][s, a, s_p] = float(line_items[-1])
                
                # More items
                else:
                    reading = f&#39;T{len(transition_params)} &#39; + &#39; &#39;.join(transition_params)
                    
            # Reading action-state line
            elif reading.startswith(&#39;T2&#39;):
                transition_params = reading.split()[1:]

                ids = []
                for i, param in enumerate(transition_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    else:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])

                for a in ids[0]:
                    for s in ids[1]:
                        # Uniform
                        if &#39;uniform&#39; in line_items:
                            model_params[&#39;transitions&#39;][s, a, :] = np.ones(state_count) / state_count
                            continue

                        for s_p, item in enumerate(line_items):
                            model_params[&#39;transitions&#39;][s, a, s_p] = float(item)

                reading = &#39;&#39;

            # Reading action matrix
            elif reading.startswith(&#39;T1&#39;):
                s = read_lines

                transition_params = reading.split()[1:]

                ids = []
                for i, param in enumerate(transition_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                
                for a in ids[0]:
                    # Uniform
                    if &#39;uniform&#39; in line_items:
                        model_params[&#39;transitions&#39;][:, a, :] = np.ones((state_count, state_count)) / state_count
                        reading = &#39;&#39;
                        continue
                    # Identity
                    if &#39;identity&#39; in line_items:
                        model_params[&#39;transitions&#39;][:, a, :] = np.eye(state_count)
                        reading = &#39;&#39;
                        continue

                    for s_p, item in enumerate(line_items):
                        model_params[&#39;transitions&#39;][s, a, s_p] = float(item)

                if (&#39;uniform&#39; not in line_items) and (&#39;identity&#39; not in line_items):
                    read_lines += 1
                
                if read_lines == state_count:
                    reading = &#39;&#39;
                    read_lines = 0


            # ----------------------------------------------------------------------------------------------
            # Observation table
            # ----------------------------------------------------------------------------------------------
            if (&#39;states&#39; in model_params) and (&#39;actions&#39; in model_params) and (&#39;observations&#39; in model_params) and not (&#39;observation_table&#39; in model_params):
                model_params[&#39;observation_table&#39;] = np.full((state_count, action_count, observation_count), 0.0)

            if line.startswith(&#39;O&#39;):
                observation_params = line.replace(&#39;:&#39;,&#39; &#39;).split()[1:]
                observation_params = observation_params[:-1] if (line.count(&#39;:&#39;) == 3) else observation_params

                ids = []
                for i, param in enumerate(observation_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    elif i == 1:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])
                    elif i == 2:
                        ids.append(np.arange(observation_count) if param == &#39;*&#39; else [model_params[&#39;observations&#39;].index(param)])
                    else:
                        raise Exception(&#39;Cant load more than 3 parameters for observations&#39;)

                # single item
                if len(observation_params) == 3:
                    for a in ids[0]:
                        for s_p in ids[1]:
                            for o in ids[2]:
                                model_params[&#39;observation_table&#39;][s_p, a, o] = float(line_items[-1])
                
                # More items
                else:
                    reading = f&#39;O{len(observation_params)} &#39; + &#39; &#39;.join(observation_params)
                    
            # Reading action-state line
            elif reading.startswith(&#39;O2&#39;):
                observation_params = reading.split()[1:]

                ids = []
                for i, param in enumerate(observation_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    else:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])

                for a in ids[0]:
                    for s_p in ids[1]:
                        # Uniform
                        if &#39;uniform&#39; in line_items:
                            model_params[&#39;observation_table&#39;][s_p, a, :] = np.ones(observation_count) / observation_count
                            continue

                        for o, item in enumerate(line_items):
                            model_params[&#39;observation_table&#39;][s_p, a, o] = float(item)

                reading = &#39;&#39;

            # Reading action matrix
            elif reading.startswith(&#39;O1&#39;):
                s_p = read_lines

                observation_params = reading.split()[1:]
                ids = []
                for i, param in enumerate(observation_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    else:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])

                for a in ids[0]:
                    # Uniform
                    if &#39;uniform&#39; in line_items:
                        model_params[&#39;observation_table&#39;][:, a, :] = np.ones((state_count, observation_count)) / observation_count
                        reading = &#39;&#39;
                        continue

                    for o, item in enumerate(line_items):
                        model_params[&#39;observation_table&#39;][s_p, a, o] = float(item)

                if &#39;uniform&#39; not in line_items:
                    read_lines += 1
                
                if read_lines == state_count:
                    reading = &#39;&#39;
                    read_lines = 0


            # ----------------------------------------------------------------------------------------------
            # Rewards table
            # ----------------------------------------------------------------------------------------------
            if (&#39;states&#39; in model_params) and (&#39;actions&#39; in model_params) and (&#39;observations&#39; in model_params) and not (&#39;rewards&#39; in model_params):
                model_params[&#39;rewards&#39;] = np.full((state_count, action_count, state_count, observation_count), 0.0)

            if line.startswith(&#39;R&#39;):
                reward_params = line.replace(&#39;:&#39;,&#39; &#39;).split()[1:]
                reward_params = reward_params[:-1] if (line.count(&#39;:&#39;) == 4) else reward_params

                ids = []
                for i, param in enumerate(reward_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    elif i in [1,2]:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])
                    elif i == 3:
                        ids.append(np.arange(observation_count) if param == &#39;*&#39; else [model_params[&#39;observations&#39;].index(param)])
                    else:
                        raise Exception(&#39;Cant load more than 4 parameters for rewards&#39;)

                # single item
                if len(reward_params) == 4:
                    for a in ids[0]:
                        for s in ids[1]:
                            for s_p in ids[2]:
                                for o in ids[3]:
                                    model_params[&#39;rewards&#39;][s, a, s_p, o] = float(line_items[-1])
                
                elif len(reward_params) == 1:
                    raise Exception(&#39;Need more than 1 parameter for rewards&#39;)

                # More items
                else:
                    reading = f&#39;R{len(reward_params)} &#39; + &#39; &#39;.join(reward_params)
                    
            # Reading action-state line
            elif reading.startswith(&#39;R3&#39;):
                reward_params = reading.split()[1:]
                
                ids = []
                for i, param in enumerate(reward_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    else:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])

                for a in ids[0]:
                    for s in ids[1]:
                        for s_p in ids[2]:
                            for o, item in enumerate(line_items):
                                model_params[&#39;rewards&#39;][s, a, s_p, o] = float(item)

                reading = &#39;&#39;

            # Reading action matrix
            elif reading.startswith(&#39;R2&#39;):
                s_p = read_lines

                reward_params = reading.split()[1:]
                ids = []
                for i, param in enumerate(reward_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    else:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])

                for a in ids[0]:
                    for s in ids[1]:
                        for o, item in enumerate(line_items):
                            model_params[&#39;rewards&#39;][s, a, s_p, o] = float(item)

                read_lines += 1
                if read_lines == state_count:
                    reading = &#39;&#39;
                    read_lines = 0

    # Generation of output
    loaded_model = Model(**model_params)
    loaded_solver = PBVI_Solver(gamma=gamma_param)

    return (loaded_model, loaded_solver)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.pomdp.load_POMDP_file"><code class="name flex">
<span>def <span class="ident">load_POMDP_file</span></span>(<span>file_name: str) ‑> Tuple[<a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, <a title="src.pomdp.PBVI_Solver" href="#src.pomdp.PBVI_Solver">PBVI_Solver</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Function to load files of .POMDP format.
This file format was implemented by Cassandra and the specifications of the format can be found here: <a href="https://pomdp.org/code/pomdp-file-spec.html">https://pomdp.org/code/pomdp-file-spec.html</a></p>
<p>Then, example models can be found on the following page: <a href="https://pomdp.org/examples/">https://pomdp.org/examples/</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code></dt>
<dd>The name and path of the file to be loaded.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>loaded_model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>A POMDP model with the parameters found in the POMDP file.</dd>
<dt><strong><code>loaded_solver</code></strong> :&ensp;<code><a title="src.pomdp.PBVI_Solver" href="#src.pomdp.PBVI_Solver">PBVI_Solver</a></code></dt>
<dd>A solver with the gamma parameter from the POMDP file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_POMDP_file(file_name:str) -&gt; Tuple[Model, PBVI_Solver]:
    &#39;&#39;&#39;
    Function to load files of .POMDP format.
    This file format was implemented by Cassandra and the specifications of the format can be found here: https://pomdp.org/code/pomdp-file-spec.html
     
    Then, example models can be found on the following page: https://pomdp.org/examples/

    Parameters
    ----------
    file_name : str
        The name and path of the file to be loaded.

    Returns
    -------
    loaded_model : pomdp.Model
        A POMDP model with the parameters found in the POMDP file. 
    loaded_solver : PBVI_Solver
        A solver with the gamma parameter from the POMDP file.
    &#39;&#39;&#39;
    # Working params
    gamma_param = 1.0
    values_param = &#39;&#39; # To investigate
    state_count = -1
    action_count = -1
    observation_count = -1

    model_params = {}
    reading:str = &#39;&#39;
    read_lines = 0

    with open(file_name) as file:
        for line in file:
            if line.startswith((&#39;#&#39;, &#39;\n&#39;)):
                continue

            # Split line
            line_items = line.replace(&#39;\n&#39;,&#39;&#39;).strip().split()

            # Discount factor
            if line.startswith(&#39;discount&#39;):
                gamma_param = float(line_items[-1])
            
            # Value (either reward or cost)
            elif line.startswith(&#39;values&#39;):
                values_param = line_items[-1] # To investigate

            # States
            elif line.startswith(&#39;states&#39;):
                if line_items[-1].isnumeric():
                    state_count = int(line_items[-1])
                    model_params[&#39;states&#39;] = [f&#39;s{i}&#39; for i in range(state_count)]
                else:
                    model_params[&#39;states&#39;] = line_items[1:]
                    state_count = len(model_params[&#39;states&#39;])

            # Actions
            elif line.startswith(&#39;actions&#39;):
                if line_items[-1].isnumeric():
                    action_count = int(line_items[-1])
                    model_params[&#39;actions&#39;] = [f&#39;a{i}&#39; for i in range(action_count)]
                else:
                    model_params[&#39;actions&#39;] = line_items[1:]
                    action_count = len(model_params[&#39;actions&#39;])

            # Observations
            elif line.startswith(&#39;observations&#39;):
                if line_items[-1].isnumeric():
                    observation_count = int(line_items[-1])
                    model_params[&#39;observations&#39;] = [f&#39;o{i}&#39; for i in range(observation_count)]
                else:
                    model_params[&#39;observations&#39;] = line_items[1:]
                    observation_count = len(model_params[&#39;observations&#39;])
            
            # Start
            elif line.startswith(&#39;start&#39;):
                if len(line_items) == 1:
                    reading = &#39;start&#39;
                else:
                    assert len(line_items[1:]) == state_count, &#39;Not enough states in initial belief&#39;
                    model_params[&#39;start_probabilities&#39;] = np.array([float(item) for item in line_items[1:]])
            elif reading == &#39;start&#39;:
                assert len(line_items) == state_count, &#39;Not enough states in initial belief&#39;
                model_params[&#39;start_probabilities&#39;] = np.array([float(item) for item in line_items])
                reading = &#39;None&#39;

            # ----------------------------------------------------------------------------------------------
            # Transition table
            # ----------------------------------------------------------------------------------------------
            if (&#39;states&#39; in model_params) and (&#39;actions&#39; in model_params) and (&#39;observations&#39; in model_params) and not (&#39;transitions&#39; in model_params):
                model_params[&#39;transitions&#39;] = np.full((state_count, action_count, state_count), 0.0)
            
            if line.startswith(&#39;T&#39;):
                transition_params = line.replace(&#39;:&#39;,&#39; &#39;).split()[1:]
                transition_params = transition_params[:-1] if (line.count(&#39;:&#39;) == 3) else transition_params

                ids = []
                for i, param in enumerate(transition_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    elif i in [1,2]:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])
                    else:
                        raise Exception(&#39;Cant load more than 3 parameters for transitions&#39;)

                # single item
                if len(transition_params) == 3:
                    for s in ids[1]:
                        for a in ids[0]:
                            for s_p in ids[2]:
                                model_params[&#39;transitions&#39;][s, a, s_p] = float(line_items[-1])
                
                # More items
                else:
                    reading = f&#39;T{len(transition_params)} &#39; + &#39; &#39;.join(transition_params)
                    
            # Reading action-state line
            elif reading.startswith(&#39;T2&#39;):
                transition_params = reading.split()[1:]

                ids = []
                for i, param in enumerate(transition_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    else:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])

                for a in ids[0]:
                    for s in ids[1]:
                        # Uniform
                        if &#39;uniform&#39; in line_items:
                            model_params[&#39;transitions&#39;][s, a, :] = np.ones(state_count) / state_count
                            continue

                        for s_p, item in enumerate(line_items):
                            model_params[&#39;transitions&#39;][s, a, s_p] = float(item)

                reading = &#39;&#39;

            # Reading action matrix
            elif reading.startswith(&#39;T1&#39;):
                s = read_lines

                transition_params = reading.split()[1:]

                ids = []
                for i, param in enumerate(transition_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                
                for a in ids[0]:
                    # Uniform
                    if &#39;uniform&#39; in line_items:
                        model_params[&#39;transitions&#39;][:, a, :] = np.ones((state_count, state_count)) / state_count
                        reading = &#39;&#39;
                        continue
                    # Identity
                    if &#39;identity&#39; in line_items:
                        model_params[&#39;transitions&#39;][:, a, :] = np.eye(state_count)
                        reading = &#39;&#39;
                        continue

                    for s_p, item in enumerate(line_items):
                        model_params[&#39;transitions&#39;][s, a, s_p] = float(item)

                if (&#39;uniform&#39; not in line_items) and (&#39;identity&#39; not in line_items):
                    read_lines += 1
                
                if read_lines == state_count:
                    reading = &#39;&#39;
                    read_lines = 0


            # ----------------------------------------------------------------------------------------------
            # Observation table
            # ----------------------------------------------------------------------------------------------
            if (&#39;states&#39; in model_params) and (&#39;actions&#39; in model_params) and (&#39;observations&#39; in model_params) and not (&#39;observation_table&#39; in model_params):
                model_params[&#39;observation_table&#39;] = np.full((state_count, action_count, observation_count), 0.0)

            if line.startswith(&#39;O&#39;):
                observation_params = line.replace(&#39;:&#39;,&#39; &#39;).split()[1:]
                observation_params = observation_params[:-1] if (line.count(&#39;:&#39;) == 3) else observation_params

                ids = []
                for i, param in enumerate(observation_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    elif i == 1:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])
                    elif i == 2:
                        ids.append(np.arange(observation_count) if param == &#39;*&#39; else [model_params[&#39;observations&#39;].index(param)])
                    else:
                        raise Exception(&#39;Cant load more than 3 parameters for observations&#39;)

                # single item
                if len(observation_params) == 3:
                    for a in ids[0]:
                        for s_p in ids[1]:
                            for o in ids[2]:
                                model_params[&#39;observation_table&#39;][s_p, a, o] = float(line_items[-1])
                
                # More items
                else:
                    reading = f&#39;O{len(observation_params)} &#39; + &#39; &#39;.join(observation_params)
                    
            # Reading action-state line
            elif reading.startswith(&#39;O2&#39;):
                observation_params = reading.split()[1:]

                ids = []
                for i, param in enumerate(observation_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    else:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])

                for a in ids[0]:
                    for s_p in ids[1]:
                        # Uniform
                        if &#39;uniform&#39; in line_items:
                            model_params[&#39;observation_table&#39;][s_p, a, :] = np.ones(observation_count) / observation_count
                            continue

                        for o, item in enumerate(line_items):
                            model_params[&#39;observation_table&#39;][s_p, a, o] = float(item)

                reading = &#39;&#39;

            # Reading action matrix
            elif reading.startswith(&#39;O1&#39;):
                s_p = read_lines

                observation_params = reading.split()[1:]
                ids = []
                for i, param in enumerate(observation_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    else:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])

                for a in ids[0]:
                    # Uniform
                    if &#39;uniform&#39; in line_items:
                        model_params[&#39;observation_table&#39;][:, a, :] = np.ones((state_count, observation_count)) / observation_count
                        reading = &#39;&#39;
                        continue

                    for o, item in enumerate(line_items):
                        model_params[&#39;observation_table&#39;][s_p, a, o] = float(item)

                if &#39;uniform&#39; not in line_items:
                    read_lines += 1
                
                if read_lines == state_count:
                    reading = &#39;&#39;
                    read_lines = 0


            # ----------------------------------------------------------------------------------------------
            # Rewards table
            # ----------------------------------------------------------------------------------------------
            if (&#39;states&#39; in model_params) and (&#39;actions&#39; in model_params) and (&#39;observations&#39; in model_params) and not (&#39;rewards&#39; in model_params):
                model_params[&#39;rewards&#39;] = np.full((state_count, action_count, state_count, observation_count), 0.0)

            if line.startswith(&#39;R&#39;):
                reward_params = line.replace(&#39;:&#39;,&#39; &#39;).split()[1:]
                reward_params = reward_params[:-1] if (line.count(&#39;:&#39;) == 4) else reward_params

                ids = []
                for i, param in enumerate(reward_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    elif i in [1,2]:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])
                    elif i == 3:
                        ids.append(np.arange(observation_count) if param == &#39;*&#39; else [model_params[&#39;observations&#39;].index(param)])
                    else:
                        raise Exception(&#39;Cant load more than 4 parameters for rewards&#39;)

                # single item
                if len(reward_params) == 4:
                    for a in ids[0]:
                        for s in ids[1]:
                            for s_p in ids[2]:
                                for o in ids[3]:
                                    model_params[&#39;rewards&#39;][s, a, s_p, o] = float(line_items[-1])
                
                elif len(reward_params) == 1:
                    raise Exception(&#39;Need more than 1 parameter for rewards&#39;)

                # More items
                else:
                    reading = f&#39;R{len(reward_params)} &#39; + &#39; &#39;.join(reward_params)
                    
            # Reading action-state line
            elif reading.startswith(&#39;R3&#39;):
                reward_params = reading.split()[1:]
                
                ids = []
                for i, param in enumerate(reward_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    else:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])

                for a in ids[0]:
                    for s in ids[1]:
                        for s_p in ids[2]:
                            for o, item in enumerate(line_items):
                                model_params[&#39;rewards&#39;][s, a, s_p, o] = float(item)

                reading = &#39;&#39;

            # Reading action matrix
            elif reading.startswith(&#39;R2&#39;):
                s_p = read_lines

                reward_params = reading.split()[1:]
                ids = []
                for i, param in enumerate(reward_params):
                    if param.isnumeric():
                        ids.append([int(param)])
                    elif i == 0:
                        ids.append(np.arange(action_count) if param == &#39;*&#39; else [model_params[&#39;actions&#39;].index(param)])
                    else:
                        ids.append(np.arange(state_count) if param == &#39;*&#39; else [model_params[&#39;states&#39;].index(param)])

                for a in ids[0]:
                    for s in ids[1]:
                        for o, item in enumerate(line_items):
                            model_params[&#39;rewards&#39;][s, a, s_p, o] = float(item)

                read_lines += 1
                if read_lines == state_count:
                    reading = &#39;&#39;
                    read_lines = 0

    # Generation of output
    loaded_model = Model(**model_params)
    loaded_solver = PBVI_Solver(gamma=gamma_param)

    return (loaded_model, loaded_solver)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.pomdp.Agent"><code class="flex name class">
<span>class <span class="ident">Agent</span></span>
<span>(</span><span>model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, value_function: Optional[<a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>The class of an Agent running on a POMDP model.
It has the ability to train using a given solver (here PBVI_Solver).
Then, once trained, it can simulate actions with a given Simulation,
either for a given amount of steps or until a single reward is received.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The model in which the agent can run</dd>
<dt><strong><code>value_function</code></strong> :&ensp;<code>ValueFunction</code>, optional</dt>
<dd>A value function the agent can use to play a simulation, in the case the model has been solved beforehand.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>value_function</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>The value function the agent has come up to after training.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Agent:
    &#39;&#39;&#39;
    The class of an Agent running on a POMDP model.
    It has the ability to train using a given solver (here PBVI_Solver).
    Then, once trained, it can simulate actions with a given Simulation,
    either for a given amount of steps or until a single reward is received.

    ...

    Parameters
    ----------
    model: pomdp.Model
        The model in which the agent can run
    value_function : ValueFunction, optional
        A value function the agent can use to play a simulation, in the case the model has been solved beforehand.
    
    Attributes
    ----------
    model: pomdp.Model
    value_function : ValueFunction
        The value function the agent has come up to after training.
    &#39;&#39;&#39;
    def __init__(self, model:Model, value_function:Union[ValueFunction,None]=None) -&gt; None:
        self.model = model
        self.value_function = value_function


    def train(self, solver:PBVI_Solver, expansions:int, horizon:int) -&gt; SolverHistory:
        &#39;&#39;&#39;
        Method to train the agent using a given solver.
        The solver will provide a value function that will map beliefs in belief space to actions.

        Parameters
        ----------
        solver : PBVI_Solver
            The solver to run.
        expansions : int
            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)
        horizon : int
            How many times the alpha vector set must be updated every time the belief set is expanded.
        
        Returns
        -------
        solve_history : SolverHistory
            The history of the solving process.
        &#39;&#39;&#39;
        self.value_function, solve_history = solver.solve(self.model, expansions, horizon)
        return solve_history


    def get_best_action(self, belief:Belief) -&gt; int:
        &#39;&#39;&#39;
        Function to retrieve the best action for a given belief based on the value function retrieved from the training.

        Parameters
        ----------
        belief : Belief
            The belief to get the best action with.
                
        Returns
        -------
        best_action : int
            The best action found.
        &#39;&#39;&#39;
        assert self.value_function is not None, &#34;No value function, training probably has to be run...&#34;

        # GPU
        xp = np if not self.value_function.is_on_gpu else cp

        best_vector = xp.argmax(xp.dot(self.value_function.alpha_vector_array, belief.values))
        best_action = int(self.value_function.actions[best_vector])

        return best_action


    def simulate(self,
                 simulator:Union[Simulation,None]=None,
                 max_steps:int=1000,
                 start_state:Union[int,None]=None,
                 print_progress:bool=True,
                 print_stats:bool=True
                 ) -&gt; SimulationHistory:
        &#39;&#39;&#39;
        Function to run a simulation with the current agent for up to &#39;max_steps&#39; amount of steps using a Simulation simulator.

        Parameters
        ----------
        simulator : pomdp.Simulation, optional
            The simulation that will be used by the agent. If not provided, the default MDP simulator will be used.
        max_steps : int, default=1000
            The max amount of steps the simulation can run for.
        start_state : int, optional
            The state the agent should start in, if not provided, will be set at random based on start probabilities of the model.
        print_progress : bool, default=True
            Whether or not to print out the progress of the simulation.
        print_stats : bool, default=True
            Whether or not to print simulation statistics at the end of the simulation.

        Returns
        -------
        history : SimulationHistory
            A list of rewards with the additional functionality that the can be plot with the plot() function.
        &#39;&#39;&#39;
        assert self.value_function is not None, &#34;No value function, training probably has to be run...&#34;

        # GPU setup
        self.model = self.model.gpu_model if self.value_function.is_on_gpu else self.model.cpu_model

        # Get or generate a default simulator
        if simulator is None:
            simulator = Simulation(self.model)

        # reset
        s = simulator.initialize_simulation(start_state=start_state) # s is only used for the simulation history
        belief = Belief(self.model)

        history = SimulationHistory(self.model, start_state=s, start_belief=belief)

        sim_start_ts = datetime.now()

        # Simulation loop
        for _ in (trange(max_steps) if print_progress else range(max_steps)):
            # Play best action
            a = self.get_best_action(belief)
            r,o = simulator.run_action(a)

            # Update the belief
            new_belief = belief.update(a, o)

            # Post action history recording
            history.add(action=a, next_state=simulator.agent_state, next_belief=new_belief, reward=r, observation=o)

            # Replace belief
            belief = new_belief

            # If simulation is considered done, the rewards are simply returned
            if simulator.is_done:
                break
            
        if print_stats:
            sim_end_ts = datetime.now()
            print(&#39;Simulation done:&#39;)
            print(f&#39;\t- Runtime (s): {(sim_end_ts - sim_start_ts).total_seconds()}&#39;)
            print(f&#39;\t- Steps: {len(history.states)}&#39;)
            print(f&#39;\t- Total rewards: {sum(history.rewards)}&#39;)
            print(f&#39;\t- End state: {self.model.state_labels[history.states[-1]]}&#39;)

        return history


    def run_n_simulations(self,
                          simulator:Union[Simulation,None]=None,
                          n:int=1000,
                          max_steps:int=1000,
                          start_state:Union[int,None]=None,
                          reward_discount:float=0.99,
                          print_progress:bool=True,
                          print_stats:bool=True
                          ) -&gt; Tuple[RewardSet, list[SimulationHistory]]:
        &#39;&#39;&#39;
        Function to run a set of simulations in a row.
        This is useful when the simulation has a &#39;done&#39; condition.
        In this case, the rewards of individual simulations are summed together under a single number.

        Parameters
        ----------
        simulator : pomdp.Simulation, optional
            The simulation that will be used by the agent. If not provided, the default MDP simulator will be used. (Optional)
        n : int, default=1000
            The amount of simulations to run.
        max_steps : int, default=1000
            The max_steps to run per simulation.
        start_state : int, optional
            The state the agent should start in, if not provided, will be set at random based on start probabilities of the model (Default: random)
        reward_discount : float, default=0.99
            The reward discount to compute the Average Discounted Reward (ADR).
        print_progress : bool, default=True
            Whether or not to print out the progress of the simulation.
        print_stats : bool, default=True
            Whether or not to print simulation statistics at the end of the simulation.

        Returns
        -------
        all_histories : RewardSet
            A list of the final rewards after each simulation.
        all_histories : list[SimulationHistory]
            A list the simulation histories gathered at each simulation.
        &#39;&#39;&#39;
        if simulator is None:
            simulator = Simulation(self.model)

        sim_start_ts = datetime.now()

        all_histories = []
        all_final_rewards = RewardSet()
        all_discounted_rewards = []
        all_sim_length = []
        done_sim_count = 0
        for _ in (trange(n) if print_progress else range(n)):
            sim_history = self.simulate(simulator, max_steps, start_state, False, False)

            if simulator.is_done:
                done_sim_count += 1

            all_histories.append(sim_history)
            all_final_rewards.append(np.sum(sim_history.rewards))
            all_discounted_rewards.append(sim_history.rewards.get_total_discounted_reward(reward_discount))
            all_sim_length.append(len(sim_history))

        if print_stats:
            sim_end_ts = datetime.now()
            print(f&#39;All {n} simulations done:&#39;)
            print(f&#39;\t- Average runtime (s): {((sim_end_ts - sim_start_ts).total_seconds() / n)}&#39;)
            print(f&#39;\t- Simulations reached goal: {done_sim_count}/{n} ({n-done_sim_count} failures)&#39;)
            print(f&#39;\t- Average step count: {(sum(all_sim_length) / n)}&#39;)
            print(f&#39;\t- Average total rewards: {(sum(all_final_rewards) / n)}&#39;)
            print(f&#39;\t- Average discounted rewards (ADR): {(sum(all_discounted_rewards) / n)}&#39;)

        return all_final_rewards, all_histories


    def run_n_simulations_parallel(self,
                                   n:int=1000,
                                   max_steps:int=1000,
                                   start_state:Union[list[int],int,None]=None,
                                   reward_discount:float=0.99,
                                   print_progress:bool=True,
                                   print_stats:bool=True
                                   ) -&gt; Tuple[RewardSet, list[SimulationHistory]]:
        &#39;&#39;&#39;
        TODO
        &#39;&#39;&#39;
        # GPU support
        xp = np if not self.value_function.is_on_gpu else cp
        model = self.model.cpu_model if not self.value_function.is_on_gpu else self.model.gpu_model

        # Genetion of an array of n beliefs
        initial_beliefs = xp.repeat(Belief(model).values[None,:], n, axis=0)

        # Generating n initial positions
        start_state_array = xp.empty(n)
        if isinstance(start_state, int):
            start_state_array = start_state
        elif isinstance(start_state, list):
            repeated_list = xp.repeat(xp.array(start_state), int(np.ceil(n / len(start_state))))
            start_state_array = xp.resize(repeated_list, n)
        else:
            start_state_array = xp.random.choice(model.states, size=n, p=model.start_probabilities)
        
        # Belief and state arrays
        beliefs = initial_beliefs
        new_beliefs = None
        states = start_state_array
        next_states = None

        # Tracking what simulations are done
        sim_is_done = xp.zeros(n, dtype=bool)
        done_at_step = xp.full(n, -1)

        # Speedup item
        simulations = xp.arange(n)
        flatten_offset = (simulations[:,None] * model.state_count)
        flat_shape = (n, (model.state_count * model.reachable_state_count))

        # 2D bincount for belief set update
        def bincount2D_vectorized(a, w):    
            a_offs = a + flatten_offset
            return xp.bincount(a_offs.ravel(), weights=w.ravel(), minlength=a.shape[0]*model.state_count).reshape(-1,model.state_count)

        # Results
        discount = reward_discount
        rewards = []
        discounted_rewards = []

        # History items
        states_history = [states]
        actions_history = []
        observations_history = []

        sim_start_ts = datetime.now()
        
        iterator = trange(max_steps) if print_progress else range(max_steps)
        for i in iterator:
            # Retrieving the top vectors according to the value function
            best_vectors = xp.argmax(xp.matmul(beliefs, self.value_function.alpha_vector_array.T), axis=1)

            # Retrieving the actions associated with the vectors chosen
            best_actions = self.value_function.actions[best_vectors]

            # Get each reachable next states for each action
            reachable_state_per_actions = model.reachable_states[:, best_actions, :]

            # Gathering new states based on the transition function and the chosen actions
            next_state_potentials = reachable_state_per_actions[states, simulations]
            if model.reachable_state_count == 1:
                next_states = next_state_potentials[:,0]
            else:
                potential_probabilities = model.reachable_probabilities[states[:,None], best_actions[:,None]][:,0,:]
                chosen_indices = xp.apply_along_axis(lambda x: xp.random.choice(len(x), size=1, p=x), axis=1, arr=potential_probabilities)
                next_states = next_state_potentials[chosen_indices][:,0,0]

            # Making observations based on the states landed in and the action that was taken
            observation_probabilities = model.observation_table[next_states[:,None], best_actions[:,None]][:,0,:]
            observations = xp.sum(xp.random.random(n)[:,None] &gt; xp.cumsum(observation_probabilities[:,:-1], axis=1), axis=1)

            # Belief set update
            reachable_probabilities = (model.reachable_transitional_observation_table[:,best_actions,observations,:] * beliefs.T[:,:,None])
            new_beliefs = bincount2D_vectorized(a=reachable_state_per_actions.swapaxes(0,1).reshape(flat_shape),
                                                w=reachable_probabilities.swapaxes(0,1).reshape(flat_shape))

            new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]

            # Rewards computation
            step_rewards = xp.array([model.immediate_reward_function(int(s), int(a), int(s_p), int(o)) for s,a,s_p,o in zip(states, best_actions, next_states, observations)])
            rewards.append(xp.where(~sim_is_done, step_rewards, 0))
            discounted_rewards.append(xp.where(~sim_is_done, step_rewards * discount, 0))

            # Checking for done condition
            are_done = xp.isin(next_states, xp.array(model.end_states))
            done_at_step[sim_is_done ^ are_done] = i+1
            sim_is_done |= are_done

            # Update iterator postfix
            if print_progress:
                iterator.set_postfix({&#39;done&#39;: xp.sum(sim_is_done)})

            # Replacing old with new
            states = next_states
            beliefs = new_beliefs
            discount *= reward_discount

            # History tracking
            states_history.append(states)
            actions_history.append(best_actions)
            observations_history.append(observations)

            # Early stopping
            if xp.all(sim_is_done):
                break

        # Wrap up
        sim_hist_list = []
        b0 = Belief(model)

        states_history = xp.array(states_history).T
        actions_history = xp.array(actions_history).T
        observations_history = xp.array(observations_history).T
        rewards = xp.array(rewards).T
        discounted_rewards = xp.array(discounted_rewards).T

        done_at_step_sum = 0

        for i, s0 in enumerate(start_state_array):
            sim_hist = SimulationHistory(self.model, int(s0), b0)
            
            last_step = int(done_at_step[i]) if bool(sim_is_done[i]) else max_steps
            done_at_step_sum += last_step

            # Setting the elements
            sim_hist.states = states_history[i,:last_step+1].tolist()
            sim_hist.actions = actions_history[i,:last_step].tolist()
            sim_hist.observations = observations_history[i,:last_step].tolist()
            sim_hist.rewards = rewards[i,:last_step].tolist()

            # Adding the sim history to the list of histories
            sim_hist_list.append(sim_hist)

        done_sim_count = sum([1 if done else 0 for done in sim_is_done])

        if print_stats:
            sim_end_ts = datetime.now()
            print(f&#39;All {n} simulations done in {(sim_end_ts - sim_start_ts).total_seconds():.3f}s:&#39;)
            print(f&#39;\t- Simulations reached goal: {done_sim_count}/{n} ({n-done_sim_count} failures)&#39;)
            print(f&#39;\t- Average step count: {(done_at_step_sum / n)}&#39;)
            print(f&#39;\t- Average total rewards: {(xp.sum(rewards) / n)}&#39;)
            print(f&#39;\t- Average discounted rewards (ADR): {(xp.sum(discounted_rewards) / n)}&#39;)

        return RewardSet(xp.sum(rewards, axis=1).tolist()), sim_hist_list</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="src.pomdp.Agent.get_best_action"><code class="name flex">
<span>def <span class="ident">get_best_action</span></span>(<span>self, belief: <a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Function to retrieve the best action for a given belief based on the value function retrieved from the training.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>belief</code></strong> :&ensp;<code><a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></code></dt>
<dd>The belief to get the best action with.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>best_action</code></strong> :&ensp;<code>int</code></dt>
<dd>The best action found.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_best_action(self, belief:Belief) -&gt; int:
    &#39;&#39;&#39;
    Function to retrieve the best action for a given belief based on the value function retrieved from the training.

    Parameters
    ----------
    belief : Belief
        The belief to get the best action with.
            
    Returns
    -------
    best_action : int
        The best action found.
    &#39;&#39;&#39;
    assert self.value_function is not None, &#34;No value function, training probably has to be run...&#34;

    # GPU
    xp = np if not self.value_function.is_on_gpu else cp

    best_vector = xp.argmax(xp.dot(self.value_function.alpha_vector_array, belief.values))
    best_action = int(self.value_function.actions[best_vector])

    return best_action</code></pre>
</details>
</dd>
<dt id="src.pomdp.Agent.run_n_simulations"><code class="name flex">
<span>def <span class="ident">run_n_simulations</span></span>(<span>self, simulator: Optional[<a title="src.pomdp.Simulation" href="#src.pomdp.Simulation">Simulation</a>] = None, n: int = 1000, max_steps: int = 1000, start_state: Optional[int] = None, reward_discount: float = 0.99, print_progress: bool = True, print_stats: bool = True) ‑> Tuple[<a title="src.mdp.RewardSet" href="mdp.html#src.mdp.RewardSet">RewardSet</a>, list[<a title="src.pomdp.SimulationHistory" href="#src.pomdp.SimulationHistory">SimulationHistory</a>]]</span>
</code></dt>
<dd>
<div class="desc"><p>Function to run a set of simulations in a row.
This is useful when the simulation has a 'done' condition.
In this case, the rewards of individual simulations are summed together under a single number.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>simulator</code></strong> :&ensp;<code>pomdp.Simulation</code>, optional</dt>
<dd>The simulation that will be used by the agent. If not provided, the default MDP simulator will be used. (Optional)</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code>, default=<code>1000</code></dt>
<dd>The amount of simulations to run.</dd>
<dt><strong><code>max_steps</code></strong> :&ensp;<code>int</code>, default=<code>1000</code></dt>
<dd>The max_steps to run per simulation.</dd>
<dt><strong><code>start_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The state the agent should start in, if not provided, will be set at random based on start probabilities of the model (Default: random)</dd>
<dt><strong><code>reward_discount</code></strong> :&ensp;<code>float</code>, default=<code>0.99</code></dt>
<dd>The reward discount to compute the Average Discounted Reward (ADR).</dd>
<dt><strong><code>print_progress</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether or not to print out the progress of the simulation.</dd>
<dt><strong><code>print_stats</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether or not to print simulation statistics at the end of the simulation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>all_histories</code></strong> :&ensp;<code>RewardSet</code></dt>
<dd>A list of the final rewards after each simulation.</dd>
<dt><strong><code>all_histories</code></strong> :&ensp;<code>list[<a title="src.pomdp.SimulationHistory" href="#src.pomdp.SimulationHistory">SimulationHistory</a>]</code></dt>
<dd>A list the simulation histories gathered at each simulation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_n_simulations(self,
                      simulator:Union[Simulation,None]=None,
                      n:int=1000,
                      max_steps:int=1000,
                      start_state:Union[int,None]=None,
                      reward_discount:float=0.99,
                      print_progress:bool=True,
                      print_stats:bool=True
                      ) -&gt; Tuple[RewardSet, list[SimulationHistory]]:
    &#39;&#39;&#39;
    Function to run a set of simulations in a row.
    This is useful when the simulation has a &#39;done&#39; condition.
    In this case, the rewards of individual simulations are summed together under a single number.

    Parameters
    ----------
    simulator : pomdp.Simulation, optional
        The simulation that will be used by the agent. If not provided, the default MDP simulator will be used. (Optional)
    n : int, default=1000
        The amount of simulations to run.
    max_steps : int, default=1000
        The max_steps to run per simulation.
    start_state : int, optional
        The state the agent should start in, if not provided, will be set at random based on start probabilities of the model (Default: random)
    reward_discount : float, default=0.99
        The reward discount to compute the Average Discounted Reward (ADR).
    print_progress : bool, default=True
        Whether or not to print out the progress of the simulation.
    print_stats : bool, default=True
        Whether or not to print simulation statistics at the end of the simulation.

    Returns
    -------
    all_histories : RewardSet
        A list of the final rewards after each simulation.
    all_histories : list[SimulationHistory]
        A list the simulation histories gathered at each simulation.
    &#39;&#39;&#39;
    if simulator is None:
        simulator = Simulation(self.model)

    sim_start_ts = datetime.now()

    all_histories = []
    all_final_rewards = RewardSet()
    all_discounted_rewards = []
    all_sim_length = []
    done_sim_count = 0
    for _ in (trange(n) if print_progress else range(n)):
        sim_history = self.simulate(simulator, max_steps, start_state, False, False)

        if simulator.is_done:
            done_sim_count += 1

        all_histories.append(sim_history)
        all_final_rewards.append(np.sum(sim_history.rewards))
        all_discounted_rewards.append(sim_history.rewards.get_total_discounted_reward(reward_discount))
        all_sim_length.append(len(sim_history))

    if print_stats:
        sim_end_ts = datetime.now()
        print(f&#39;All {n} simulations done:&#39;)
        print(f&#39;\t- Average runtime (s): {((sim_end_ts - sim_start_ts).total_seconds() / n)}&#39;)
        print(f&#39;\t- Simulations reached goal: {done_sim_count}/{n} ({n-done_sim_count} failures)&#39;)
        print(f&#39;\t- Average step count: {(sum(all_sim_length) / n)}&#39;)
        print(f&#39;\t- Average total rewards: {(sum(all_final_rewards) / n)}&#39;)
        print(f&#39;\t- Average discounted rewards (ADR): {(sum(all_discounted_rewards) / n)}&#39;)

    return all_final_rewards, all_histories</code></pre>
</details>
</dd>
<dt id="src.pomdp.Agent.run_n_simulations_parallel"><code class="name flex">
<span>def <span class="ident">run_n_simulations_parallel</span></span>(<span>self, n: int = 1000, max_steps: int = 1000, start_state: Union[list[int], int, ForwardRef(None)] = None, reward_discount: float = 0.99, print_progress: bool = True, print_stats: bool = True) ‑> Tuple[<a title="src.mdp.RewardSet" href="mdp.html#src.mdp.RewardSet">RewardSet</a>, list[<a title="src.pomdp.SimulationHistory" href="#src.pomdp.SimulationHistory">SimulationHistory</a>]]</span>
</code></dt>
<dd>
<div class="desc"><p>TODO</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_n_simulations_parallel(self,
                               n:int=1000,
                               max_steps:int=1000,
                               start_state:Union[list[int],int,None]=None,
                               reward_discount:float=0.99,
                               print_progress:bool=True,
                               print_stats:bool=True
                               ) -&gt; Tuple[RewardSet, list[SimulationHistory]]:
    &#39;&#39;&#39;
    TODO
    &#39;&#39;&#39;
    # GPU support
    xp = np if not self.value_function.is_on_gpu else cp
    model = self.model.cpu_model if not self.value_function.is_on_gpu else self.model.gpu_model

    # Genetion of an array of n beliefs
    initial_beliefs = xp.repeat(Belief(model).values[None,:], n, axis=0)

    # Generating n initial positions
    start_state_array = xp.empty(n)
    if isinstance(start_state, int):
        start_state_array = start_state
    elif isinstance(start_state, list):
        repeated_list = xp.repeat(xp.array(start_state), int(np.ceil(n / len(start_state))))
        start_state_array = xp.resize(repeated_list, n)
    else:
        start_state_array = xp.random.choice(model.states, size=n, p=model.start_probabilities)
    
    # Belief and state arrays
    beliefs = initial_beliefs
    new_beliefs = None
    states = start_state_array
    next_states = None

    # Tracking what simulations are done
    sim_is_done = xp.zeros(n, dtype=bool)
    done_at_step = xp.full(n, -1)

    # Speedup item
    simulations = xp.arange(n)
    flatten_offset = (simulations[:,None] * model.state_count)
    flat_shape = (n, (model.state_count * model.reachable_state_count))

    # 2D bincount for belief set update
    def bincount2D_vectorized(a, w):    
        a_offs = a + flatten_offset
        return xp.bincount(a_offs.ravel(), weights=w.ravel(), minlength=a.shape[0]*model.state_count).reshape(-1,model.state_count)

    # Results
    discount = reward_discount
    rewards = []
    discounted_rewards = []

    # History items
    states_history = [states]
    actions_history = []
    observations_history = []

    sim_start_ts = datetime.now()
    
    iterator = trange(max_steps) if print_progress else range(max_steps)
    for i in iterator:
        # Retrieving the top vectors according to the value function
        best_vectors = xp.argmax(xp.matmul(beliefs, self.value_function.alpha_vector_array.T), axis=1)

        # Retrieving the actions associated with the vectors chosen
        best_actions = self.value_function.actions[best_vectors]

        # Get each reachable next states for each action
        reachable_state_per_actions = model.reachable_states[:, best_actions, :]

        # Gathering new states based on the transition function and the chosen actions
        next_state_potentials = reachable_state_per_actions[states, simulations]
        if model.reachable_state_count == 1:
            next_states = next_state_potentials[:,0]
        else:
            potential_probabilities = model.reachable_probabilities[states[:,None], best_actions[:,None]][:,0,:]
            chosen_indices = xp.apply_along_axis(lambda x: xp.random.choice(len(x), size=1, p=x), axis=1, arr=potential_probabilities)
            next_states = next_state_potentials[chosen_indices][:,0,0]

        # Making observations based on the states landed in and the action that was taken
        observation_probabilities = model.observation_table[next_states[:,None], best_actions[:,None]][:,0,:]
        observations = xp.sum(xp.random.random(n)[:,None] &gt; xp.cumsum(observation_probabilities[:,:-1], axis=1), axis=1)

        # Belief set update
        reachable_probabilities = (model.reachable_transitional_observation_table[:,best_actions,observations,:] * beliefs.T[:,:,None])
        new_beliefs = bincount2D_vectorized(a=reachable_state_per_actions.swapaxes(0,1).reshape(flat_shape),
                                            w=reachable_probabilities.swapaxes(0,1).reshape(flat_shape))

        new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]

        # Rewards computation
        step_rewards = xp.array([model.immediate_reward_function(int(s), int(a), int(s_p), int(o)) for s,a,s_p,o in zip(states, best_actions, next_states, observations)])
        rewards.append(xp.where(~sim_is_done, step_rewards, 0))
        discounted_rewards.append(xp.where(~sim_is_done, step_rewards * discount, 0))

        # Checking for done condition
        are_done = xp.isin(next_states, xp.array(model.end_states))
        done_at_step[sim_is_done ^ are_done] = i+1
        sim_is_done |= are_done

        # Update iterator postfix
        if print_progress:
            iterator.set_postfix({&#39;done&#39;: xp.sum(sim_is_done)})

        # Replacing old with new
        states = next_states
        beliefs = new_beliefs
        discount *= reward_discount

        # History tracking
        states_history.append(states)
        actions_history.append(best_actions)
        observations_history.append(observations)

        # Early stopping
        if xp.all(sim_is_done):
            break

    # Wrap up
    sim_hist_list = []
    b0 = Belief(model)

    states_history = xp.array(states_history).T
    actions_history = xp.array(actions_history).T
    observations_history = xp.array(observations_history).T
    rewards = xp.array(rewards).T
    discounted_rewards = xp.array(discounted_rewards).T

    done_at_step_sum = 0

    for i, s0 in enumerate(start_state_array):
        sim_hist = SimulationHistory(self.model, int(s0), b0)
        
        last_step = int(done_at_step[i]) if bool(sim_is_done[i]) else max_steps
        done_at_step_sum += last_step

        # Setting the elements
        sim_hist.states = states_history[i,:last_step+1].tolist()
        sim_hist.actions = actions_history[i,:last_step].tolist()
        sim_hist.observations = observations_history[i,:last_step].tolist()
        sim_hist.rewards = rewards[i,:last_step].tolist()

        # Adding the sim history to the list of histories
        sim_hist_list.append(sim_hist)

    done_sim_count = sum([1 if done else 0 for done in sim_is_done])

    if print_stats:
        sim_end_ts = datetime.now()
        print(f&#39;All {n} simulations done in {(sim_end_ts - sim_start_ts).total_seconds():.3f}s:&#39;)
        print(f&#39;\t- Simulations reached goal: {done_sim_count}/{n} ({n-done_sim_count} failures)&#39;)
        print(f&#39;\t- Average step count: {(done_at_step_sum / n)}&#39;)
        print(f&#39;\t- Average total rewards: {(xp.sum(rewards) / n)}&#39;)
        print(f&#39;\t- Average discounted rewards (ADR): {(xp.sum(discounted_rewards) / n)}&#39;)

    return RewardSet(xp.sum(rewards, axis=1).tolist()), sim_hist_list</code></pre>
</details>
</dd>
<dt id="src.pomdp.Agent.simulate"><code class="name flex">
<span>def <span class="ident">simulate</span></span>(<span>self, simulator: Optional[<a title="src.pomdp.Simulation" href="#src.pomdp.Simulation">Simulation</a>] = None, max_steps: int = 1000, start_state: Optional[int] = None, print_progress: bool = True, print_stats: bool = True) ‑> <a title="src.pomdp.SimulationHistory" href="#src.pomdp.SimulationHistory">SimulationHistory</a></span>
</code></dt>
<dd>
<div class="desc"><p>Function to run a simulation with the current agent for up to 'max_steps' amount of steps using a Simulation simulator.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>simulator</code></strong> :&ensp;<code>pomdp.Simulation</code>, optional</dt>
<dd>The simulation that will be used by the agent. If not provided, the default MDP simulator will be used.</dd>
<dt><strong><code>max_steps</code></strong> :&ensp;<code>int</code>, default=<code>1000</code></dt>
<dd>The max amount of steps the simulation can run for.</dd>
<dt><strong><code>start_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The state the agent should start in, if not provided, will be set at random based on start probabilities of the model.</dd>
<dt><strong><code>print_progress</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether or not to print out the progress of the simulation.</dd>
<dt><strong><code>print_stats</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether or not to print simulation statistics at the end of the simulation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>history</code></strong> :&ensp;<code><a title="src.pomdp.SimulationHistory" href="#src.pomdp.SimulationHistory">SimulationHistory</a></code></dt>
<dd>A list of rewards with the additional functionality that the can be plot with the plot() function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulate(self,
             simulator:Union[Simulation,None]=None,
             max_steps:int=1000,
             start_state:Union[int,None]=None,
             print_progress:bool=True,
             print_stats:bool=True
             ) -&gt; SimulationHistory:
    &#39;&#39;&#39;
    Function to run a simulation with the current agent for up to &#39;max_steps&#39; amount of steps using a Simulation simulator.

    Parameters
    ----------
    simulator : pomdp.Simulation, optional
        The simulation that will be used by the agent. If not provided, the default MDP simulator will be used.
    max_steps : int, default=1000
        The max amount of steps the simulation can run for.
    start_state : int, optional
        The state the agent should start in, if not provided, will be set at random based on start probabilities of the model.
    print_progress : bool, default=True
        Whether or not to print out the progress of the simulation.
    print_stats : bool, default=True
        Whether or not to print simulation statistics at the end of the simulation.

    Returns
    -------
    history : SimulationHistory
        A list of rewards with the additional functionality that the can be plot with the plot() function.
    &#39;&#39;&#39;
    assert self.value_function is not None, &#34;No value function, training probably has to be run...&#34;

    # GPU setup
    self.model = self.model.gpu_model if self.value_function.is_on_gpu else self.model.cpu_model

    # Get or generate a default simulator
    if simulator is None:
        simulator = Simulation(self.model)

    # reset
    s = simulator.initialize_simulation(start_state=start_state) # s is only used for the simulation history
    belief = Belief(self.model)

    history = SimulationHistory(self.model, start_state=s, start_belief=belief)

    sim_start_ts = datetime.now()

    # Simulation loop
    for _ in (trange(max_steps) if print_progress else range(max_steps)):
        # Play best action
        a = self.get_best_action(belief)
        r,o = simulator.run_action(a)

        # Update the belief
        new_belief = belief.update(a, o)

        # Post action history recording
        history.add(action=a, next_state=simulator.agent_state, next_belief=new_belief, reward=r, observation=o)

        # Replace belief
        belief = new_belief

        # If simulation is considered done, the rewards are simply returned
        if simulator.is_done:
            break
        
    if print_stats:
        sim_end_ts = datetime.now()
        print(&#39;Simulation done:&#39;)
        print(f&#39;\t- Runtime (s): {(sim_end_ts - sim_start_ts).total_seconds()}&#39;)
        print(f&#39;\t- Steps: {len(history.states)}&#39;)
        print(f&#39;\t- Total rewards: {sum(history.rewards)}&#39;)
        print(f&#39;\t- End state: {self.model.state_labels[history.states[-1]]}&#39;)

    return history</code></pre>
</details>
</dd>
<dt id="src.pomdp.Agent.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, solver: <a title="src.pomdp.PBVI_Solver" href="#src.pomdp.PBVI_Solver">PBVI_Solver</a>, expansions: int, horizon: int) ‑> <a title="src.pomdp.SolverHistory" href="#src.pomdp.SolverHistory">SolverHistory</a></span>
</code></dt>
<dd>
<div class="desc"><p>Method to train the agent using a given solver.
The solver will provide a value function that will map beliefs in belief space to actions.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>solver</code></strong> :&ensp;<code><a title="src.pomdp.PBVI_Solver" href="#src.pomdp.PBVI_Solver">PBVI_Solver</a></code></dt>
<dd>The solver to run.</dd>
<dt><strong><code>expansions</code></strong> :&ensp;<code>int</code></dt>
<dd>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</dd>
<dt><strong><code>horizon</code></strong> :&ensp;<code>int</code></dt>
<dd>How many times the alpha vector set must be updated every time the belief set is expanded.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>solve_history</code></strong> :&ensp;<code><a title="src.pomdp.SolverHistory" href="#src.pomdp.SolverHistory">SolverHistory</a></code></dt>
<dd>The history of the solving process.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, solver:PBVI_Solver, expansions:int, horizon:int) -&gt; SolverHistory:
    &#39;&#39;&#39;
    Method to train the agent using a given solver.
    The solver will provide a value function that will map beliefs in belief space to actions.

    Parameters
    ----------
    solver : PBVI_Solver
        The solver to run.
    expansions : int
        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)
    horizon : int
        How many times the alpha vector set must be updated every time the belief set is expanded.
    
    Returns
    -------
    solve_history : SolverHistory
        The history of the solving process.
    &#39;&#39;&#39;
    self.value_function, solve_history = solver.solve(self.model, expansions, horizon)
    return solve_history</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.pomdp.Belief"><code class="flex name class">
<span>class <span class="ident">Belief</span></span>
<span>(</span><span>model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, values: Optional[numpy.ndarray] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>A class representing a belief in the space of a given model. It is the belief to be in any combination of states:
eg:
- In a 2 state POMDP: a belief of (0.5, 0.5) represent the complete ignorance of which state we are in. Where a (1.0, 0.0) belief is the certainty to be in state 0.</p>
<p>The belief update function has been implemented based on the belief update define in the paper of J. Pineau, G. Gordon, and S. Thrun, 'Point-based approximations for fast POMDP solving'</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The model on which the belief applies on.</dd>
<dt><strong><code>values</code></strong> :&ensp;<code>np.ndarray</code>, optional</dt>
<dd>A vector of the probabilities to be in each state of the model. The sum of the probabilities must sum to 1.
If not specified, it will be set as the start probabilities of the model.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>values</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>bytes_repr</code></strong> :&ensp;<code>bytes</code></dt>
<dd>A representation in bytes of the value of the belief</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Belief:
    &#39;&#39;&#39;
    A class representing a belief in the space of a given model. It is the belief to be in any combination of states:
    eg:
        - In a 2 state POMDP: a belief of (0.5, 0.5) represent the complete ignorance of which state we are in. Where a (1.0, 0.0) belief is the certainty to be in state 0.

    The belief update function has been implemented based on the belief update define in the paper of J. Pineau, G. Gordon, and S. Thrun, &#39;Point-based approximations for fast POMDP solving&#39;

    ...

    Parameters
    ----------
    model : pomdp.Model
        The model on which the belief applies on.
    values : np.ndarray, optional
        A vector of the probabilities to be in each state of the model. The sum of the probabilities must sum to 1.
        If not specified, it will be set as the start probabilities of the model.

    Attributes
    ----------
    model : pomdp.Model
    values : np.ndarray
    bytes_repr : bytes
        A representation in bytes of the value of the belief
    &#39;&#39;&#39;
    def __init__(self, model:Model, values:Union[np.ndarray,None]=None):
        assert model is not None
        self.model = model

        if values is not None:
            assert values.shape[0] == model.state_count, &#34;Belief must contain be of dimension |S|&#34;

            xp = np if not gpu_support else cp.get_array_module(values)

            prob_sum = xp.sum(values)
            rounded_sum = xp.round(prob_sum, decimals=3)
            assert rounded_sum == 1.0, f&#34;States probabilities in belief must sum to 1 (found: {prob_sum}; rounded {rounded_sum})&#34;

            self._values = values
        else:
            self._values = model.start_probabilities


    def __new__(cls, *args, **kwargs):
        instance = super().__new__(cls)

        instance._bytes_repr = None
        instance._successors = {}
        
        return instance


    @property
    def bytes_repr(self) -&gt; bytes:
        if self._bytes_repr is None:
            self._bytes_repr = self.values.tobytes()
        return self._bytes_repr


    def __eq__(self, other: object) -&gt; bool:
        return self.bytes_repr == other.bytes_repr

    
    @property
    def values(self) -&gt; np.ndarray:
        &#39;&#39;&#39;
        An array of the probability distribution to be in each state.
        &#39;&#39;&#39;
        return self._values
    

    def update(self, a:int, o:int) -&gt; &#39;Belief&#39;:
        &#39;&#39;&#39;
        Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).

        Parameters
        ----------
        a : int
            The most recent action.
        o : int
            The most recent observation.

        Returns
        -------
        new_belief : Belief
            An updated belief
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(self._values)

        succ_id = f&#39;{a}_{o}&#39;
        succ = self._successors.get(succ_id)
        if succ is not None:
            return succ

        reachable_state_probabilities = self.model.reachable_transitional_observation_table[:,a,o,:] * self.values[:,None]
        new_state_probabilities = xp.bincount(self.model.reachable_states[:,a,:].flatten(), weights=reachable_state_probabilities.flatten(), minlength=self.model.state_count)
        
        # Normalization
        new_state_probabilities /= xp.sum(new_state_probabilities)

        # Generation of new belief from new state probabilities
        new_belief = self.__new__(self.__class__)
        new_belief.model = self.model
        new_belief._values = new_state_probabilities

        # Remember generated successor
        self._successors[succ_id] = new_belief

        return new_belief
    

    def generate_successors(self) -&gt; list[&#39;Belief&#39;]:
        &#39;&#39;&#39;
        Function to generate a set of belief that can be reached for each actions and observations available in the model.

        Returns
        -------
        successor_beliefs : list[Belief]
            The successor beliefs.
        &#39;&#39;&#39;
        successor_beliefs = []
        for a in self.model.actions:
            for o in self.model.observations:
                successor_beliefs.append(self.update(a,o))

        return successor_beliefs


    def random_state(self) -&gt; int:
        &#39;&#39;&#39;
        Returns a random state of the model weighted by the belief probabily.

        Returns
        -------
        rand_s : int
            A random state.
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(self._values)

        rand_s = int(xp.random.choice(a=self.model.states, size=1, p=self._values)[0])
        return rand_s
    

    def plot(self, size:int=5) -&gt; None:
        &#39;&#39;&#39;
        Function to plot a heatmap of the belief distribution if the belief is of a grid model.

        Parameters
        ----------
        size : int, default=5
            The scale of the plot.
        &#39;&#39;&#39;
        # Plot setup
        plt.figure(figsize=(size*1.2,size))

        model = self.model.cpu_model

        # Ticks
        dimensions = model.state_grid.shape
        x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))
        y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))

        plt.xticks(x_ticks)
        plt.yticks(y_ticks)

        # Title
        plt.title(f&#39;Belief (probability distribution over states)&#39;)

        # Actual plot
        belief_values = self._values if (not gpu_support) or (cp.get_array_module(self._values) == np) else cp.asnumpy(self._values)
        grid_values = belief_values[model.state_grid]
        plt.imshow(grid_values,cmap=&#39;Blues&#39;)
        plt.colorbar()
        plt.show()</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="src.pomdp.Belief.bytes_repr"><code class="name">var <span class="ident">bytes_repr</span> : bytes</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def bytes_repr(self) -&gt; bytes:
    if self._bytes_repr is None:
        self._bytes_repr = self.values.tobytes()
    return self._bytes_repr</code></pre>
</details>
</dd>
<dt id="src.pomdp.Belief.values"><code class="name">var <span class="ident">values</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>An array of the probability distribution to be in each state.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def values(self) -&gt; np.ndarray:
    &#39;&#39;&#39;
    An array of the probability distribution to be in each state.
    &#39;&#39;&#39;
    return self._values</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.pomdp.Belief.generate_successors"><code class="name flex">
<span>def <span class="ident">generate_successors</span></span>(<span>self) ‑> list[<a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Function to generate a set of belief that can be reached for each actions and observations available in the model.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>successor_beliefs</code></strong> :&ensp;<code>list[<a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>]</code></dt>
<dd>The successor beliefs.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_successors(self) -&gt; list[&#39;Belief&#39;]:
    &#39;&#39;&#39;
    Function to generate a set of belief that can be reached for each actions and observations available in the model.

    Returns
    -------
    successor_beliefs : list[Belief]
        The successor beliefs.
    &#39;&#39;&#39;
    successor_beliefs = []
    for a in self.model.actions:
        for o in self.model.observations:
            successor_beliefs.append(self.update(a,o))

    return successor_beliefs</code></pre>
</details>
</dd>
<dt id="src.pomdp.Belief.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, size: int = 5) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to plot a heatmap of the belief distribution if the belief is of a grid model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code>, default=<code>5</code></dt>
<dd>The scale of the plot.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self, size:int=5) -&gt; None:
    &#39;&#39;&#39;
    Function to plot a heatmap of the belief distribution if the belief is of a grid model.

    Parameters
    ----------
    size : int, default=5
        The scale of the plot.
    &#39;&#39;&#39;
    # Plot setup
    plt.figure(figsize=(size*1.2,size))

    model = self.model.cpu_model

    # Ticks
    dimensions = model.state_grid.shape
    x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))
    y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))

    plt.xticks(x_ticks)
    plt.yticks(y_ticks)

    # Title
    plt.title(f&#39;Belief (probability distribution over states)&#39;)

    # Actual plot
    belief_values = self._values if (not gpu_support) or (cp.get_array_module(self._values) == np) else cp.asnumpy(self._values)
    grid_values = belief_values[model.state_grid]
    plt.imshow(grid_values,cmap=&#39;Blues&#39;)
    plt.colorbar()
    plt.show()</code></pre>
</details>
</dd>
<dt id="src.pomdp.Belief.random_state"><code class="name flex">
<span>def <span class="ident">random_state</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a random state of the model weighted by the belief probabily.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>rand_s</code></strong> :&ensp;<code>int</code></dt>
<dd>A random state.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_state(self) -&gt; int:
    &#39;&#39;&#39;
    Returns a random state of the model weighted by the belief probabily.

    Returns
    -------
    rand_s : int
        A random state.
    &#39;&#39;&#39;
    xp = np if not gpu_support else cp.get_array_module(self._values)

    rand_s = int(xp.random.choice(a=self.model.states, size=1, p=self._values)[0])
    return rand_s</code></pre>
</details>
</dd>
<dt id="src.pomdp.Belief.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, a: int, o: int) ‑> <a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></span>
</code></dt>
<dd>
<div class="desc"><p>Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>int</code></dt>
<dd>The most recent action.</dd>
<dt><strong><code>o</code></strong> :&ensp;<code>int</code></dt>
<dd>The most recent observation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>new_belief</code></strong> :&ensp;<code><a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></code></dt>
<dd>An updated belief</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, a:int, o:int) -&gt; &#39;Belief&#39;:
    &#39;&#39;&#39;
    Returns a new belief based on this current belief, the most recent action (a) and the most recent observation (o).

    Parameters
    ----------
    a : int
        The most recent action.
    o : int
        The most recent observation.

    Returns
    -------
    new_belief : Belief
        An updated belief
    &#39;&#39;&#39;
    xp = np if not gpu_support else cp.get_array_module(self._values)

    succ_id = f&#39;{a}_{o}&#39;
    succ = self._successors.get(succ_id)
    if succ is not None:
        return succ

    reachable_state_probabilities = self.model.reachable_transitional_observation_table[:,a,o,:] * self.values[:,None]
    new_state_probabilities = xp.bincount(self.model.reachable_states[:,a,:].flatten(), weights=reachable_state_probabilities.flatten(), minlength=self.model.state_count)
    
    # Normalization
    new_state_probabilities /= xp.sum(new_state_probabilities)

    # Generation of new belief from new state probabilities
    new_belief = self.__new__(self.__class__)
    new_belief.model = self.model
    new_belief._values = new_state_probabilities

    # Remember generated successor
    self._successors[succ_id] = new_belief

    return new_belief</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.pomdp.BeliefSet"><code class="flex name class">
<span>class <span class="ident">BeliefSet</span></span>
<span>(</span><span>model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, beliefs: Union[list[<a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>], numpy.ndarray])</span>
</code></dt>
<dd>
<div class="desc"><p>Class to represent a set of beliefs with regard to a POMDP model.
It has the purpose to store the beliefs in numpy array format and be able to conver it to a list of Belief class objects.
This class also provides the option to display the beliefs when operating on a 2 or 3d space with the plot() function.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The model on which the beliefs apply.</dd>
<dt><strong><code>beliefs</code></strong> :&ensp;<code>list[Belief] | np.ndarray</code></dt>
<dd>The actual set of beliefs.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>belief_array</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 2D array of shape N x S of N belief vectors.</dd>
<dt><strong><code>belief_list</code></strong> :&ensp;<code>list[<a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>]</code></dt>
<dd>A list of N Belief object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BeliefSet:
    &#39;&#39;&#39;
    Class to represent a set of beliefs with regard to a POMDP model.
    It has the purpose to store the beliefs in numpy array format and be able to conver it to a list of Belief class objects.
    This class also provides the option to display the beliefs when operating on a 2 or 3d space with the plot() function.
    
    ...

    Parameters
    ----------
    model : pomdp.Model
        The model on which the beliefs apply.
    beliefs : list[Belief] | np.ndarray
        The actual set of beliefs.

    Attributes
    ----------
    model : pomdp.Model
    belief_array : np.ndarray
        A 2D array of shape N x S of N belief vectors.
    belief_list : list[Belief]
        A list of N Belief object.
    &#39;&#39;&#39;
    def __init__(self, model:Model, beliefs:Union[list[Belief],np.ndarray]) -&gt; None:
        self.model = model

        self._belief_list = None
        self._belief_array = None

        self.is_on_gpu = False

        if isinstance(beliefs, list):
            assert all(len(b.values) == model.state_count for b in beliefs), f&#34;Beliefs in belief list provided dont all have shape ({model.state_count},)&#34;
            self._belief_list = beliefs

            # Check if on gpu and make sure all beliefs are also on the gpu
            if (len(beliefs) &gt; 0) and gpu_support and cp.get_array_module(beliefs[0].values) == cp:
                assert all(cp.get_array_module(b.values) == cp for b in beliefs), &#34;Either all or none of the alpha vectors should be on the GPU, not just some.&#34;
                self.is_on_gpu = True
        else:
            assert beliefs.shape[1] == model.state_count, f&#34;Belief array provided doesnt have the right shape (expected (-,{model.state_count}), received {beliefs.shape})&#34;
            
            self._belief_list = []
            for belief_values in beliefs:
                self._belief_list.append(Belief(model, belief_values))

            # Check if array is on gpu
            if gpu_support and cp.get_array_module(beliefs) == cp:
                self.is_on_gpu = True

        # # Deduplication
        # self._uniqueness_dict = {belief.values.tobytes(): belief for belief in self._belief_list}
        # self._belief_list = list(self._uniqueness_dict.values())


    @property
    def belief_array(self) -&gt; np.ndarray:
        &#39;&#39;&#39;
        A matrix of size N x S containing N belief vectors. If belief set is stored as a list of Belief objects, the matrix of beliefs will be generated from them.
        &#39;&#39;&#39;
        xp = cp if (gpu_support and self.is_on_gpu) else np

        if self._belief_array is None:
            self._belief_array = xp.array([b.values for b in self._belief_list])
        return self._belief_array
    

    @property
    def belief_list(self) -&gt; list[Belief]:
        &#39;&#39;&#39;
        A list of Belief objects. If the belief set is represented as a matrix of Belief vectors, the list of Belief objects will be generated from it.
        &#39;&#39;&#39;
        if self._belief_list is None:
            self._belief_list = [Belief(self.model, belief_vector) for belief_vector in self._belief_array]
        return self._belief_list
    

    def generate_all_successors(self) -&gt; &#39;BeliefSet&#39;:
        &#39;&#39;&#39;
        Function to generate the successors beliefs of all the beliefs in the belief set.

        Returns
        -------
        all_successors : BeliefSet
            All successors of all beliefs in the belief set.
        &#39;&#39;&#39;
        all_successors = []
        for belief in self.belief_list:
            all_successors.extend(belief.generate_successors())
        return BeliefSet(self.model, all_successors)
    

    def __len__(self) -&gt; int:
        return len(self._belief_list) if self._belief_list is not None else self._belief_array.shape[0]
    

    def to_gpu(self) -&gt; &#39;BeliefSet&#39;:
        &#39;&#39;&#39;
        Function returning an equivalent belief set object with the array of values stored on GPU instead of CPU.

        Returns
        -------
        gpu_belief_set : BeliefSet
            A new belief set with array on GPU.
        &#39;&#39;&#39;
        assert gpu_support, &#34;GPU support is not enabled, unable to execute this function&#34;

        gpu_model = self.model.gpu_model

        gpu_belief_set = None
        if self._belief_array is not None:
            gpu_belief_array = cp.array(self._belief_array)
            gpu_belief_set = BeliefSet(gpu_model, gpu_belief_array)
        else:
            gpu_belief_list = [Belief(gpu_model, cp.array(b.values)) for b in self._belief_list]
            gpu_belief_set = BeliefSet(gpu_model, gpu_belief_list)

        return gpu_belief_set
    

    def to_cpu(self) -&gt; &#39;BeliefSet&#39;:
        &#39;&#39;&#39;
        Function returning an equivalent belief set object with the array of values stored on CPU instead of GPU.

        Returns
        -------
        cpu_belief_set : BeliefSet
            A new belief set with array on CPU.
        &#39;&#39;&#39;
        assert gpu_support, &#34;GPU support is not enabled, unable to execute this function&#34;

        cpu_model = self.model.cpu_model

        cpu_belief_set = None
        if self._belief_array is not None:
            cpu_belief_array = cp.asnumpy(self._belief_array)
            cpu_belief_set = BeliefSet(cpu_model, cpu_belief_array)
        
        else:
            cpu_belief_list = [Belief(cpu_model, cp.asnumpy(b.values)) for b in self._belief_list]
            cpu_belief_set = BeliefSet(cpu_model, cpu_belief_list)

        return cpu_belief_set
    
    
    def plot(self, size:int=15):
        &#39;&#39;&#39;
        Function to plot the beliefs in the belief set.
        Note: Only works for 2-state and 3-state believes.

        Parameters
        ----------
        size : int, default=15
            The figure size and general scaling factor
        &#39;&#39;&#39;
        assert self.model.state_count in [2,3], &#34;Can&#39;t plot for models with state count other than 2 or 3&#34;

        # If on GPU, convert to CPU and plot that one
        if self.is_on_gpu:
            print(&#39;[Warning] Value function on GPU, converting to numpy before plotting...&#39;)
            cpu_belief_set = self.to_cpu()
            cpu_belief_set.plot(size)
            return

        if self.model.state_count == 2:
            self._plot_2D(size)
        elif self.model.state_count == 3:
            self._plot_3D(size)


    def _plot_2D(self, size=15):
        beliefs_x = self.belief_array[:,1]

        plt.figure(figsize=(size, max([int(size/7),1])))
        plt.scatter(beliefs_x, np.zeros(beliefs_x.shape[0]), c=list(range(beliefs_x.shape[0])), cmap=&#39;Blues&#39;)
        ax = plt.gca()
        ax.get_yaxis().set_visible(False)

        # Set title and ax-label
        ax.set_title(&#39;Set of beliefs&#39;)
        ax.set_xlabel(&#39;Belief space&#39;)

        # X-axis setting
        ticks = [0,0.25,0.5,0.75,1]
        x_ticks = [str(t) for t in ticks]
        x_ticks[0] = self.model.state_labels[0]
        x_ticks[-1] = self.model.state_labels[1]

        plt.xticks(ticks, x_ticks)
        plt.show()


    def _plot_3D(self, size=15):
        # Function to project points to a simplex triangle
        def projectSimplex(points):
            &#34;&#34;&#34; 
            Project probabilities on the 3-simplex to a 2D triangle
            
            N points are given as N x 3 array
            &#34;&#34;&#34;
            # Convert points one at a time
            tripts = np.zeros((points.shape[0],2))
            for idx in range(points.shape[0]):
                # Init to triangle centroid
                x = 1.0 / 2
                y = 1.0 / (2 * np.sqrt(3))
                # Vector 1 - bisect out of lower left vertex 
                p1 = points[idx, 0]
                x = x - (1.0 / np.sqrt(3)) * p1 * np.cos(np.pi / 6)
                y = y - (1.0 / np.sqrt(3)) * p1 * np.sin(np.pi / 6)
                # Vector 2 - bisect out of lower right vertex  
                p2 = points[idx, 1]  
                x = x + (1.0 / np.sqrt(3)) * p2 * np.cos(np.pi / 6)
                y = y - (1.0 / np.sqrt(3)) * p2 * np.sin(np.pi / 6)        
                # Vector 3 - bisect out of top vertex
                p3 = points[idx, 2]
                y = y + (1.0 / np.sqrt(3) * p3)
            
                tripts[idx,:] = (x,y)

            return tripts
        
        # Plotting the simplex 
        def plotSimplex(points,
                        fig=None,
                        vertexlabels=[&#39;s_0&#39;,&#39;s_1&#39;,&#39;s_2&#39;],
                        **kwargs):
            &#34;&#34;&#34;
            Plot Nx3 points array on the 3-simplex 
            (with optionally labeled vertices) 
            
            kwargs will be passed along directly to matplotlib.pyplot.scatter    
            Returns Figure, caller must .show()
            &#34;&#34;&#34;
            if(fig == None):        
                fig = plt.figure()
            # Draw the triangle
            l1 = Line2D([0, 0.5, 1.0, 0], # xcoords
                        [0, np.sqrt(3) / 2, 0, 0], # ycoords
                        color=&#39;k&#39;)
            fig.gca().add_line(l1)
            fig.gca().xaxis.set_major_locator(ticker.NullLocator())
            fig.gca().yaxis.set_major_locator(ticker.NullLocator())
            # Draw vertex labels
            fig.gca().text(-0.05, -0.05, vertexlabels[0])
            fig.gca().text(1.05, -0.05, vertexlabels[1])
            fig.gca().text(0.5, np.sqrt(3) / 2 + 0.05, vertexlabels[2])
            # Project and draw the actual points
            projected = projectSimplex(points)
            plt.scatter(projected[:,0], projected[:,1], **kwargs)              
            # Leave some buffer around the triangle for vertex labels
            fig.gca().set_xlim(-0.2, 1.2)
            fig.gca().set_ylim(-0.2, 1.2)

            return fig

        # Actual plot
        fig = plt.figure(figsize=(size,size))
        plt.title(&#39;Set of Beliefs&#39;)

        cmap = cm.get_cmap(&#39;Blues&#39;)
        norm = colors.Normalize(vmin=0, vmax=self.belief_array.shape[0])
        c = range(self.belief_array.shape[0])
        # Do scatter plot
        fig = plotSimplex(self.belief_array, fig=fig, vertexlabels=self.model.state_labels, s=size, c=c, cmap=cmap, norm=norm)

        plt.show()</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="src.pomdp.BeliefSet.belief_array"><code class="name">var <span class="ident">belief_array</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>A matrix of size N x S containing N belief vectors. If belief set is stored as a list of Belief objects, the matrix of beliefs will be generated from them.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def belief_array(self) -&gt; np.ndarray:
    &#39;&#39;&#39;
    A matrix of size N x S containing N belief vectors. If belief set is stored as a list of Belief objects, the matrix of beliefs will be generated from them.
    &#39;&#39;&#39;
    xp = cp if (gpu_support and self.is_on_gpu) else np

    if self._belief_array is None:
        self._belief_array = xp.array([b.values for b in self._belief_list])
    return self._belief_array</code></pre>
</details>
</dd>
<dt id="src.pomdp.BeliefSet.belief_list"><code class="name">var <span class="ident">belief_list</span> : list[<a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>]</code></dt>
<dd>
<div class="desc"><p>A list of Belief objects. If the belief set is represented as a matrix of Belief vectors, the list of Belief objects will be generated from it.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def belief_list(self) -&gt; list[Belief]:
    &#39;&#39;&#39;
    A list of Belief objects. If the belief set is represented as a matrix of Belief vectors, the list of Belief objects will be generated from it.
    &#39;&#39;&#39;
    if self._belief_list is None:
        self._belief_list = [Belief(self.model, belief_vector) for belief_vector in self._belief_array]
    return self._belief_list</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.pomdp.BeliefSet.generate_all_successors"><code class="name flex">
<span>def <span class="ident">generate_all_successors</span></span>(<span>self) ‑> <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>Function to generate the successors beliefs of all the beliefs in the belief set.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>all_successors</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>All successors of all beliefs in the belief set.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_all_successors(self) -&gt; &#39;BeliefSet&#39;:
    &#39;&#39;&#39;
    Function to generate the successors beliefs of all the beliefs in the belief set.

    Returns
    -------
    all_successors : BeliefSet
        All successors of all beliefs in the belief set.
    &#39;&#39;&#39;
    all_successors = []
    for belief in self.belief_list:
        all_successors.extend(belief.generate_successors())
    return BeliefSet(self.model, all_successors)</code></pre>
</details>
</dd>
<dt id="src.pomdp.BeliefSet.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, size: int = 15)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to plot the beliefs in the belief set.
Note: Only works for 2-state and 3-state believes.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code>, default=<code>15</code></dt>
<dd>The figure size and general scaling factor</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(self, size:int=15):
    &#39;&#39;&#39;
    Function to plot the beliefs in the belief set.
    Note: Only works for 2-state and 3-state believes.

    Parameters
    ----------
    size : int, default=15
        The figure size and general scaling factor
    &#39;&#39;&#39;
    assert self.model.state_count in [2,3], &#34;Can&#39;t plot for models with state count other than 2 or 3&#34;

    # If on GPU, convert to CPU and plot that one
    if self.is_on_gpu:
        print(&#39;[Warning] Value function on GPU, converting to numpy before plotting...&#39;)
        cpu_belief_set = self.to_cpu()
        cpu_belief_set.plot(size)
        return

    if self.model.state_count == 2:
        self._plot_2D(size)
    elif self.model.state_count == 3:
        self._plot_3D(size)</code></pre>
</details>
</dd>
<dt id="src.pomdp.BeliefSet.to_cpu"><code class="name flex">
<span>def <span class="ident">to_cpu</span></span>(<span>self) ‑> <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>Function returning an equivalent belief set object with the array of values stored on CPU instead of GPU.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>cpu_belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>A new belief set with array on CPU.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_cpu(self) -&gt; &#39;BeliefSet&#39;:
    &#39;&#39;&#39;
    Function returning an equivalent belief set object with the array of values stored on CPU instead of GPU.

    Returns
    -------
    cpu_belief_set : BeliefSet
        A new belief set with array on CPU.
    &#39;&#39;&#39;
    assert gpu_support, &#34;GPU support is not enabled, unable to execute this function&#34;

    cpu_model = self.model.cpu_model

    cpu_belief_set = None
    if self._belief_array is not None:
        cpu_belief_array = cp.asnumpy(self._belief_array)
        cpu_belief_set = BeliefSet(cpu_model, cpu_belief_array)
    
    else:
        cpu_belief_list = [Belief(cpu_model, cp.asnumpy(b.values)) for b in self._belief_list]
        cpu_belief_set = BeliefSet(cpu_model, cpu_belief_list)

    return cpu_belief_set</code></pre>
</details>
</dd>
<dt id="src.pomdp.BeliefSet.to_gpu"><code class="name flex">
<span>def <span class="ident">to_gpu</span></span>(<span>self) ‑> <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>Function returning an equivalent belief set object with the array of values stored on GPU instead of CPU.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>gpu_belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>A new belief set with array on GPU.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_gpu(self) -&gt; &#39;BeliefSet&#39;:
    &#39;&#39;&#39;
    Function returning an equivalent belief set object with the array of values stored on GPU instead of CPU.

    Returns
    -------
    gpu_belief_set : BeliefSet
        A new belief set with array on GPU.
    &#39;&#39;&#39;
    assert gpu_support, &#34;GPU support is not enabled, unable to execute this function&#34;

    gpu_model = self.model.gpu_model

    gpu_belief_set = None
    if self._belief_array is not None:
        gpu_belief_array = cp.array(self._belief_array)
        gpu_belief_set = BeliefSet(gpu_model, gpu_belief_array)
    else:
        gpu_belief_list = [Belief(gpu_model, cp.array(b.values)) for b in self._belief_list]
        gpu_belief_set = BeliefSet(gpu_model, gpu_belief_list)

    return gpu_belief_set</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.pomdp.BeliefValueMapping"><code class="flex name class">
<span>class <span class="ident">BeliefValueMapping</span></span>
<span>(</span><span>model, corner_belief_values: <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Alternate representation of a value function, particularly for pomdp models.
It works by adding adding belief and associated value to the object.
To evaluate this version of the value function the sawtooth algorithm is used (described in Shani G. et al., "A survey of point-based POMDP solvers")</p>
<p>We can also compute the Q value for a particular belief b and action using the qva function.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The model on which the value function applies on</dd>
<dt><strong><code>corner_belief_values</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>A general value function to define the value at corner points in belief space (ie: at certainty beliefs, or when beliefs have a probability of 1 for a given state).
This is usually the solution of the MDP version of the problem.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>corner_belief_values</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>corner_values</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>Array of |S| shape, having the max value at each state based on the corner_belief_values.</dd>
<dt><strong><code>beliefs</code></strong> :&ensp;<code><a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></code></dt>
<dd>Beliefs contained in the belief-value mapping.</dd>
<dt><strong><code>belief_value_mapping</code></strong> :&ensp;<code>dict[bytes, float]</code></dt>
<dd>Mapping of beliefs points with their associated value.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BeliefValueMapping:
    &#39;&#39;&#39;
    Alternate representation of a value function, particularly for pomdp models.
    It works by adding adding belief and associated value to the object.
    To evaluate this version of the value function the sawtooth algorithm is used (described in Shani G. et al., &#34;A survey of point-based POMDP solvers&#34;)
    
    We can also compute the Q value for a particular belief b and action using the qva function.

    Parameters
    ----------
    model : pomdp.Model
        The model on which the value function applies on
    corner_belief_values : ValueFunction
        A general value function to define the value at corner points in belief space (ie: at certainty beliefs, or when beliefs have a probability of 1 for a given state).
        This is usually the solution of the MDP version of the problem.

    Attributes
    ----------
    model : pomdp.Model
    corner_belief_values : ValueFunction
    corner_values : np.ndarray
        Array of |S| shape, having the max value at each state based on the corner_belief_values.
    beliefs : Belief
        Beliefs contained in the belief-value mapping.
    belief_value_mapping : dict[bytes, float]
        Mapping of beliefs points with their associated value.
    
    &#39;&#39;&#39;
    def __init__(self, model, corner_belief_values:ValueFunction) -&gt; None:
        xp = np if not gpu_support else cp.get_array_module(corner_belief_values.alpha_vector_array)

        self.model = model
        self.corner_belief_values = corner_belief_values
        
        self.corner_values = xp.max(corner_belief_values.alpha_vector_array, axis=0)

        self.beliefs = []
        self.belief_value_mapping = {}

        self._belief_array = None
        self._value_array = None

    
    def add(self, b:Belief, v:float) -&gt; None:
        &#39;&#39;&#39;
        Function to a belief point and its associated value to the belief value mappings

        Parameters
        ----------
        b: Belief
        v: float
        &#39;&#39;&#39;
        if b not in self.beliefs:
            self.beliefs.append(b)
            self.belief_value_mapping[b.bytes_repr] = v


    @property
    def belief_array(self) -&gt; np.ndarray:
        xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)

        if self._belief_array is None:
            self._belief_array = xp.array([b.values for b in self.beliefs])

        return self._belief_array
    

    @property
    def value_array(self) -&gt; np.ndarray:
        xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)

        if self._value_array is None:
            self._value_array = xp.array(list(self.belief_value_mapping.values()))

        return self._value_array
    

    def update(self) -&gt; None:
        &#39;&#39;&#39;
        Function to update the belief and value arrays to speed up computation.
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)

        self._belief_array = xp.array([b.values for b in self.beliefs])
        self._value_array = xp.array(list(self.belief_value_mapping.values()))


    def evaluate(self, belief:Belief) -&gt; float:
        &#39;&#39;&#39;
        Runs the sawtooth algorithm to find the value at a given belief point.

        Parameters
        ----------
        belief: Belief
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(belief.values)

        # Shortcut if belief already exists in the mapping
        if belief in self.beliefs:
            return self.belief_value_mapping[belief.bytes_repr]

        v0 = xp.dot(belief.values, self.corner_values)

        if len(self.beliefs) == 0:
            return float(v0)

        with np.errstate(divide=&#39;ignore&#39;, invalid=&#39;ignore&#39;):
            vb = v0 + ((self.value_array - xp.dot(self.belief_array, self.corner_values)) * xp.min(belief.values / self.belief_array, axis=1))

        return float(xp.min(xp.append(vb, v0)))</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="src.pomdp.BeliefValueMapping.belief_array"><code class="name">var <span class="ident">belief_array</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def belief_array(self) -&gt; np.ndarray:
    xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)

    if self._belief_array is None:
        self._belief_array = xp.array([b.values for b in self.beliefs])

    return self._belief_array</code></pre>
</details>
</dd>
<dt id="src.pomdp.BeliefValueMapping.value_array"><code class="name">var <span class="ident">value_array</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def value_array(self) -&gt; np.ndarray:
    xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)

    if self._value_array is None:
        self._value_array = xp.array(list(self.belief_value_mapping.values()))

    return self._value_array</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.pomdp.BeliefValueMapping.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, b: <a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>, v: float) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to a belief point and its associated value to the belief value mappings</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>b</code></strong> :&ensp;<code><a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>v</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(self, b:Belief, v:float) -&gt; None:
    &#39;&#39;&#39;
    Function to a belief point and its associated value to the belief value mappings

    Parameters
    ----------
    b: Belief
    v: float
    &#39;&#39;&#39;
    if b not in self.beliefs:
        self.beliefs.append(b)
        self.belief_value_mapping[b.bytes_repr] = v</code></pre>
</details>
</dd>
<dt id="src.pomdp.BeliefValueMapping.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, belief: <a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Runs the sawtooth algorithm to find the value at a given belief point.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>belief</code></strong> :&ensp;<code><a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, belief:Belief) -&gt; float:
    &#39;&#39;&#39;
    Runs the sawtooth algorithm to find the value at a given belief point.

    Parameters
    ----------
    belief: Belief
    &#39;&#39;&#39;
    xp = np if not gpu_support else cp.get_array_module(belief.values)

    # Shortcut if belief already exists in the mapping
    if belief in self.beliefs:
        return self.belief_value_mapping[belief.bytes_repr]

    v0 = xp.dot(belief.values, self.corner_values)

    if len(self.beliefs) == 0:
        return float(v0)

    with np.errstate(divide=&#39;ignore&#39;, invalid=&#39;ignore&#39;):
        vb = v0 + ((self.value_array - xp.dot(self.belief_array, self.corner_values)) * xp.min(belief.values / self.belief_array, axis=1))

    return float(xp.min(xp.append(vb, v0)))</code></pre>
</details>
</dd>
<dt id="src.pomdp.BeliefValueMapping.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to update the belief and value arrays to speed up computation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self) -&gt; None:
    &#39;&#39;&#39;
    Function to update the belief and value arrays to speed up computation.
    &#39;&#39;&#39;
    xp = np if not gpu_support else cp.get_array_module(self.beliefs[0].values)

    self._belief_array = xp.array([b.values for b in self.beliefs])
    self._value_array = xp.array(list(self.belief_value_mapping.values()))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.pomdp.FSVI_Solver"><code class="flex name class">
<span>class <span class="ident">FSVI_Solver</span></span>
<span>(</span><span>gamma: float = 0.9, eps: float = 0.001, mdp_policy: Optional[<a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Solver to solve a POMDP problem based on the Forward Search Value Iteration principle.
It has been built based on the paper of G. Shani, R. I. Brafman, and S. I. Shimony: 'Forward Search Value Iteration for POMDPS'.
It works by utilizing the optimal MDP policy to generate paths to explore that lead to series of Beliefs
that can then be used by the Backup function to update the value function.</p>
<p>The solve function is the same as the pomdp.PBVI_Solver class with some parameters prefilled.</p>
<p>Note:
- The backup mode is set to new points only
- Only one update/backup is run for each iteration.</p>
<p>&hellip;
Parameters</p>
<hr>
<dl>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code>, default=<code>0.9</code></dt>
<dd>The learning rate, used to control how fast the value function will change after the each iterations.</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, default=<code>0.001</code></dt>
<dd>The treshold for convergence. If the max change between value function is lower that eps, the algorithm is considered to have converged.</dd>
<dt><strong><code>mdp_policy</code></strong> :&ensp;<code>ValueFunction</code>, optional</dt>
<dd>The value that will be used to decide actions during the MDP explore procedure. If not provided, it will be computed at solve time.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FSVI_Solver(PBVI_Solver):
    &#39;&#39;&#39;
    Solver to solve a POMDP problem based on the Forward Search Value Iteration principle.
    It has been built based on the paper of G. Shani, R. I. Brafman, and S. I. Shimony: &#39;Forward Search Value Iteration for POMDPS&#39;.
    It works by utilizing the optimal MDP policy to generate paths to explore that lead to series of Beliefs
    that can then be used by the Backup function to update the value function.

    The solve function is the same as the pomdp.PBVI_Solver class with some parameters prefilled.

    Note:
    - The backup mode is set to new points only
    - Only one update/backup is run for each iteration.
    
    ...
    Parameters
    ----------
    gamma : float, default=0.9
        The learning rate, used to control how fast the value function will change after the each iterations.
    eps : float, default=0.001
        The treshold for convergence. If the max change between value function is lower that eps, the algorithm is considered to have converged.
    mdp_policy : ValueFunction, optional
        The value that will be used to decide actions during the MDP explore procedure. If not provided, it will be computed at solve time.

    Attributes
    ----------
    gamma : float
    eps : float
    &#39;&#39;&#39;
    def __init__(self,
                 gamma:float=0.9,
                 eps:float=0.001,
                 mdp_policy:Union[ValueFunction,None]=None):
        super().__init__(gamma=gamma,
                         eps=eps,
                         expand_function=&#39;fsvi&#39;,
                         mdp_policy=mdp_policy)
        

    def solve(self,
              model:Model,
              expansions:int,
              max_belief_growth:int=10,
              initial_belief:Union[BeliefSet, Belief, None]=None,
              initial_value_function:Union[ValueFunction, None]=None,
              prune_level:int=1,
              prune_interval:int=10,
              use_gpu:bool=False,
              history_tracking_level:int=1,
              print_progress:bool=True
              ) -&gt; tuple[ValueFunction, SolverHistory]:
        return super().solve(model=model,
                             expansions=expansions,
                             full_backup=False,
                             update_passes=1,
                             max_belief_growth=max_belief_growth,
                             initial_belief=initial_belief,
                             initial_value_function=initial_value_function,
                             prune_level=prune_level,
                             prune_interval=prune_interval,
                             use_gpu=use_gpu,
                             history_tracking_level=history_tracking_level,
                             print_progress=print_progress)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="src.pomdp.PBVI_Solver" href="#src.pomdp.PBVI_Solver">PBVI_Solver</a></li>
<li><a title="src.pomdp.Solver" href="#src.pomdp.Solver">Solver</a></li>
<li><a title="src.mdp.Solver" href="mdp.html#src.mdp.Solver">Solver</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="src.pomdp.PBVI_Solver" href="#src.pomdp.PBVI_Solver">PBVI_Solver</a></b></code>:
<ul class="hlist">
<li><code><a title="src.pomdp.PBVI_Solver.backup" href="#src.pomdp.PBVI_Solver.backup">backup</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.compute_change" href="#src.pomdp.PBVI_Solver.compute_change">compute_change</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand" href="#src.pomdp.PBVI_Solver.expand">expand</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_fsvi" href="#src.pomdp.PBVI_Solver.expand_fsvi">expand_fsvi</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ger" href="#src.pomdp.PBVI_Solver.expand_ger">expand_ger</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_hsvi" href="#src.pomdp.PBVI_Solver.expand_hsvi">expand_hsvi</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_perseus" href="#src.pomdp.PBVI_Solver.expand_perseus">expand_perseus</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ra" href="#src.pomdp.PBVI_Solver.expand_ra">expand_ra</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ssea" href="#src.pomdp.PBVI_Solver.expand_ssea">expand_ssea</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ssga" href="#src.pomdp.PBVI_Solver.expand_ssga">expand_ssga</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ssra" href="#src.pomdp.PBVI_Solver.expand_ssra">expand_ssra</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.solve" href="#src.pomdp.PBVI_Solver.solve">solve</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.test_n_simulations" href="#src.pomdp.PBVI_Solver.test_n_simulations">test_n_simulations</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="src.pomdp.HSVI_Solver"><code class="flex name class">
<span>class <span class="ident">HSVI_Solver</span></span>
<span>(</span><span>gamma: float = 0.99, eps: float = 0.001, mdp_solution: Optional[<a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>TODO</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HSVI_Solver(PBVI_Solver):
    &#39;&#39;&#39;
    TODO
    &#39;&#39;&#39;
    def __init__(self,
                 gamma:float=0.99,
                 eps:float=0.001,
                 mdp_solution:Union[ValueFunction,None]=None):

        super().__init__(gamma=gamma,
                         eps=eps,
                         expand_function=&#39;hsvi&#39;,
                         mdp_policy=mdp_solution)
        
    def solve(self,
              model:Model,
              expansions:int,
              max_belief_growth:int=10,
              initial_belief:Union[BeliefSet,Belief,None]=None,
              initial_value_function:Union[ValueFunction,None]=None,
              prune_level:int=1,
              prune_interval:int=10,
              use_gpu:bool=False,
              history_tracking_level:int=1,
              print_progress:bool=True
              ) -&gt; tuple[ValueFunction, SolverHistory]:
        return super().solve(model=model,
                             expansions=expansions,
                             full_backup=False,
                             update_passes=1,
                             max_belief_growth=max_belief_growth,
                             initial_belief=initial_belief,
                             initial_value_function=initial_value_function,
                             prune_level=prune_level,
                             prune_interval=prune_interval,
                             use_gpu=use_gpu,
                             history_tracking_level=history_tracking_level,
                             print_progress=print_progress)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="src.pomdp.PBVI_Solver" href="#src.pomdp.PBVI_Solver">PBVI_Solver</a></li>
<li><a title="src.pomdp.Solver" href="#src.pomdp.Solver">Solver</a></li>
<li><a title="src.mdp.Solver" href="mdp.html#src.mdp.Solver">Solver</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="src.pomdp.PBVI_Solver" href="#src.pomdp.PBVI_Solver">PBVI_Solver</a></b></code>:
<ul class="hlist">
<li><code><a title="src.pomdp.PBVI_Solver.backup" href="#src.pomdp.PBVI_Solver.backup">backup</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.compute_change" href="#src.pomdp.PBVI_Solver.compute_change">compute_change</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand" href="#src.pomdp.PBVI_Solver.expand">expand</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_fsvi" href="#src.pomdp.PBVI_Solver.expand_fsvi">expand_fsvi</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ger" href="#src.pomdp.PBVI_Solver.expand_ger">expand_ger</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_hsvi" href="#src.pomdp.PBVI_Solver.expand_hsvi">expand_hsvi</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_perseus" href="#src.pomdp.PBVI_Solver.expand_perseus">expand_perseus</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ra" href="#src.pomdp.PBVI_Solver.expand_ra">expand_ra</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ssea" href="#src.pomdp.PBVI_Solver.expand_ssea">expand_ssea</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ssga" href="#src.pomdp.PBVI_Solver.expand_ssga">expand_ssga</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ssra" href="#src.pomdp.PBVI_Solver.expand_ssra">expand_ssra</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.solve" href="#src.pomdp.PBVI_Solver.solve">solve</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.test_n_simulations" href="#src.pomdp.PBVI_Solver.test_n_simulations">test_n_simulations</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="src.pomdp.Model"><code class="flex name class">
<span>class <span class="ident">Model</span></span>
<span>(</span><span>states: Union[int, list[str], list[list[str]]], actions: Union[int, list], observations: Union[int, list], transitions=None, reachable_states=None, rewards=None, observation_table=None, rewards_are_probabilistic: bool = False, state_grid=None, start_probabilities: Optional[list] = None, end_states: list[int] = [], end_actions: list[int] = [])</span>
</code></dt>
<dd>
<div class="desc"><p>POMDP Model class. Partially Observable Markov Decision Process Model.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>states</code></strong> :&ensp;<code>int</code> or <code>list[str]</code> or <code>list[list[str]]</code></dt>
<dd>A list of state labels or an amount of states to be used. Also allows to provide a matrix of states to define a grid model.</dd>
<dt><strong><code>actions</code></strong> :&ensp;<code>int</code> or <code>list</code></dt>
<dd>A list of action labels or an amount of actions to be used.</dd>
<dt><strong><code>observations</code></strong> :&ensp;<code>int</code> or <code>list</code></dt>
<dd>A list of observation labels or an amount of observations to be used</dd>
<dt><strong><code>transitions</code></strong> :&ensp;<code>array-like</code> or <code>function</code>, optional</dt>
<dd>The transitions between states, an array can be provided and has to be |S| x |A| x |S| or a function can be provided.
If a function is provided, it has be able to deal with np.array arguments.
If none is provided, it will be randomly generated.</dd>
<dt><strong><code>reachable_states</code></strong> :&ensp;<code>array-like</code>, optional</dt>
<dd>A list of states that can be reached from each state and actions. It must be a matrix of size |S| x |A| x |R| where |R| is the max amount of states reachable from any given state and action pair.
It is optional but useful for speedup purposes.</dd>
<dt><strong><code>rewards</code></strong> :&ensp;<code>array-like</code> or <code>function</code>, optional</dt>
<dd>The reward matrix, has to be |S| x |A| x |S|.
A function can also be provided here but it has to be able to deal with np.array arguments.
If provided, it will be use in combination with the transition matrix to fill to expected rewards.</dd>
<dt><strong><code>observation_table</code></strong> :&ensp;<code>array-like</code> or <code>function</code>, optional</dt>
<dd>The observation matrix, has to be |S| x |A| x |O|. If none is provided, it will be randomly generated.</dd>
<dt><strong><code>rewards_are_probabilistic</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether the rewards provided are probabilistic or pure rewards. If probabilist 0 or 1 will be the reward with a certain probability.</dd>
<dt><strong><code>state_grid</code></strong> :&ensp;<code>array-like</code>, optional</dt>
<dd>If provided, the model will be converted to a grid model.</dd>
<dt><strong><code>start_probabilities</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>The distribution of chances to start in each state. If not provided, there will be an uniform chance for each state. It is also used to represent a belief of complete uncertainty.</dd>
<dt><strong><code>end_states</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Entering either state in the list during a simulation will end the simulation.</dd>
<dt><strong><code>end_action</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>Playing action of the list during a simulation will end the simulation.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>states</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 1D array of states indices. Used to loop over states.</dd>
<dt><strong><code>state_labels</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>A list of state labels. (To be mainly used for plotting)</dd>
<dt><strong><code>state_count</code></strong> :&ensp;<code>int</code></dt>
<dd>How many states are in the Model.</dd>
<dt><strong><code>state_grid</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>The state indices organized as a 2D grid. (Used for plotting purposes)</dd>
<dt><strong><code>actions</code></strong> :&ensp;<code>np.ndarry</code></dt>
<dd>A 1D array of action indices. Used to loop over actions.</dd>
<dt><strong><code>action_labels</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>A list of action labels. (To be mainly used for plotting)</dd>
<dt><strong><code>action_count</code></strong> :&ensp;<code>int</code></dt>
<dd>How many action are in the Model.</dd>
<dt><strong><code>observations</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 1D array of observation indices. Used to loop over obervations.</dd>
<dt><strong><code>observation_labels</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>A list of observation labels. (To be mainly used for plotting)</dd>
<dt><strong><code>observation_count</code></strong> :&ensp;<code>int</code></dt>
<dd>How many observations can be made in the Model.</dd>
<dt><strong><code>transition_table</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 3D matrix of the transition probabilities.
Can be None in the case a transition function is provided instead.
Note: When possible, use reachable states and reachable probabilities instead.</dd>
<dt><strong><code>transition_function</code></strong> :&ensp;<code>function</code></dt>
<dd>A callable function taking 3 arguments: s, a, s_p; and returning a float between 0.0 and 1.0.
Can be None in the case a transition table is provided instead.
Note: When possible, use reachable states and reachable probabilities instead.</dd>
<dt><strong><code>observation_table</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 3D matrix of shape S x A x O representing the probabilies of obsevating o when taking action a and leading to state s_p.</dd>
<dt><strong><code>reachable_states</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 3D array of the shape S x A x R, where R is max amount to states that can be reached from any state-action pair.</dd>
<dt><strong><code>reachable_probabilities</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 3D array of the same shape as reachable_states, the array represent the probability of reaching the state pointed by the reachable_states matrix.</dd>
<dt><strong><code>reachable_state_count</code></strong> :&ensp;<code>int</code></dt>
<dd>The maximum of states that can be reached from any state-action combination.</dd>
<dt><strong><code>reachable_transitional_observation_table</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 4D array of shape S x A x O x R, representing the probabiliies of landing if each reachable state r, while observing o after having taken action a from state s.
Mainly used to speedup repeated operations in solver.</dd>
<dt><strong><code>immediate_reward_table</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 3D matrix of shape S x A x S x O of the reward that will received when taking action a, in state s, landing in state s_p, and observing o.
Can be None in the case an immediate rewards function is provided instead.</dd>
<dt><strong><code>immediate_reward_function</code></strong> :&ensp;<code>function</code></dt>
<dd>A callable function taking 4 argments: s, a, s_p, o and returning the immediate reward the agent will receive.
Can be None in the case an immediate rewards function is provided instead.</dd>
<dt><strong><code>expected_reward_table</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 2D array of shape S x A. It represents the rewards that is expected to be received when taking action a from state s.
It is made by taking the weighted average of immediate rewards with the transitions and the observation probabilities.</dd>
<dt><strong><code>start_probabilities</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>A 1D array of length |S| containing the probility distribution of the agent starting in each state.</dd>
<dt><strong><code>rewards_are_probabilisitic</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the immediate rewards are probabilitic, ie: returning a 0 or 1 based on the reward that is considered to be a probability.</dd>
<dt><strong><code>end_states</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>A list of states that, when reached, terminate a simulation.</dd>
<dt><strong><code>end_actions</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>A list of actions that, when taken, terminate a simulation.</dd>
<dt><strong><code>is_on_gpu</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the numpy array of the model are stored on the gpu or not.</dd>
<dt><strong><code>gpu_model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>An equivalent model with the np.ndarray objects on GPU. (If already on GPU, returns self)</dd>
<dt><strong><code>cpu_model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>An equivalent model with the np.ndarray objects on CPU. (If already on CPU, returns self)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Model(MDP_Model):
    &#39;&#39;&#39;
    POMDP Model class. Partially Observable Markov Decision Process Model.

    ...

    Parameters
    ----------
    states : int or list[str] or list[list[str]]
        A list of state labels or an amount of states to be used. Also allows to provide a matrix of states to define a grid model.
    actions : int or list
        A list of action labels or an amount of actions to be used.
    observations : int or list
        A list of observation labels or an amount of observations to be used
    transitions : array-like or function, optional
        The transitions between states, an array can be provided and has to be |S| x |A| x |S| or a function can be provided. 
        If a function is provided, it has be able to deal with np.array arguments.
        If none is provided, it will be randomly generated.
    reachable_states : array-like, optional
        A list of states that can be reached from each state and actions. It must be a matrix of size |S| x |A| x |R| where |R| is the max amount of states reachable from any given state and action pair.
        It is optional but useful for speedup purposes.
    rewards : array-like or function, optional
        The reward matrix, has to be |S| x |A| x |S|.
        A function can also be provided here but it has to be able to deal with np.array arguments.
        If provided, it will be use in combination with the transition matrix to fill to expected rewards.
    observation_table : array-like or function, optional
        The observation matrix, has to be |S| x |A| x |O|. If none is provided, it will be randomly generated.
    rewards_are_probabilistic: bool, default=False
        Whether the rewards provided are probabilistic or pure rewards. If probabilist 0 or 1 will be the reward with a certain probability.
    state_grid : array-like, optional
        If provided, the model will be converted to a grid model.
    start_probabilities : list, optional
        The distribution of chances to start in each state. If not provided, there will be an uniform chance for each state. It is also used to represent a belief of complete uncertainty.
    end_states : list, optional
        Entering either state in the list during a simulation will end the simulation.
    end_action : list, optional
        Playing action of the list during a simulation will end the simulation.

    Attributes
    ----------
    states : np.ndarray
        A 1D array of states indices. Used to loop over states.
    state_labels : list[str]
        A list of state labels. (To be mainly used for plotting)
    state_count : int
        How many states are in the Model.
    state_grid : np.ndarray
        The state indices organized as a 2D grid. (Used for plotting purposes)
    actions : np.ndarry
        A 1D array of action indices. Used to loop over actions.
    action_labels : list[str]
        A list of action labels. (To be mainly used for plotting)
    action_count : int
        How many action are in the Model.
    observations : np.ndarray
        A 1D array of observation indices. Used to loop over obervations.
    observation_labels : list[str]
        A list of observation labels. (To be mainly used for plotting)
    observation_count : int
        How many observations can be made in the Model.
    transition_table : np.ndarray
        A 3D matrix of the transition probabilities.
        Can be None in the case a transition function is provided instead.
        Note: When possible, use reachable states and reachable probabilities instead.
    transition_function : function
        A callable function taking 3 arguments: s, a, s_p; and returning a float between 0.0 and 1.0.
        Can be None in the case a transition table is provided instead.
        Note: When possible, use reachable states and reachable probabilities instead.
    observation_table : np.ndarray
        A 3D matrix of shape S x A x O representing the probabilies of obsevating o when taking action a and leading to state s_p.
    reachable_states : np.ndarray
        A 3D array of the shape S x A x R, where R is max amount to states that can be reached from any state-action pair.
    reachable_probabilities : np.ndarray
        A 3D array of the same shape as reachable_states, the array represent the probability of reaching the state pointed by the reachable_states matrix.
    reachable_state_count : int
        The maximum of states that can be reached from any state-action combination.
    reachable_transitional_observation_table : np.ndarray
        A 4D array of shape S x A x O x R, representing the probabiliies of landing if each reachable state r, while observing o after having taken action a from state s.
        Mainly used to speedup repeated operations in solver.
    immediate_reward_table : np.ndarray
        A 3D matrix of shape S x A x S x O of the reward that will received when taking action a, in state s, landing in state s_p, and observing o.
        Can be None in the case an immediate rewards function is provided instead.
    immediate_reward_function : function
        A callable function taking 4 argments: s, a, s_p, o and returning the immediate reward the agent will receive.
        Can be None in the case an immediate rewards function is provided instead.
    expected_reward_table : np.ndarray
        A 2D array of shape S x A. It represents the rewards that is expected to be received when taking action a from state s.
        It is made by taking the weighted average of immediate rewards with the transitions and the observation probabilities.
    start_probabilities : np.ndarray
        A 1D array of length |S| containing the probility distribution of the agent starting in each state.
    rewards_are_probabilisitic : bool
        Whether the immediate rewards are probabilitic, ie: returning a 0 or 1 based on the reward that is considered to be a probability.
    end_states : list[int]
        A list of states that, when reached, terminate a simulation.
    end_actions : list[int]
        A list of actions that, when taken, terminate a simulation.
    is_on_gpu : bool
        Whether the numpy array of the model are stored on the gpu or not.
    gpu_model : mdp.Model
        An equivalent model with the np.ndarray objects on GPU. (If already on GPU, returns self)
    cpu_model : mdp.Model
        An equivalent model with the np.ndarray objects on CPU. (If already on CPU, returns self)
    &#39;&#39;&#39;
    def __init__(self,
                 states:Union[int, list[str], list[list[str]]],
                 actions:Union[int, list],
                 observations:Union[int, list],
                 transitions=None,
                 reachable_states=None,
                 rewards=None,
                 observation_table=None,
                 rewards_are_probabilistic:bool=False,
                 state_grid=None,
                 start_probabilities:Union[list,None]=None,
                 end_states:list[int]=[],
                 end_actions:list[int]=[]
                 ):
        
        super().__init__(states=states,
                         actions=actions,
                         transitions=transitions,
                         reachable_states=reachable_states,
                         rewards=-1, # Defined here lower since immediate reward table has different shape for MDP is different than for POMDP
                         rewards_are_probabilistic=rewards_are_probabilistic,
                         state_grid=state_grid,
                         start_probabilities=start_probabilities,
                         end_states=end_states,
                         end_actions=end_actions)

        print()
        log(&#39;POMDP particular parameters:&#39;)

        def end_reward_function(s, a, sn, o):
            return (np.isin(sn, self.end_states) | np.isin(a, self.end_actions)).astype(int)

        # ------------------------- Observations -------------------------
        if isinstance(observations, int):
            self.observation_labels = [f&#39;o_{i}&#39; for i in range(observations)]
        else:
            self.observation_labels = observations
        self.observation_count = len(self.observation_labels)
        self.observations = np.arange(self.observation_count)

        if observation_table is None:
            # If no observation matrix given, generate random one
            random_probs = np.random.rand(self.state_count, self.action_count, self.observation_count)
            # Normalization to have s_p probabilies summing to 1
            self.observation_table = random_probs / np.sum(random_probs, axis=2, keepdims=True)
        else:
            self.observation_table = np.array(observation_table)
            o_shape = self.observation_table.shape
            exp_shape = (self.state_count, self.action_count, self.observation_count)
            assert o_shape == exp_shape, f&#34;Observations table doesnt have the right shape, it should be SxAxO (expected: {exp_shape}, received: {o_shape}).&#34;

        log(f&#39;- {self.observation_count} observations&#39;)

        # ------------------------- Reachable transitional observation probabilities -------------------------
        log(&#39;- Starting of transitional observations for reachable states table&#39;)
        start_ts = datetime.now()

        reachable_observations = self.observation_table[self.reachable_states[:,:,None,:], self.actions[None,:,None,None], self.observations[None,None,:,None]] # SAOR
        self.reachable_transitional_observation_table = np.einsum(&#39;sar,saor-&gt;saor&#39;, self.reachable_probabilities, reachable_observations)
        
        duration = (datetime.now() - start_ts).total_seconds()
        log(f&#39;    &gt; Done in {duration:.3f}s&#39;)

        # ------------------------- Rewards -------------------------
        self.immediate_reward_table = None
        self.immediate_reward_function = None
        
        if rewards is None:
            if (len(self.end_states) &gt; 0) or (len(self.end_actions) &gt; 0):
                log(&#39;- [Warning] Rewards are not define but end states/actions are, reaching an end state or doing an end action will give a reward of 1.&#39;)
                self.immediate_reward_function = self._end_reward_function
            else:
                # If no reward matrix given, generate random one
                self.immediate_reward_table = np.random.rand(self.state_count, self.action_count, self.state_count, self.observation_count)
        elif callable(rewards):
            # Rewards is a function
            log(&#39;- [Warning] The rewards are provided as a function, if the model is saved, the rewards will need to be defined before loading model.&#39;)
            log(&#39;    &gt; Alternative: Setting end states/actions and leaving the rewards can be done to make the end states/action giving a reward of 1 by default.&#39;)
            self.immediate_reward_function = rewards
            assert len(signature(rewards).parameters) == 4, &#34;Reward function should accept 4 parameters: s, a, sn, o...&#34;
        else:
            # Array like
            self.immediate_reward_table = np.array(rewards)
            r_shape = self.immediate_reward_table.shape
            exp_shape = (self.state_count, self.action_count, self.state_count, self.observation_count)
            assert r_shape == exp_shape, f&#34;Rewards table doesnt have the right shape, it should be SxAxSxO (expected: {exp_shape}, received {r_shape})&#34;
        
        # ------------------------- Expected rewards -------------------------
        log(&#39;- Starting generation of expected rewards table&#39;)
        start_ts = datetime.now()

        reachable_rewards = None
        if self.immediate_reward_table is not None:
            reachable_rewards = rewards[self.states[:,None,None,None], self.actions[None,:,None,None], self.reachable_states[:,:,:,None], self.observations[None,None,None,:]]
        else:
            def reach_reward_func(s,a,ri,o):
                s = s.astype(int)
                a = a.astype(int)
                ri = ri.astype(int)
                o = o.astype(int)
                return self.immediate_reward_function(s,a,self.reachable_states[s,a,ri],o)
            
            reachable_rewards = np.fromfunction(reach_reward_func, (*self.reachable_states.shape, self.observation_count))

        self._min_reward = float(np.min(reachable_rewards))
        self._max_reward = float(np.max(reachable_rewards))

        self.expected_rewards_table = np.einsum(&#39;saor,saro-&gt;sa&#39;, self.reachable_transitional_observation_table, reachable_rewards)

        duration = (datetime.now() - start_ts).total_seconds()
        log(f&#39;    &gt; Done in {duration:.3f}s&#39;)


    def _end_reward_function(self, s, a, sn, o):
        return (np.isin(sn, self.end_states) | np.isin(a, self.end_actions)).astype(int)
    

    def reward(self, s:int, a:int, s_p:int, o:int) -&gt; Union[int,float]:
        &#39;&#39;&#39;
        Returns the rewards of playing action a when in state s and landing in state s_p.
        If the rewards are probabilistic, it will return 0 or 1.

        Parameters
        ----------
        s : int
            The current state.
        a : int
            The action taking in state s.
        s_p : int
            The state landing in after taking action a in state s
        o : int
            The observation that is done after having played action a in state s and landing in s_p

        Returns
        -------
        reward : int or float
            The reward received.
        &#39;&#39;&#39;
        reward = float(self.immediate_reward_table[s,a,s_p,o] if self.immediate_reward_table is not None else self.immediate_reward_function(s,a,s_p,o))
        if self.rewards_are_probabilistic:
            rnd = random.random()
            return 1 if rnd &lt; reward else 0
        else:
            return reward
    

    def observe(self, s_p:int, a:int) -&gt; int:
        &#39;&#39;&#39;
        Returns a random observation knowing action a is taken from state s, it is weighted by the observation probabilities.

        Parameters
        ----------
        s_p : int
            The state landed on after having done action a.
        a : int
            The action to take.

        Returns
        -------
        o : int
            A random observation.
        &#39;&#39;&#39;
        xp = cp if self.is_on_gpu else np
        o = int(xp.random.choice(a=self.observations, size=1, p=self.observation_table[s_p,a])[0])
        return o</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="src.mdp.Model" href="mdp.html#src.mdp.Model">Model</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.pomdp.Model.observe"><code class="name flex">
<span>def <span class="ident">observe</span></span>(<span>self, s_p: int, a: int) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a random observation knowing action a is taken from state s, it is weighted by the observation probabilities.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>s_p</code></strong> :&ensp;<code>int</code></dt>
<dd>The state landed on after having done action a.</dd>
<dt><strong><code>a</code></strong> :&ensp;<code>int</code></dt>
<dd>The action to take.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>o</code></strong> :&ensp;<code>int</code></dt>
<dd>A random observation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def observe(self, s_p:int, a:int) -&gt; int:
    &#39;&#39;&#39;
    Returns a random observation knowing action a is taken from state s, it is weighted by the observation probabilities.

    Parameters
    ----------
    s_p : int
        The state landed on after having done action a.
    a : int
        The action to take.

    Returns
    -------
    o : int
        A random observation.
    &#39;&#39;&#39;
    xp = cp if self.is_on_gpu else np
    o = int(xp.random.choice(a=self.observations, size=1, p=self.observation_table[s_p,a])[0])
    return o</code></pre>
</details>
</dd>
<dt id="src.pomdp.Model.reward"><code class="name flex">
<span>def <span class="ident">reward</span></span>(<span>self, s: int, a: int, s_p: int, o: int) ‑> Union[int, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the rewards of playing action a when in state s and landing in state s_p.
If the rewards are probabilistic, it will return 0 or 1.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>s</code></strong> :&ensp;<code>int</code></dt>
<dd>The current state.</dd>
<dt><strong><code>a</code></strong> :&ensp;<code>int</code></dt>
<dd>The action taking in state s.</dd>
<dt><strong><code>s_p</code></strong> :&ensp;<code>int</code></dt>
<dd>The state landing in after taking action a in state s</dd>
<dt><strong><code>o</code></strong> :&ensp;<code>int</code></dt>
<dd>The observation that is done after having played action a in state s and landing in s_p</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>reward</code></strong> :&ensp;<code>int</code> or <code>float</code></dt>
<dd>The reward received.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reward(self, s:int, a:int, s_p:int, o:int) -&gt; Union[int,float]:
    &#39;&#39;&#39;
    Returns the rewards of playing action a when in state s and landing in state s_p.
    If the rewards are probabilistic, it will return 0 or 1.

    Parameters
    ----------
    s : int
        The current state.
    a : int
        The action taking in state s.
    s_p : int
        The state landing in after taking action a in state s
    o : int
        The observation that is done after having played action a in state s and landing in s_p

    Returns
    -------
    reward : int or float
        The reward received.
    &#39;&#39;&#39;
    reward = float(self.immediate_reward_table[s,a,s_p,o] if self.immediate_reward_table is not None else self.immediate_reward_function(s,a,s_p,o))
    if self.rewards_are_probabilistic:
        rnd = random.random()
        return 1 if rnd &lt; reward else 0
    else:
        return reward</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="src.mdp.Model" href="mdp.html#src.mdp.Model">Model</a></b></code>:
<ul class="hlist">
<li><code><a title="src.mdp.Model.cpu_model" href="mdp.html#src.mdp.Model.cpu_model">cpu_model</a></code></li>
<li><code><a title="src.mdp.Model.get_coords" href="mdp.html#src.mdp.Model.get_coords">get_coords</a></code></li>
<li><code><a title="src.mdp.Model.gpu_model" href="mdp.html#src.mdp.Model.gpu_model">gpu_model</a></code></li>
<li><code><a title="src.mdp.Model.load_from_file" href="mdp.html#src.mdp.Model.load_from_file">load_from_file</a></code></li>
<li><code><a title="src.mdp.Model.save" href="mdp.html#src.mdp.Model.save">save</a></code></li>
<li><code><a title="src.mdp.Model.transition" href="mdp.html#src.mdp.Model.transition">transition</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="src.pomdp.PBVI_Solver"><code class="flex name class">
<span>class <span class="ident">PBVI_Solver</span></span>
<span>(</span><span>gamma: float = 0.99, eps: float = 0.001, expand_function: str = 'ssea', **expand_function_params)</span>
</code></dt>
<dd>
<div class="desc"><p>The Point-Based Value Iteration solver for POMDP Models. It works in two steps, first the backup step that updates the alpha vector set that approximates the value function.
Then, the expand function that expands the belief set.</p>
<p>The various expand functions and the backup function have been implemented based on the pseudocodes found the paper from J. Pineau, G. Gordon, and S. Thrun, 'Point-based approximations for fast POMDP solving'</p>
<p>&hellip;
Parameters</p>
<hr>
<dl>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code>, default=<code>0.99</code></dt>
<dd>The learning rate, used to control how fast the value function will change after the each iterations.</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, default=<code>0.001</code></dt>
<dd>The treshold for convergence. If the max change between value function is lower that eps, the algorithm is considered to have converged.</dd>
<dt><strong><code>expand_function</code></strong> :&ensp;<code>str</code>, default=<code>'ssea'</code></dt>
<dd>The type of expand strategy to use to expand the belief set.</dd>
<dt><strong><code>expand_function_params</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Other required parameters to be sent to the expand function.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>expand_function</code></strong> :&ensp;<code>str</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>expand_function_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PBVI_Solver(Solver):
    &#39;&#39;&#39;
    The Point-Based Value Iteration solver for POMDP Models. It works in two steps, first the backup step that updates the alpha vector set that approximates the value function.
    Then, the expand function that expands the belief set.

    The various expand functions and the backup function have been implemented based on the pseudocodes found the paper from J. Pineau, G. Gordon, and S. Thrun, &#39;Point-based approximations for fast POMDP solving&#39;

    ...
    Parameters
    ----------
    gamma : float, default=0.99
        The learning rate, used to control how fast the value function will change after the each iterations.
    eps : float, default=0.001
        The treshold for convergence. If the max change between value function is lower that eps, the algorithm is considered to have converged.
    expand_function : str, default=&#39;ssea&#39;
        The type of expand strategy to use to expand the belief set.
    expand_function_params : dict, optional
        Other required parameters to be sent to the expand function.

    Attributes
    ----------
    gamma : float
    eps : float
    expand_function : str
    expand_function_params : dict
    &#39;&#39;&#39;
    def __init__(self,
                 gamma:float=0.99,
                 eps:float=0.001,
                 expand_function:str=&#39;ssea&#39;,
                 **expand_function_params):
        self.gamma = gamma
        self.eps = eps
        self.expand_function = expand_function
        self.expand_function_params = expand_function_params


    def test_n_simulations(self, model:Model, value_function:ValueFunction, n:int=1000, horizon:int=300, print_progress:bool=False):
        &#39;&#39;&#39;
        Function that tests a value function with n simulations. It returns the start states, the amount of steps in which the simulation reached an end state, the rewards received and the discounted rewards received.

        Parameters
        ----------
        model : pomdp.Model
            The model on which to run the simulations.
        value_function : ValueFunction
            The value function that will be evaluated.
        n : int, default=1000
            The amount of simulations to run.
        horizon : int, default=300
            The maximum amount of steps the simulation can run for.
        print_progress : bool, default=False
            Whether to display a progress bar of how many simulation steps have been run so far. 
        &#39;&#39;&#39;
        # GPU support
        xp = np if not value_function.is_on_gpu else cp
        model = model.cpu_model if not value_function.is_on_gpu else model.gpu_model

        # Genetion of an array of n beliefs
        initial_beliefs = xp.repeat(Belief(model).values[None,:], n, axis=0)

        # Generating n initial positions
        start_states = xp.random.choice(model.states, size=n, p=model.start_probabilities)
        
        # Belief and state arrays
        beliefs = initial_beliefs
        new_beliefs = None
        states = start_states
        next_states = None

        # Tracking what simulations are done
        sim_is_done = xp.zeros(n, dtype=bool)
        done_at_step = xp.full(n, -1)

        # Speedup item
        simulations = xp.arange(n)
        flatten_offset = (simulations[:,None] * model.state_count)
        flat_shape = (n, (model.state_count * model.reachable_state_count))

        # 2D bincount for belief set update
        def bincount2D_vectorized(a, w):    
            a_offs = a + flatten_offset
            return xp.bincount(a_offs.ravel(), weights=w.ravel(), minlength=a.shape[0]*model.state_count).reshape(-1,model.state_count)

        # Results
        discount = self.gamma
        rewards = []
        discounted_rewards = []
        
        iterator = trange(horizon) if print_progress else range(horizon)
        for i in iterator:
            # Retrieving the top vectors according to the value function
            best_vectors = xp.argmax(xp.matmul(beliefs, value_function.alpha_vector_array.T), axis=1)

            # Retrieving the actions associated with the vectors chosen
            best_actions = value_function.actions[best_vectors]

            # Get each reachable next states for each action
            reachable_state_per_actions = model.reachable_states[:, best_actions, :]

            # Gathering new states based on the transition function and the chosen actions
            next_state_potentials = reachable_state_per_actions[states, simulations]
            if model.reachable_state_count == 1:
                next_states = next_state_potentials[:,0]
            else:
                potential_probabilities = model.reachable_probabilities[states[:,None], best_actions[:,None]][:,0,:]
                chosen_indices = xp.apply_along_axis(lambda x: xp.random.choice(len(x), size=1, p=x), axis=1, arr=potential_probabilities)
                next_states = next_state_potentials[chosen_indices][:,0,0]

            # Making observations based on the states landed in and the action that was taken
            observation_probabilities = model.observation_table[next_states[:,None], best_actions[:,None]][:,0,:]
            observations = xp.sum(xp.random.random(n)[:,None] &gt; xp.cumsum(observation_probabilities[:,:-1], axis=1), axis=1)

            # Belief set update
            reachable_probabilities = (model.reachable_transitional_observation_table[:,best_actions,observations,:] * beliefs.T[:,:,None])
            new_beliefs = bincount2D_vectorized(a=reachable_state_per_actions.swapaxes(0,1).reshape(flat_shape),
                                                w=reachable_probabilities.swapaxes(0,1).reshape(flat_shape))

            new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]

            # Rewards computation
            step_rewards = xp.array([model.immediate_reward_function(s,a,s_p,o) for s,a,s_p,o in zip(states, best_actions, next_states, observations)])
            rewards.append(xp.where(~sim_is_done, step_rewards, 0))
            discounted_rewards.append(xp.where(~sim_is_done, step_rewards * discount, 0))

            # Checking for done condition
            are_done = xp.isin(next_states, xp.array(model.end_states))
            done_at_step[sim_is_done ^ are_done] = i+1
            sim_is_done |= are_done

            # Update iterator postfix
            if print_progress:
                iterator.set_postfix({&#39;done&#39;: xp.sum(sim_is_done)})

            # Replacing old with new
            states = next_states
            beliefs = new_beliefs
            discount *= self.gamma

            # Early stopping
            if xp.all(sim_is_done):
                break

        return start_states, done_at_step, rewards, discounted_rewards


    def backup(self,
               model:Model,
               belief_set:BeliefSet,
               value_function:ValueFunction,
               append:bool=False,
               belief_dominance_prune:bool=True
               ) -&gt; ValueFunction:
        &#39;&#39;&#39;
        This function has purpose to update the set of alpha vectors. It does so in 3 steps:
        1. It creates projections from each alpha vector for each possible action and each possible observation
        2. It collapses this set of generated alpha vectors by taking the weighted sum of the alpha vectors weighted by the observation probability and this for each action and for each belief.
        3. Then it further collapses the set to take the best alpha vector and action per belief
        In the end we have a set of alpha vectors as large as the amount of beliefs.

        The alpha vectors are also pruned to avoid duplicates and remove dominated ones.

        Parameters
        ----------
        model : pomdp.Model
            The model on which to run the backup method on.
        belief_set : BeliefSet
            The belief set to use to generate the new alpha vectors with.
        value_function : ValueFunction
            The alpha vectors to generate the new set from.
        append : bool, default=False
            Whether to append the new alpha vectors generated to the old alpha vectors before pruning.
        belief_dominance_prune : bool, default=True
            Whether, before returning the new value function, checks what alpha vectors have a supperior value, if so it adds it.
            
        Returns
        -------
        new_alpha_set : ValueFunction
            A list of updated alpha vectors.
        &#39;&#39;&#39;
        # Get numpy corresponding to the arrays
        xp = np if not gpu_support else cp.get_array_module(value_function.alpha_vector_array)

        # Step 1
        vector_array = value_function.alpha_vector_array
        vectors_array_reachable_states = vector_array[xp.arange(vector_array.shape[0])[:,None,None,None], model.reachable_states[None,:,:,:]]
        
        gamma_a_o_t = self.gamma * xp.einsum(&#39;saor,vsar-&gt;aovs&#39;, model.reachable_transitional_observation_table, vectors_array_reachable_states)

        # Step 2
        belief_array = belief_set.belief_array # bs
        best_alpha_ind = xp.argmax(xp.tensordot(belief_array, gamma_a_o_t, (1,3)), axis=3) # argmax(bs,aovs-&gt;baov) -&gt; bao

        best_alphas_per_o = gamma_a_o_t[model.actions[None,:,None,None], model.observations[None,None,:,None], best_alpha_ind[:,:,:,None], model.states[None,None,None,:]] # baos

        alpha_a = model.expected_rewards_table.T + xp.sum(best_alphas_per_o, axis=2) # as + bas

        # Step 3
        best_actions = xp.argmax(xp.einsum(&#39;bas,bs-&gt;ba&#39;, alpha_a, belief_array), axis=1)
        alpha_vectors = xp.take_along_axis(alpha_a, best_actions[:,None,None],axis=1)[:,0,:]

        # Belief domination
        if belief_dominance_prune:
            best_value_per_belief = xp.sum((belief_array * alpha_vectors), axis=1)
            old_best_value_per_belief = xp.max(xp.matmul(belief_array, vector_array.T), axis=1)
            dominating_vectors = best_value_per_belief &gt; old_best_value_per_belief

            best_actions = best_actions[dominating_vectors]
            alpha_vectors = alpha_vectors[dominating_vectors]

        # Creation of value function
        new_value_function = ValueFunction(model, alpha_vectors, best_actions)

        # Union with previous value function
        if append:
            new_value_function.extend(value_function)
        
        return new_value_function
    

    def expand_ra(self, model:Model, belief_set:BeliefSet, max_generation:int=10) -&gt; BeliefSet:
        &#39;&#39;&#39;
        This expansion technique relies only randomness and will generate at most &#39;max_generation&#39; beliefs.

        Parameters
        model : pomdp.Model
            The POMDP model on which to expand the belief set on.
        belief_set : BeliefSet
            List of beliefs to expand on.
        max_generation : int, default=10
            The max amount of beliefs that can be added to the belief set at once.
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

        # How many new beliefs to add
        generation_count = min(belief_set.belief_array.shape[0], max_generation)

        # Generation of the new beliefs at random
        new_beliefs = xp.random.random((generation_count, model.state_count))
        new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]

        return BeliefSet(model, new_beliefs)

    
    def expand_ssra(self, model:Model, belief_set:BeliefSet, max_generation:int=10) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Stochastic Simulation with Random Action.
        Simulates running a single-step forward from the beliefs in the &#34;belief_set&#34;.
        The step forward is taking assuming we are in a random state (weighted by the belief) and taking a random action leading to a state s_p and a observation o.
        From this action a and observation o we can update our belief.

        Parameters
        ----------
        model : pomdp.Model
            The POMDP model on which to expand the belief set on.
        belief_set : BeliefSet
            List of beliefs to expand on.
        max_generation : int, default=10
            The max amount of beliefs that can be added to the belief set at once.

        Returns
        -------
        belief_set_new : BeliefSet
            Union of the belief_set and the expansions of the beliefs in the belief_set
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

        old_shape = belief_set.belief_array.shape
        to_generate = min(max_generation, old_shape[0])

        new_belief_array = xp.empty((to_generate, old_shape[1]))

        # Random previous beliefs
        rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)

        for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):
            b = Belief(model, belief_vector)
            s = b.random_state()
            a = random.choice(model.actions)
            s_p = model.transition(s, a)
            o = model.observe(s_p, a)
            b_new = b.update(a, o)
            
            new_belief_array[i] = b_new.values
            
        return BeliefSet(model, new_belief_array)
    

    def expand_ssga(self, model:Model, belief_set:BeliefSet, value_function:ValueFunction, epsilon:float=0.1, max_generation:int=10) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Stochastic Simulation with Greedy Action.
        Simulates running a single-step forward from the beliefs in the &#34;belief_set&#34;.
        The step forward is taking assuming we are in a random state s (weighted by the belief),
         then taking the best action a based on the belief with probability &#39;epsilon&#39;.
        These lead to a new state s_p and a observation o.
        From this action a and observation o we can update our belief. 

        Parameters
        ----------
        model : pomdp.Model
            The POMDP model on which to expand the belief set on.
        belief_set : BeliefSet
            List of beliefs to expand on.
        value_function : ValueFunction
            Used to find the best action knowing the belief.
        eps : float
            Parameter tuning how often we take a greedy approach and how often we move randomly.
        max_generation : int, default=10
            The max amount of beliefs that can be added to the belief set at once.

        Returns
        -------
        belief_set_new : BeliefSet
            Union of the belief_set and the expansions of the beliefs in the belief_set
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

        old_shape = belief_set.belief_array.shape
        to_generate = min(max_generation, old_shape[0])

        new_belief_array = xp.empty((to_generate, old_shape[1]))

        # Random previous beliefs
        rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)

        for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):
            b = Belief(model, belief_vector)
            s = b.random_state()
            
            if random.random() &lt; epsilon:
                a = random.choice(model.actions)
            else:
                best_alpha_index = xp.argmax(xp.dot(value_function.alpha_vector_array, b.values))
                a = value_function.actions[best_alpha_index]
            
            s_p = model.transition(s, a)
            o = model.observe(s_p, a)
            b_new = b.update(a, o)
            
            new_belief_array[i] = b_new.values
            
        return BeliefSet(model, new_belief_array)
    

    def expand_ssea(self, model:Model, belief_set:BeliefSet, max_generation:int=10) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Stochastic Simulation with Exploratory Action.
        Simulates running steps forward for each possible action knowing we are a state s, chosen randomly with according to the belief probability.
        These lead to a new state s_p and a observation o for each action.
        From all these and observation o we can generate updated beliefs. 
        Then it takes the belief that is furthest away from other beliefs, meaning it explores the most the belief space.

        Parameters
        ----------
        model : pomdp.Model
            The POMDP model on which to expand the belief set on.
        belief_set : BeliefSet
            List of beliefs to expand on.
        max_generation : int, default=10
            The max amount of beliefs that can be added to the belief set at once.

        Returns
        -------
        belief_set_new : BeliefSet
            Union of the belief_set and the expansions of the beliefs in the belief_set
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

        old_shape = belief_set.belief_array.shape
        to_generate = min(max_generation, old_shape[0])

        # Generation of successors
        successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])
        
        # Compute the distances between each pair and of successor are source beliefs
        diff = (belief_set.belief_array[:, None,None,None, :] - successor_beliefs)
        dist = xp.sqrt(xp.einsum(&#39;bnaos,bnaos-&gt;bnao&#39;, diff, diff))

        # Taking the min distance for each belief
        belief_min_dists = xp.min(dist,axis=0)

        # Taking the max distanced successors
        b_star, a_star, o_star = xp.unravel_index(xp.argsort(belief_min_dists, axis=None)[::-1][:to_generate], successor_beliefs.shape[:-1])

        # Selecting successor beliefs
        new_belief_array = successor_beliefs[b_star[:,None], a_star[:,None], o_star[:,None], model.states[None,:]]

        return BeliefSet(model, new_belief_array)
    

    def expand_ger(self, model:Model, belief_set:BeliefSet, value_function:ValueFunction, max_generation:int=10) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Greedy Error Reduction.
        It attempts to choose the believes that will maximize the improvement of the value function by minimizing the error.
        The error is computed by the sum of the change between two beliefs and their two corresponding alpha vectors.

        Parameters
        ----------
        model : pomdp.Model
            The POMDP model on which to expand the belief set on.
        belief_set : BeliefSet
            List of beliefs to expand on.
        value_function : ValueFunction
            Used to find the best action knowing the belief.
        max_generation : int, default=10
            The max amount of beliefs that can be added to the belief set at once.

        Returns
        -------
        belief_set_new : BeliefSet
            Union of the belief_set and the expansions of the beliefs in the belief_set
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

        old_shape = belief_set.belief_array.shape
        to_generate = min(max_generation, old_shape[0])

        new_belief_array = xp.empty((old_shape[0] + to_generate, old_shape[1]))
        new_belief_array[:old_shape[0]] = belief_set.belief_array

        # Finding the min and max rewards for computation of the epsilon
        r_min = model._min_reward / (1 - self.gamma)
        r_max = model._max_reward / (1 - self.gamma)

        # Generation of all potential successor beliefs
        successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])
        
        # Finding the alphas associated with each previous beliefs
        best_alpha = xp.argmax(xp.dot(belief_set.belief_array, value_function.alpha_vector_array.T), axis = 1)
        b_alphas = value_function.alpha_vector_array[best_alpha]

        # Difference between beliefs and their successors
        b_diffs = successor_beliefs - belief_set.belief_array[:,None,None,:]

        # Computing a &#39;next&#39; alpha vector made of the max and min
        alphas_p = xp.where(b_diffs &gt;= 0, r_max, r_min)

        # Difference between alpha vectors and their successors alpha vector
        alphas_diffs = alphas_p - b_alphas[:,None,None,:]

        # Computing epsilon for all successor beliefs
        eps = xp.einsum(&#39;baos,baos-&gt;bao&#39;, alphas_diffs, b_diffs)

        # Computing the probability of the b and doing action a and receiving observation o
        bao_probs = xp.einsum(&#39;bs,saor-&gt;bao&#39;, belief_set.belief_array, model.reachable_transitional_observation_table)

        # Taking the sumproduct of the probs with the epsilons
        res = xp.einsum(&#39;bao,bao-&gt;ba&#39;, bao_probs, eps)

        # Picking the correct amount of initial beliefs and ideal actions
        b_stars, a_stars = xp.unravel_index(xp.argsort(res, axis=None)[::-1][:to_generate], res.shape)

        # And picking the ideal observations
        o_star = xp.argmax(bao_probs[b_stars[:,None], a_stars[:,None], model.observations[None,:]] * eps[b_stars[:,None], a_stars[:,None], model.observations[None,:]], axis=1)

        # Selecting the successor beliefs
        new_belief_array = successor_beliefs[b_stars[:,None], a_stars[:,None], o_star[:,None], model.states[None,:]]

        return BeliefSet(model, new_belief_array)


    def expand_hsvi(self,
                    model:Model,
                    b:Belief,
                    value_function:ValueFunction,
                    upper_bound_belief_value_map:BeliefValueMapping,
                    conv_term:Union[float,None]=None,
                    max_generation:int=10
                    ) -&gt; BeliefSet:
        &#39;&#39;&#39;
        The expand function of the  Heruistic Search Value Iteration (HSVI) technique.
        It is a redursive function attempting to minimize the bound between the upper and lower estimations of the value function.

        It is developped by Smith T. and Simmons R. and described in the paper &#34;Heuristic Search Value Iteration for POMDPs&#34;.
        
        Parameters
        ----------
        model : pomdp.Model
            The model in which the exploration process will happen.
        b : Belief
            A belief to be added to the returned belief sequence and updated for the next step of the recursion.
        value_function : ValueFunction
            The lower bound of the value function.
        upper_bound_belief_value_map : BeliefValueMapping
            The upper bound of the value function.
            Initially it is define with the mdp policy of the model (run: &#34;BeliefValueMapping(model, mdp_policy)&#34;).
            It is then refined through the expansion process by adding newly found belief and value pairs.
        max_generation : int, default=10
            The maximum recursion depth that can be reached before the generated belief sequence is returned.
        
        Returns
        -------
        belief_set : BeliefSet
            A new sequence of beliefs.
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(b.values)

        if conv_term is None:
            conv_term = self.eps

        # Update convergence term
        conv_term /= self.gamma

        # Find best a based on upper bound v
        max_qv = -xp.inf
        best_a = -1
        for a in model.actions:
            b_probs = xp.einsum(&#39;sor,s-&gt;o&#39;, model.reachable_transitional_observation_table[:,a,:,:], b.values)

            b_prob_val = 0
            for o in model.observations:
                b_prob_val += (b_probs[o] * upper_bound_belief_value_map.evaluate(b.update(a,o)))
            
            qva = float(xp.dot(model.expected_rewards_table[:,a], b.values) + (self.gamma * b_prob_val))

            # qva = upper_bound_belief_value_map.qva(b, a, gamma=self.gamma)
            if qva &gt; max_qv:
                max_qv = qva
                best_a = a

        # Choose o that max gap between bounds
        b_probs = xp.einsum(&#39;sor,s-&gt;o&#39;, model.reachable_transitional_observation_table[:,best_a,:,:], b.values)

        max_o_val = -xp.inf
        best_v_diff = -xp.inf
        next_b = b

        for o in model.observations:
            bao = b.update(best_a, o)

            upper_v_bao = upper_bound_belief_value_map.evaluate(bao)
            lower_v_bao = xp.max(xp.dot(value_function.alpha_vector_array, bao.values))

            v_diff = (upper_v_bao - lower_v_bao)

            o_val = b_probs[o] * v_diff
            
            if o_val &gt; max_o_val:
                max_o_val = o_val
                best_v_diff = v_diff
                next_b = bao

        # if bounds_split &lt; conv_term or max_generation &lt;= 0:
        if best_v_diff &lt; conv_term or max_generation &lt;= 1:
            return BeliefSet(model, [next_b])
        
        # Add the belief point and associated value to the belief-value mapping
        upper_bound_belief_value_map.add(b, max_qv)

        # Go one step deeper in the recursion
        b_set = self.expand_hsvi(model=model,
                                 b=next_b,
                                 value_function=value_function,
                                 upper_bound_belief_value_map=upper_bound_belief_value_map,
                                 conv_term=conv_term,
                                 max_generation=max_generation-1)
        
        # Append the nex belief of this iteration to the deeper beliefs
        new_belief_list = b_set.belief_list
        new_belief_list.append(next_b)

        return BeliefSet(model, new_belief_list)


    def expand_fsvi(self,
                   model:Model,
                   b:Belief,
                   mdp_policy:ValueFunction,
                   s:Union[int, None]=None,
                   max_generation:int=10,
                   sequence_string:str=&#39;&#39;
                   ) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs following the the Forward Search Value Iteration principles.
        It is a recursive function that is started by a initial state &#39;s&#39; and using the MDP policy, chooses the best action to take.
        Following this, a random next state &#39;s_p&#39; is being sampled from the transition probabilities and a random observation &#39;o&#39; based on the observation probabilities.
        Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence.
        Once the state is a goal state, the recursion is done and the belief sequence is returned.

        Parameters
        ----------
        model : pomdp.Model
            The model in which the exploration process will happen.
        b : Belief
            A belief to be added to the returned belief sequence and updated for the next step of the recursion.
        mdp_policy : ValueFunction
            The mdp policy used to choose the action from with the given state &#39;s&#39;.
        s : int
            The state that starts the exploration sequence and based on which an action will be chosen.
        max_generation : int, default=10
            The maximum recursion depth that can be reached before the generated belief sequence is returned.
        sequence_string : str, default=&#39;&#39;
            The sequence of previously explored actions and observations.
        
        Returns
        -------
        belief_set : BeliefSet
            A new sequence of beliefs.
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(b.values)
        belief_list = [b]

        # If state not provided pick a random one
        if s is None:
            s = b.random_state()

        # If end is not reached
        if (s not in model.end_states) and (max_generation &gt; 0):
            # Choose action based on mdp value function
            a_star = xp.argmax(mdp_policy.alpha_vector_array[:,s])

            # Pick a random next state (weighted by transition probabilities)
            s_p = model.transition(s, a_star)

            # Pick a random observation weighted by observation probabilities in state s_p and after having done action a_star
            o = model.observe(s_p, a_star)

            # Update sequence string
            sequence_string += (&#39;-&#39; if len(sequence_string) &gt; 0 else &#39;&#39;) + f&#39;{a_star},{o}&#39;

            # Generate a new belief based on a_star and o
            b_p = b.update(a_star, o)

            # Recursive call to go closer to goal
            b_set = self.expand_fsvi(model=model,
                                     b=b_p,
                                     mdp_policy=mdp_policy,
                                     s=s_p,
                                     max_generation=max_generation-1,
                                     sequence_string=sequence_string)
            belief_list.extend(b_set.belief_list)
        
        return BeliefSet(model, belief_list)
    

    def expand_perseus(self,
                       model:Model,
                       b:Belief,
                       max_generation:int=10
                       ) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs.
        It is a recursive function that is started by a initial state &#39;s&#39; and using the MDP policy, chooses the best action to take.
        Following this, a random next state &#39;s_p&#39; is being sampled from the transition probabilities and a random observation &#39;o&#39; based on the observation probabilities.
        Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence.
        Once the state is a goal state, the recursion is done and the belief sequence is returned.

        Parameters
        ----------
        model : pomdp.Model
            The model in which the exploration process will happen.
        b : Belief
            A belief to be added to the returned belief sequence and updated for the next step of the recursion.
        max_generation : int, default=10
            The maximum recursion depth that can be reached before the generated belief sequence is returned.
        
        Returns
        -------
        belief_set : BeliefSet
            A new sequence of beliefs.
        &#39;&#39;&#39;
        xp = np if not gpu_support else cp.get_array_module(b.values)

        initial_belief = b
        belief_sequence = []

        for i in range(max_generation):
            # Choose random action
            a = int(xp.random.choice(model.actions, size=1)[0])

            # Choose random observation based on prob: P(o|b,a)
            obs_prob = xp.einsum(&#39;sor,s-&gt;o&#39;, model.reachable_transitional_observation_table[:,a,:,:], b.values)
            o = int(xp.random.choice(model.observations, size=1, p=obs_prob)[0])

            # Update belief
            bao = b.update(a,o)

            # Finalization
            belief_sequence.append(bao)
            b = bao

        return BeliefSet(model, belief_sequence)
    

    def expand(self, model:Model, belief_set:BeliefSet, max_generation:int, **function_specific_parameters) -&gt; BeliefSet:
        &#39;&#39;&#39;
        Central method to call one of the functions for a particular expansion strategy:
            - Random selction (RA)
            - Stochastic Simulation with Random Action (ssra)
            - Stochastic Simulation with Greedy Action (ssga)
            - Stochastic Simulation with Exploratory Action (ssea)
            - Greedy Error Reduction (ger)
            - Heuristic Search Value Iteration (hsvi)
            - Forward Search Value Iteration (fsvi)
            - Perseus (perseus)
                
        Parameters
        ----------
        model : pomdp.Model
            The model on which to run the belief expansion on.
        belief_set : BeliefSet
            The set of beliefs to expand.
        max_generation : int
            The max amount of beliefs that can be added to the belief set at once.
        function_specific_parameters
            Potential additional parameters necessary for the specific expand function.

        Returns
        -------
        belief_set_new : BeliefSet
            The belief set the expansion function returns. 
        &#39;&#39;&#39;
        if self.expand_function in &#39;expand_ra&#39;:
            return self.expand_ra(model=model, belief_set=belief_set, max_generation=max_generation)

        elif self.expand_function in &#39;expand_ssra&#39;:
            return self.expand_ssra(model=model, belief_set=belief_set, max_generation=max_generation)
        
        elif self.expand_function in &#39;expand_ssga&#39;:
            args = {arg: function_specific_parameters[arg] for arg in [&#39;value_function&#39;, &#39;epsilon&#39;] if arg in function_specific_parameters}
            return self.expand_ssga(model=model, belief_set=belief_set, max_generation=max_generation, **args)
        
        elif self.expand_function in &#39;expand_ssea&#39;:
            return self.expand_ssea(model=model, belief_set=belief_set, max_generation=max_generation)
        
        elif self.expand_function in &#39;expand_ger&#39;:
            args = {arg: function_specific_parameters[arg] for arg in [&#39;value_function&#39;] if arg in function_specific_parameters}
            return self.expand_ger(model=model, belief_set=belief_set, max_generation=max_generation, **args)
        
        elif self.expand_function in &#39;expand_hsvi&#39;:
            args = {arg: function_specific_parameters[arg] for arg in [&#39;value_function&#39;, &#39;mdp_policy&#39;] if arg in function_specific_parameters}
            if not hasattr(self, &#39;_upper_bound&#39;):
                self._upper_bound = BeliefValueMapping(model, args[&#39;mdp_policy&#39;])
            else:
                self._upper_bound.update()
            return self.expand_hsvi(model=model, 
                                    b=belief_set.belief_list[0],
                                    value_function=args[&#39;value_function&#39;],
                                    upper_bound_belief_value_map=self._upper_bound,
                                    max_generation=max_generation)
        
        elif self.expand_function in &#39;expand_fsvi&#39;:
            args = {arg: function_specific_parameters[arg] for arg in [&#39;mdp_policy&#39;] if arg in function_specific_parameters}
            return self.expand_fsvi(model=model, 
                                    b=belief_set.belief_list[0],
                                    mdp_policy=args[&#39;mdp_policy&#39;],
                                    max_generation=max_generation)
        
        elif self.expand_function in &#39;expand_perseus&#39;:
            return self.expand_perseus(model=model, b=belief_set.belief_list[0], max_generation=max_generation)
        
        else:
            raise Exception(&#39;Not implemented&#39;)

        return []


    def compute_change(self, value_function:ValueFunction, new_value_function:ValueFunction, belief_set:BeliefSet) -&gt; float:
        &#39;&#39;&#39;
        Function to compute whether the change between two value functions can be considered as having converged based on the eps parameter of the Solver.
        It check for each belief, the maximum value and take the max change between believe&#39;s value functions.
        If this max change is lower than eps * (gamma / (1 - gamma)).

        Parameters
        ----------
        value_function : ValueFunction
            The first value function to compare.
        new_value_function : ValueFunction
            The second value function to compare.
        belief_set : BeliefSet
            The set of believes to check the values on to compute the max change on.

        Returns
        -------
        max_change : float
            The maximum change between value functions at belief points.
        &#39;&#39;&#39;
        # Get numpy corresponding to the arrays
        xp = np if not gpu_support else cp.get_array_module(value_function.alpha_vector_array)

        # Computing Delta for each beliefs
        max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, value_function.alpha_vector_array.T), axis=1)
        new_max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, new_value_function.alpha_vector_array.T), axis=1)
        max_change = xp.max(xp.abs(new_max_val_per_belief - max_val_per_belief))

        return max_change


    def solve(self,
              model:Model,
              expansions:int,
              full_backup:Union[bool,None]=None,
              update_passes:int=1,
              max_belief_growth:int=10,
              initial_belief:Union[BeliefSet, Belief, None]=None,
              initial_value_function:Union[ValueFunction,None]=None,
              prune_level:int=1,
              prune_interval:int=10,
              use_gpu:bool=False,
              history_tracking_level:int=1,
              print_progress:bool=True
              ) -&gt; tuple[ValueFunction, SolverHistory]:
        &#39;&#39;&#39;
        Main loop of the Point-Based Value Iteration algorithm.
        It consists in 2 steps, Backup and Expand.
        1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function
        2. Backup: Updates the alpha vectors based on the current belief set

        Depending on the expand strategy chosen, various extra parameters are needed. List of the available expand strategies and their extra required parameters:
            - ssra: Stochastic Simulation with Random Action. Extra params: /
            - ssga: Stochastic Simulation with Greedy Action. Extra params: epsilon (float)
            - ssea: Stochastic Simulation with Exploratory Action. Extra params: /
            - ger: Greedy Error Reduction. Extra params: /
            - hsvi: Heuristic Search Value Iteration. Extra param: mdp_policy (ValueFunction)
            - fsvi: Forward Search Value Iteration: Extra param: mdp_policy (ValueFunction)
            - perseus: Perseus. Extra params: /

        Parameters
        ----------
        model : pomdp.Model
            The model to solve.
        expansions : int
            How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)
        full_backup : bool, optional
            Whether to force the backup function has to be run on the full set beliefs uncovered since the beginning or only on the new points.
            By default, it will be determined by which expand function is chosen (False if: fsvi, hsvi, perseus; True otherwise)
        update_passes : int, default=1
            How many times the backup function has to be run every time the belief set is expanded.
        max_belief_growth : int, default=10
            How many beliefs can be added at every expansion step to the belief set.
        initial_belief : BeliefSet or Belief, optional
            An initial list of beliefs to start with.
        initial_value_function : ValueFunction, optional
            An initial value function to start the solving process with.
        prune_level : int, default=1
            Parameter to prune the value function further before the expand function.
        prune_interval : int, default=10
            How often to prune the value function. It is counted in number of backup iterations.
        use_gpu : bool, default=False
            Whether to use the GPU with cupy array to accelerate solving.
        history_tracking_level : int, default=1
            How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)
        print_progress : bool, default=True
            Whether or not to print out the progress of the value iteration process.

        Returns
        -------
        value_function : ValueFunction
            The alpha vectors approximating the value function.
        solver_history : SolverHistory
            The history of the solving process with some plotting options.
        &#39;&#39;&#39;
        # numpy or cupy module
        xp = np

        # If GPU usage
        if use_gpu:
            assert gpu_support, &#34;GPU support is not enabled, Cupy might need to be installed...&#34;
            model = model.gpu_model

            # Replace numpy module by cupy for computations
            xp = cp

        # Initial belief
        if initial_belief is None:
            belief_set = BeliefSet(model, [Belief(model)])
        elif isinstance(initial_belief, BeliefSet):
            belief_set = initial_belief.to_gpu() if use_gpu else initial_belief 
        else:
            initial_belief = Belief(model, xp.array(initial_belief.values))
            belief_set = BeliefSet(model, [initial_belief])
        
        # Initial value function
        if initial_value_function is None:
            value_function = ValueFunction(model, model.expected_rewards_table.T, model.actions)
        else:
            value_function = initial_value_function.to_gpu() if use_gpu else initial_value_function

        # Full backup setter if not forced
        if full_backup is None:
            full_backup = any([self.expand_function in func for func in [&#39;expand_ra&#39;, &#39;expand_ssra&#39;, &#39;expand_ssga&#39;, &#39;expand_ssea&#39;, &#39;expand_ger&#39;]])

        # For hsvi of fsvi, mdp policy is required as upper bound, so if it is not required, generate it
        if (&#39;mdp_policy&#39; not in self.expand_function_params) or (self.expand_function_params[&#39;mdp_policy&#39;] is None):
            log(&#39;[Warning] MDP solution not provided, running value iteration on the problem to retrieve it...&#39;)
            vi_solver = VI_Solver(gamma=self.gamma, eps=self.eps)

            log(&#39;    &gt; Starting MDP Value Iteration...&#39;)
            mdp_solution, hist = vi_solver.solve(model,
                                                 use_gpu=use_gpu,
                                                 print_progress=False)
            
            log(f&#39;    &gt; Value Iteration stopped or converged in {sum(hist.iteration_times):.3f}s, and after {len(hist.iteration_times)} iteration.\n&#39;)

            self.expand_function_params[&#39;mdp_policy&#39;] = mdp_solution

        # Convergence check boundary
        max_allowed_change = self.eps * (self.gamma / (1-self.gamma))

        # History tracking
        solver_history = SolverHistory(tracking_level=history_tracking_level,
                                       model=model,
                                       gamma=self.gamma,
                                       eps=self.eps,
                                       expand_function=self.expand_function,
                                       expand_append=full_backup,
                                       initial_value_function=value_function,
                                       initial_belief_set=belief_set)

        # Loop
        iteration = 0
        expand_value_function = value_function
        old_value_function = value_function

        try:
            for expansion_i in range(expansions) if not print_progress else trange(expansions, desc=&#39;Expansions&#39;):

                # 1: Expand belief set
                start_ts = datetime.now()

                new_belief_set = self.expand(model=model,
                                             belief_set=belief_set,
                                             value_function=value_function,
                                             max_generation=max_belief_growth,
                                             **self.expand_function_params)

                # If full backup append the newly generated set to the old belief_set
                if full_backup:
                    belief_set = BeliefSet(model, xp.vstack((belief_set.belief_array, new_belief_set.belief_array)))
                else:
                    belief_set = new_belief_set

                expand_time = (datetime.now() - start_ts).total_seconds()
                solver_history.add_expand_step(expansion_time=expand_time, belief_set=belief_set)

                # 2: Backup, update value function (alpha vector set)
                for _ in range(update_passes) if (not print_progress or update_passes &lt;= 1) else trange(update_passes, desc=f&#39;Backups {expansion_i}&#39;):
                    start_ts = datetime.now()

                    # Backup step
                    value_function = self.backup(model,
                                                 belief_set,
                                                 value_function,
                                                 append=(not full_backup),
                                                 belief_dominance_prune=False)
                    backup_time = (datetime.now() - start_ts).total_seconds()

                    # Additional pruning
                    if (iteration % prune_interval) == 0 and iteration &gt; 0:
                        start_ts = datetime.now()
                        vf_len = len(value_function)

                        value_function.prune(prune_level)

                        prune_time = (datetime.now() - start_ts).total_seconds()
                        alpha_vectors_pruned = len(value_function) - vf_len
                        solver_history.add_prune_step(prune_time, alpha_vectors_pruned)
                    
                    # Compute the change between value functions
                    max_change = self.compute_change(value_function, old_value_function, belief_set)

                    # History tracking
                    solver_history.add_backup_step(backup_time, max_change, value_function)

                    # Convergence check
                    if max_change &lt; max_allowed_change:
                        break

                    old_value_function = value_function

                    # Update iteration counter
                    iteration += 1

                # Compute change with old expansion value function
                expand_max_change = self.compute_change(expand_value_function, value_function, belief_set)

                if expand_max_change &lt; max_allowed_change:
                    print(&#39;Converged!&#39;)
                    break

                expand_value_function = value_function
        except MemoryError as e:
            print(f&#39;Memory full: {e}&#39;)
            print(&#39;Returning value function and history as is...\n&#39;)

        # Final pruning
        start_ts = datetime.now()
        vf_len = len(value_function)

        value_function.prune(prune_level)

        prune_time = (datetime.now() - start_ts).total_seconds()
        alpha_vectors_pruned = len(value_function) - vf_len
        solver_history.add_prune_step(prune_time, alpha_vectors_pruned)

        return value_function, solver_history</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="src.pomdp.Solver" href="#src.pomdp.Solver">Solver</a></li>
<li><a title="src.mdp.Solver" href="mdp.html#src.mdp.Solver">Solver</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="src.pomdp.FSVI_Solver" href="#src.pomdp.FSVI_Solver">FSVI_Solver</a></li>
<li><a title="src.pomdp.HSVI_Solver" href="#src.pomdp.HSVI_Solver">HSVI_Solver</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.pomdp.PBVI_Solver.backup"><code class="name flex">
<span>def <span class="ident">backup</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, belief_set: <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a>, value_function: <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>, append: bool = False, belief_dominance_prune: bool = True) ‑> <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a></span>
</code></dt>
<dd>
<div class="desc"><p>This function has purpose to update the set of alpha vectors. It does so in 3 steps:
1. It creates projections from each alpha vector for each possible action and each possible observation
2. It collapses this set of generated alpha vectors by taking the weighted sum of the alpha vectors weighted by the observation probability and this for each action and for each belief.
3. Then it further collapses the set to take the best alpha vector and action per belief
In the end we have a set of alpha vectors as large as the amount of beliefs.</p>
<p>The alpha vectors are also pruned to avoid duplicates and remove dominated ones.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The model on which to run the backup method on.</dd>
<dt><strong><code>belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>The belief set to use to generate the new alpha vectors with.</dd>
<dt><strong><code>value_function</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>The alpha vectors to generate the new set from.</dd>
<dt><strong><code>append</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to append the new alpha vectors generated to the old alpha vectors before pruning.</dd>
<dt><strong><code>belief_dominance_prune</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether, before returning the new value function, checks what alpha vectors have a supperior value, if so it adds it.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>new_alpha_set</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>A list of updated alpha vectors.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backup(self,
           model:Model,
           belief_set:BeliefSet,
           value_function:ValueFunction,
           append:bool=False,
           belief_dominance_prune:bool=True
           ) -&gt; ValueFunction:
    &#39;&#39;&#39;
    This function has purpose to update the set of alpha vectors. It does so in 3 steps:
    1. It creates projections from each alpha vector for each possible action and each possible observation
    2. It collapses this set of generated alpha vectors by taking the weighted sum of the alpha vectors weighted by the observation probability and this for each action and for each belief.
    3. Then it further collapses the set to take the best alpha vector and action per belief
    In the end we have a set of alpha vectors as large as the amount of beliefs.

    The alpha vectors are also pruned to avoid duplicates and remove dominated ones.

    Parameters
    ----------
    model : pomdp.Model
        The model on which to run the backup method on.
    belief_set : BeliefSet
        The belief set to use to generate the new alpha vectors with.
    value_function : ValueFunction
        The alpha vectors to generate the new set from.
    append : bool, default=False
        Whether to append the new alpha vectors generated to the old alpha vectors before pruning.
    belief_dominance_prune : bool, default=True
        Whether, before returning the new value function, checks what alpha vectors have a supperior value, if so it adds it.
        
    Returns
    -------
    new_alpha_set : ValueFunction
        A list of updated alpha vectors.
    &#39;&#39;&#39;
    # Get numpy corresponding to the arrays
    xp = np if not gpu_support else cp.get_array_module(value_function.alpha_vector_array)

    # Step 1
    vector_array = value_function.alpha_vector_array
    vectors_array_reachable_states = vector_array[xp.arange(vector_array.shape[0])[:,None,None,None], model.reachable_states[None,:,:,:]]
    
    gamma_a_o_t = self.gamma * xp.einsum(&#39;saor,vsar-&gt;aovs&#39;, model.reachable_transitional_observation_table, vectors_array_reachable_states)

    # Step 2
    belief_array = belief_set.belief_array # bs
    best_alpha_ind = xp.argmax(xp.tensordot(belief_array, gamma_a_o_t, (1,3)), axis=3) # argmax(bs,aovs-&gt;baov) -&gt; bao

    best_alphas_per_o = gamma_a_o_t[model.actions[None,:,None,None], model.observations[None,None,:,None], best_alpha_ind[:,:,:,None], model.states[None,None,None,:]] # baos

    alpha_a = model.expected_rewards_table.T + xp.sum(best_alphas_per_o, axis=2) # as + bas

    # Step 3
    best_actions = xp.argmax(xp.einsum(&#39;bas,bs-&gt;ba&#39;, alpha_a, belief_array), axis=1)
    alpha_vectors = xp.take_along_axis(alpha_a, best_actions[:,None,None],axis=1)[:,0,:]

    # Belief domination
    if belief_dominance_prune:
        best_value_per_belief = xp.sum((belief_array * alpha_vectors), axis=1)
        old_best_value_per_belief = xp.max(xp.matmul(belief_array, vector_array.T), axis=1)
        dominating_vectors = best_value_per_belief &gt; old_best_value_per_belief

        best_actions = best_actions[dominating_vectors]
        alpha_vectors = alpha_vectors[dominating_vectors]

    # Creation of value function
    new_value_function = ValueFunction(model, alpha_vectors, best_actions)

    # Union with previous value function
    if append:
        new_value_function.extend(value_function)
    
    return new_value_function</code></pre>
</details>
</dd>
<dt id="src.pomdp.PBVI_Solver.compute_change"><code class="name flex">
<span>def <span class="ident">compute_change</span></span>(<span>self, value_function: <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>, new_value_function: <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>, belief_set: <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a>) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Function to compute whether the change between two value functions can be considered as having converged based on the eps parameter of the Solver.
It check for each belief, the maximum value and take the max change between believe's value functions.
If this max change is lower than eps * (gamma / (1 - gamma)).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>value_function</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>The first value function to compare.</dd>
<dt><strong><code>new_value_function</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>The second value function to compare.</dd>
<dt><strong><code>belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>The set of believes to check the values on to compute the max change on.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>max_change</code></strong> :&ensp;<code>float</code></dt>
<dd>The maximum change between value functions at belief points.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_change(self, value_function:ValueFunction, new_value_function:ValueFunction, belief_set:BeliefSet) -&gt; float:
    &#39;&#39;&#39;
    Function to compute whether the change between two value functions can be considered as having converged based on the eps parameter of the Solver.
    It check for each belief, the maximum value and take the max change between believe&#39;s value functions.
    If this max change is lower than eps * (gamma / (1 - gamma)).

    Parameters
    ----------
    value_function : ValueFunction
        The first value function to compare.
    new_value_function : ValueFunction
        The second value function to compare.
    belief_set : BeliefSet
        The set of believes to check the values on to compute the max change on.

    Returns
    -------
    max_change : float
        The maximum change between value functions at belief points.
    &#39;&#39;&#39;
    # Get numpy corresponding to the arrays
    xp = np if not gpu_support else cp.get_array_module(value_function.alpha_vector_array)

    # Computing Delta for each beliefs
    max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, value_function.alpha_vector_array.T), axis=1)
    new_max_val_per_belief = xp.max(xp.matmul(belief_set.belief_array, new_value_function.alpha_vector_array.T), axis=1)
    max_change = xp.max(xp.abs(new_max_val_per_belief - max_val_per_belief))

    return max_change</code></pre>
</details>
</dd>
<dt id="src.pomdp.PBVI_Solver.expand"><code class="name flex">
<span>def <span class="ident">expand</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, belief_set: <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a>, max_generation: int, **function_specific_parameters) ‑> <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>Central method to call one of the functions for a particular expansion strategy:
- Random selction (RA)
- Stochastic Simulation with Random Action (ssra)
- Stochastic Simulation with Greedy Action (ssga)
- Stochastic Simulation with Exploratory Action (ssea)
- Greedy Error Reduction (ger)
- Heuristic Search Value Iteration (hsvi)
- Forward Search Value Iteration (fsvi)
- Perseus (perseus)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The model on which to run the belief expansion on.</dd>
<dt><strong><code>belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>The set of beliefs to expand.</dd>
<dt><strong><code>max_generation</code></strong> :&ensp;<code>int</code></dt>
<dd>The max amount of beliefs that can be added to the belief set at once.</dd>
<dt><strong><code>function_specific_parameters</code></strong></dt>
<dd>Potential additional parameters necessary for the specific expand function.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>belief_set_new</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>The belief set the expansion function returns.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand(self, model:Model, belief_set:BeliefSet, max_generation:int, **function_specific_parameters) -&gt; BeliefSet:
    &#39;&#39;&#39;
    Central method to call one of the functions for a particular expansion strategy:
        - Random selction (RA)
        - Stochastic Simulation with Random Action (ssra)
        - Stochastic Simulation with Greedy Action (ssga)
        - Stochastic Simulation with Exploratory Action (ssea)
        - Greedy Error Reduction (ger)
        - Heuristic Search Value Iteration (hsvi)
        - Forward Search Value Iteration (fsvi)
        - Perseus (perseus)
            
    Parameters
    ----------
    model : pomdp.Model
        The model on which to run the belief expansion on.
    belief_set : BeliefSet
        The set of beliefs to expand.
    max_generation : int
        The max amount of beliefs that can be added to the belief set at once.
    function_specific_parameters
        Potential additional parameters necessary for the specific expand function.

    Returns
    -------
    belief_set_new : BeliefSet
        The belief set the expansion function returns. 
    &#39;&#39;&#39;
    if self.expand_function in &#39;expand_ra&#39;:
        return self.expand_ra(model=model, belief_set=belief_set, max_generation=max_generation)

    elif self.expand_function in &#39;expand_ssra&#39;:
        return self.expand_ssra(model=model, belief_set=belief_set, max_generation=max_generation)
    
    elif self.expand_function in &#39;expand_ssga&#39;:
        args = {arg: function_specific_parameters[arg] for arg in [&#39;value_function&#39;, &#39;epsilon&#39;] if arg in function_specific_parameters}
        return self.expand_ssga(model=model, belief_set=belief_set, max_generation=max_generation, **args)
    
    elif self.expand_function in &#39;expand_ssea&#39;:
        return self.expand_ssea(model=model, belief_set=belief_set, max_generation=max_generation)
    
    elif self.expand_function in &#39;expand_ger&#39;:
        args = {arg: function_specific_parameters[arg] for arg in [&#39;value_function&#39;] if arg in function_specific_parameters}
        return self.expand_ger(model=model, belief_set=belief_set, max_generation=max_generation, **args)
    
    elif self.expand_function in &#39;expand_hsvi&#39;:
        args = {arg: function_specific_parameters[arg] for arg in [&#39;value_function&#39;, &#39;mdp_policy&#39;] if arg in function_specific_parameters}
        if not hasattr(self, &#39;_upper_bound&#39;):
            self._upper_bound = BeliefValueMapping(model, args[&#39;mdp_policy&#39;])
        else:
            self._upper_bound.update()
        return self.expand_hsvi(model=model, 
                                b=belief_set.belief_list[0],
                                value_function=args[&#39;value_function&#39;],
                                upper_bound_belief_value_map=self._upper_bound,
                                max_generation=max_generation)
    
    elif self.expand_function in &#39;expand_fsvi&#39;:
        args = {arg: function_specific_parameters[arg] for arg in [&#39;mdp_policy&#39;] if arg in function_specific_parameters}
        return self.expand_fsvi(model=model, 
                                b=belief_set.belief_list[0],
                                mdp_policy=args[&#39;mdp_policy&#39;],
                                max_generation=max_generation)
    
    elif self.expand_function in &#39;expand_perseus&#39;:
        return self.expand_perseus(model=model, b=belief_set.belief_list[0], max_generation=max_generation)
    
    else:
        raise Exception(&#39;Not implemented&#39;)

    return []</code></pre>
</details>
</dd>
<dt id="src.pomdp.PBVI_Solver.expand_fsvi"><code class="name flex">
<span>def <span class="ident">expand_fsvi</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, b: <a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>, mdp_policy: <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>, s: Optional[int] = None, max_generation: int = 10, sequence_string: str = '') ‑> <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs following the the Forward Search Value Iteration principles.
It is a recursive function that is started by a initial state 's' and using the MDP policy, chooses the best action to take.
Following this, a random next state 's_p' is being sampled from the transition probabilities and a random observation 'o' based on the observation probabilities.
Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence.
Once the state is a goal state, the recursion is done and the belief sequence is returned.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The model in which the exploration process will happen.</dd>
<dt><strong><code>b</code></strong> :&ensp;<code><a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></code></dt>
<dd>A belief to be added to the returned belief sequence and updated for the next step of the recursion.</dd>
<dt><strong><code>mdp_policy</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>The mdp policy used to choose the action from with the given state 's'.</dd>
<dt><strong><code>s</code></strong> :&ensp;<code>int</code></dt>
<dd>The state that starts the exploration sequence and based on which an action will be chosen.</dd>
<dt><strong><code>max_generation</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>The maximum recursion depth that can be reached before the generated belief sequence is returned.</dd>
<dt><strong><code>sequence_string</code></strong> :&ensp;<code>str</code>, default=<code>''</code></dt>
<dd>The sequence of previously explored actions and observations.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>A new sequence of beliefs.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_fsvi(self,
               model:Model,
               b:Belief,
               mdp_policy:ValueFunction,
               s:Union[int, None]=None,
               max_generation:int=10,
               sequence_string:str=&#39;&#39;
               ) -&gt; BeliefSet:
    &#39;&#39;&#39;
    Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs following the the Forward Search Value Iteration principles.
    It is a recursive function that is started by a initial state &#39;s&#39; and using the MDP policy, chooses the best action to take.
    Following this, a random next state &#39;s_p&#39; is being sampled from the transition probabilities and a random observation &#39;o&#39; based on the observation probabilities.
    Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence.
    Once the state is a goal state, the recursion is done and the belief sequence is returned.

    Parameters
    ----------
    model : pomdp.Model
        The model in which the exploration process will happen.
    b : Belief
        A belief to be added to the returned belief sequence and updated for the next step of the recursion.
    mdp_policy : ValueFunction
        The mdp policy used to choose the action from with the given state &#39;s&#39;.
    s : int
        The state that starts the exploration sequence and based on which an action will be chosen.
    max_generation : int, default=10
        The maximum recursion depth that can be reached before the generated belief sequence is returned.
    sequence_string : str, default=&#39;&#39;
        The sequence of previously explored actions and observations.
    
    Returns
    -------
    belief_set : BeliefSet
        A new sequence of beliefs.
    &#39;&#39;&#39;
    xp = np if not gpu_support else cp.get_array_module(b.values)
    belief_list = [b]

    # If state not provided pick a random one
    if s is None:
        s = b.random_state()

    # If end is not reached
    if (s not in model.end_states) and (max_generation &gt; 0):
        # Choose action based on mdp value function
        a_star = xp.argmax(mdp_policy.alpha_vector_array[:,s])

        # Pick a random next state (weighted by transition probabilities)
        s_p = model.transition(s, a_star)

        # Pick a random observation weighted by observation probabilities in state s_p and after having done action a_star
        o = model.observe(s_p, a_star)

        # Update sequence string
        sequence_string += (&#39;-&#39; if len(sequence_string) &gt; 0 else &#39;&#39;) + f&#39;{a_star},{o}&#39;

        # Generate a new belief based on a_star and o
        b_p = b.update(a_star, o)

        # Recursive call to go closer to goal
        b_set = self.expand_fsvi(model=model,
                                 b=b_p,
                                 mdp_policy=mdp_policy,
                                 s=s_p,
                                 max_generation=max_generation-1,
                                 sequence_string=sequence_string)
        belief_list.extend(b_set.belief_list)
    
    return BeliefSet(model, belief_list)</code></pre>
</details>
</dd>
<dt id="src.pomdp.PBVI_Solver.expand_ger"><code class="name flex">
<span>def <span class="ident">expand_ger</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, belief_set: <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a>, value_function: <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>, max_generation: int = 10) ‑> <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>Greedy Error Reduction.
It attempts to choose the believes that will maximize the improvement of the value function by minimizing the error.
The error is computed by the sum of the change between two beliefs and their two corresponding alpha vectors.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The POMDP model on which to expand the belief set on.</dd>
<dt><strong><code>belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>List of beliefs to expand on.</dd>
<dt><strong><code>value_function</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>Used to find the best action knowing the belief.</dd>
<dt><strong><code>max_generation</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>The max amount of beliefs that can be added to the belief set at once.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>belief_set_new</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>Union of the belief_set and the expansions of the beliefs in the belief_set</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_ger(self, model:Model, belief_set:BeliefSet, value_function:ValueFunction, max_generation:int=10) -&gt; BeliefSet:
    &#39;&#39;&#39;
    Greedy Error Reduction.
    It attempts to choose the believes that will maximize the improvement of the value function by minimizing the error.
    The error is computed by the sum of the change between two beliefs and their two corresponding alpha vectors.

    Parameters
    ----------
    model : pomdp.Model
        The POMDP model on which to expand the belief set on.
    belief_set : BeliefSet
        List of beliefs to expand on.
    value_function : ValueFunction
        Used to find the best action knowing the belief.
    max_generation : int, default=10
        The max amount of beliefs that can be added to the belief set at once.

    Returns
    -------
    belief_set_new : BeliefSet
        Union of the belief_set and the expansions of the beliefs in the belief_set
    &#39;&#39;&#39;
    xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

    old_shape = belief_set.belief_array.shape
    to_generate = min(max_generation, old_shape[0])

    new_belief_array = xp.empty((old_shape[0] + to_generate, old_shape[1]))
    new_belief_array[:old_shape[0]] = belief_set.belief_array

    # Finding the min and max rewards for computation of the epsilon
    r_min = model._min_reward / (1 - self.gamma)
    r_max = model._max_reward / (1 - self.gamma)

    # Generation of all potential successor beliefs
    successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])
    
    # Finding the alphas associated with each previous beliefs
    best_alpha = xp.argmax(xp.dot(belief_set.belief_array, value_function.alpha_vector_array.T), axis = 1)
    b_alphas = value_function.alpha_vector_array[best_alpha]

    # Difference between beliefs and their successors
    b_diffs = successor_beliefs - belief_set.belief_array[:,None,None,:]

    # Computing a &#39;next&#39; alpha vector made of the max and min
    alphas_p = xp.where(b_diffs &gt;= 0, r_max, r_min)

    # Difference between alpha vectors and their successors alpha vector
    alphas_diffs = alphas_p - b_alphas[:,None,None,:]

    # Computing epsilon for all successor beliefs
    eps = xp.einsum(&#39;baos,baos-&gt;bao&#39;, alphas_diffs, b_diffs)

    # Computing the probability of the b and doing action a and receiving observation o
    bao_probs = xp.einsum(&#39;bs,saor-&gt;bao&#39;, belief_set.belief_array, model.reachable_transitional_observation_table)

    # Taking the sumproduct of the probs with the epsilons
    res = xp.einsum(&#39;bao,bao-&gt;ba&#39;, bao_probs, eps)

    # Picking the correct amount of initial beliefs and ideal actions
    b_stars, a_stars = xp.unravel_index(xp.argsort(res, axis=None)[::-1][:to_generate], res.shape)

    # And picking the ideal observations
    o_star = xp.argmax(bao_probs[b_stars[:,None], a_stars[:,None], model.observations[None,:]] * eps[b_stars[:,None], a_stars[:,None], model.observations[None,:]], axis=1)

    # Selecting the successor beliefs
    new_belief_array = successor_beliefs[b_stars[:,None], a_stars[:,None], o_star[:,None], model.states[None,:]]

    return BeliefSet(model, new_belief_array)</code></pre>
</details>
</dd>
<dt id="src.pomdp.PBVI_Solver.expand_hsvi"><code class="name flex">
<span>def <span class="ident">expand_hsvi</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, b: <a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>, value_function: <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>, upper_bound_belief_value_map: <a title="src.pomdp.BeliefValueMapping" href="#src.pomdp.BeliefValueMapping">BeliefValueMapping</a>, conv_term: Optional[float] = None, max_generation: int = 10) ‑> <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>The expand function of the
Heruistic Search Value Iteration (HSVI) technique.
It is a redursive function attempting to minimize the bound between the upper and lower estimations of the value function.</p>
<p>It is developped by Smith T. and Simmons R. and described in the paper "Heuristic Search Value Iteration for POMDPs".</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The model in which the exploration process will happen.</dd>
<dt><strong><code>b</code></strong> :&ensp;<code><a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></code></dt>
<dd>A belief to be added to the returned belief sequence and updated for the next step of the recursion.</dd>
<dt><strong><code>value_function</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>The lower bound of the value function.</dd>
<dt><strong><code>upper_bound_belief_value_map</code></strong> :&ensp;<code><a title="src.pomdp.BeliefValueMapping" href="#src.pomdp.BeliefValueMapping">BeliefValueMapping</a></code></dt>
<dd>The upper bound of the value function.
Initially it is define with the mdp policy of the model (run: "BeliefValueMapping(model, mdp_policy)").
It is then refined through the expansion process by adding newly found belief and value pairs.</dd>
<dt><strong><code>max_generation</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>The maximum recursion depth that can be reached before the generated belief sequence is returned.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>A new sequence of beliefs.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_hsvi(self,
                model:Model,
                b:Belief,
                value_function:ValueFunction,
                upper_bound_belief_value_map:BeliefValueMapping,
                conv_term:Union[float,None]=None,
                max_generation:int=10
                ) -&gt; BeliefSet:
    &#39;&#39;&#39;
    The expand function of the  Heruistic Search Value Iteration (HSVI) technique.
    It is a redursive function attempting to minimize the bound between the upper and lower estimations of the value function.

    It is developped by Smith T. and Simmons R. and described in the paper &#34;Heuristic Search Value Iteration for POMDPs&#34;.
    
    Parameters
    ----------
    model : pomdp.Model
        The model in which the exploration process will happen.
    b : Belief
        A belief to be added to the returned belief sequence and updated for the next step of the recursion.
    value_function : ValueFunction
        The lower bound of the value function.
    upper_bound_belief_value_map : BeliefValueMapping
        The upper bound of the value function.
        Initially it is define with the mdp policy of the model (run: &#34;BeliefValueMapping(model, mdp_policy)&#34;).
        It is then refined through the expansion process by adding newly found belief and value pairs.
    max_generation : int, default=10
        The maximum recursion depth that can be reached before the generated belief sequence is returned.
    
    Returns
    -------
    belief_set : BeliefSet
        A new sequence of beliefs.
    &#39;&#39;&#39;
    xp = np if not gpu_support else cp.get_array_module(b.values)

    if conv_term is None:
        conv_term = self.eps

    # Update convergence term
    conv_term /= self.gamma

    # Find best a based on upper bound v
    max_qv = -xp.inf
    best_a = -1
    for a in model.actions:
        b_probs = xp.einsum(&#39;sor,s-&gt;o&#39;, model.reachable_transitional_observation_table[:,a,:,:], b.values)

        b_prob_val = 0
        for o in model.observations:
            b_prob_val += (b_probs[o] * upper_bound_belief_value_map.evaluate(b.update(a,o)))
        
        qva = float(xp.dot(model.expected_rewards_table[:,a], b.values) + (self.gamma * b_prob_val))

        # qva = upper_bound_belief_value_map.qva(b, a, gamma=self.gamma)
        if qva &gt; max_qv:
            max_qv = qva
            best_a = a

    # Choose o that max gap between bounds
    b_probs = xp.einsum(&#39;sor,s-&gt;o&#39;, model.reachable_transitional_observation_table[:,best_a,:,:], b.values)

    max_o_val = -xp.inf
    best_v_diff = -xp.inf
    next_b = b

    for o in model.observations:
        bao = b.update(best_a, o)

        upper_v_bao = upper_bound_belief_value_map.evaluate(bao)
        lower_v_bao = xp.max(xp.dot(value_function.alpha_vector_array, bao.values))

        v_diff = (upper_v_bao - lower_v_bao)

        o_val = b_probs[o] * v_diff
        
        if o_val &gt; max_o_val:
            max_o_val = o_val
            best_v_diff = v_diff
            next_b = bao

    # if bounds_split &lt; conv_term or max_generation &lt;= 0:
    if best_v_diff &lt; conv_term or max_generation &lt;= 1:
        return BeliefSet(model, [next_b])
    
    # Add the belief point and associated value to the belief-value mapping
    upper_bound_belief_value_map.add(b, max_qv)

    # Go one step deeper in the recursion
    b_set = self.expand_hsvi(model=model,
                             b=next_b,
                             value_function=value_function,
                             upper_bound_belief_value_map=upper_bound_belief_value_map,
                             conv_term=conv_term,
                             max_generation=max_generation-1)
    
    # Append the nex belief of this iteration to the deeper beliefs
    new_belief_list = b_set.belief_list
    new_belief_list.append(next_b)

    return BeliefSet(model, new_belief_list)</code></pre>
</details>
</dd>
<dt id="src.pomdp.PBVI_Solver.expand_perseus"><code class="name flex">
<span>def <span class="ident">expand_perseus</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, b: <a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>, max_generation: int = 10) ‑> <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs.
It is a recursive function that is started by a initial state 's' and using the MDP policy, chooses the best action to take.
Following this, a random next state 's_p' is being sampled from the transition probabilities and a random observation 'o' based on the observation probabilities.
Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence.
Once the state is a goal state, the recursion is done and the belief sequence is returned.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The model in which the exploration process will happen.</dd>
<dt><strong><code>b</code></strong> :&ensp;<code><a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></code></dt>
<dd>A belief to be added to the returned belief sequence and updated for the next step of the recursion.</dd>
<dt><strong><code>max_generation</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>The maximum recursion depth that can be reached before the generated belief sequence is returned.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>A new sequence of beliefs.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_perseus(self,
                   model:Model,
                   b:Belief,
                   max_generation:int=10
                   ) -&gt; BeliefSet:
    &#39;&#39;&#39;
    Function implementing the exploration process using the MDP policy in order to generate a sequence of Beliefs.
    It is a recursive function that is started by a initial state &#39;s&#39; and using the MDP policy, chooses the best action to take.
    Following this, a random next state &#39;s_p&#39; is being sampled from the transition probabilities and a random observation &#39;o&#39; based on the observation probabilities.
    Then the given belief is updated using the chosen action and the observation received and the updated belief is added to the sequence.
    Once the state is a goal state, the recursion is done and the belief sequence is returned.

    Parameters
    ----------
    model : pomdp.Model
        The model in which the exploration process will happen.
    b : Belief
        A belief to be added to the returned belief sequence and updated for the next step of the recursion.
    max_generation : int, default=10
        The maximum recursion depth that can be reached before the generated belief sequence is returned.
    
    Returns
    -------
    belief_set : BeliefSet
        A new sequence of beliefs.
    &#39;&#39;&#39;
    xp = np if not gpu_support else cp.get_array_module(b.values)

    initial_belief = b
    belief_sequence = []

    for i in range(max_generation):
        # Choose random action
        a = int(xp.random.choice(model.actions, size=1)[0])

        # Choose random observation based on prob: P(o|b,a)
        obs_prob = xp.einsum(&#39;sor,s-&gt;o&#39;, model.reachable_transitional_observation_table[:,a,:,:], b.values)
        o = int(xp.random.choice(model.observations, size=1, p=obs_prob)[0])

        # Update belief
        bao = b.update(a,o)

        # Finalization
        belief_sequence.append(bao)
        b = bao

    return BeliefSet(model, belief_sequence)</code></pre>
</details>
</dd>
<dt id="src.pomdp.PBVI_Solver.expand_ra"><code class="name flex">
<span>def <span class="ident">expand_ra</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, belief_set: <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a>, max_generation: int = 10) ‑> <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>This expansion technique relies only randomness and will generate at most 'max_generation' beliefs.</p>
<p>Parameters
model : pomdp.Model
The POMDP model on which to expand the belief set on.
belief_set : BeliefSet
List of beliefs to expand on.
max_generation : int, default=10
The max amount of beliefs that can be added to the belief set at once.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_ra(self, model:Model, belief_set:BeliefSet, max_generation:int=10) -&gt; BeliefSet:
    &#39;&#39;&#39;
    This expansion technique relies only randomness and will generate at most &#39;max_generation&#39; beliefs.

    Parameters
    model : pomdp.Model
        The POMDP model on which to expand the belief set on.
    belief_set : BeliefSet
        List of beliefs to expand on.
    max_generation : int, default=10
        The max amount of beliefs that can be added to the belief set at once.
    &#39;&#39;&#39;
    xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

    # How many new beliefs to add
    generation_count = min(belief_set.belief_array.shape[0], max_generation)

    # Generation of the new beliefs at random
    new_beliefs = xp.random.random((generation_count, model.state_count))
    new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]

    return BeliefSet(model, new_beliefs)</code></pre>
</details>
</dd>
<dt id="src.pomdp.PBVI_Solver.expand_ssea"><code class="name flex">
<span>def <span class="ident">expand_ssea</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, belief_set: <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a>, max_generation: int = 10) ‑> <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>Stochastic Simulation with Exploratory Action.
Simulates running steps forward for each possible action knowing we are a state s, chosen randomly with according to the belief probability.
These lead to a new state s_p and a observation o for each action.
From all these and observation o we can generate updated beliefs.
Then it takes the belief that is furthest away from other beliefs, meaning it explores the most the belief space.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The POMDP model on which to expand the belief set on.</dd>
<dt><strong><code>belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>List of beliefs to expand on.</dd>
<dt><strong><code>max_generation</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>The max amount of beliefs that can be added to the belief set at once.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>belief_set_new</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>Union of the belief_set and the expansions of the beliefs in the belief_set</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_ssea(self, model:Model, belief_set:BeliefSet, max_generation:int=10) -&gt; BeliefSet:
    &#39;&#39;&#39;
    Stochastic Simulation with Exploratory Action.
    Simulates running steps forward for each possible action knowing we are a state s, chosen randomly with according to the belief probability.
    These lead to a new state s_p and a observation o for each action.
    From all these and observation o we can generate updated beliefs. 
    Then it takes the belief that is furthest away from other beliefs, meaning it explores the most the belief space.

    Parameters
    ----------
    model : pomdp.Model
        The POMDP model on which to expand the belief set on.
    belief_set : BeliefSet
        List of beliefs to expand on.
    max_generation : int, default=10
        The max amount of beliefs that can be added to the belief set at once.

    Returns
    -------
    belief_set_new : BeliefSet
        Union of the belief_set and the expansions of the beliefs in the belief_set
    &#39;&#39;&#39;
    xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

    old_shape = belief_set.belief_array.shape
    to_generate = min(max_generation, old_shape[0])

    # Generation of successors
    successor_beliefs = xp.array([[[b.update(a,o).values for o in model.observations] for a in model.actions] for b in belief_set.belief_list])
    
    # Compute the distances between each pair and of successor are source beliefs
    diff = (belief_set.belief_array[:, None,None,None, :] - successor_beliefs)
    dist = xp.sqrt(xp.einsum(&#39;bnaos,bnaos-&gt;bnao&#39;, diff, diff))

    # Taking the min distance for each belief
    belief_min_dists = xp.min(dist,axis=0)

    # Taking the max distanced successors
    b_star, a_star, o_star = xp.unravel_index(xp.argsort(belief_min_dists, axis=None)[::-1][:to_generate], successor_beliefs.shape[:-1])

    # Selecting successor beliefs
    new_belief_array = successor_beliefs[b_star[:,None], a_star[:,None], o_star[:,None], model.states[None,:]]

    return BeliefSet(model, new_belief_array)</code></pre>
</details>
</dd>
<dt id="src.pomdp.PBVI_Solver.expand_ssga"><code class="name flex">
<span>def <span class="ident">expand_ssga</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, belief_set: <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a>, value_function: <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>, epsilon: float = 0.1, max_generation: int = 10) ‑> <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>Stochastic Simulation with Greedy Action.
Simulates running a single-step forward from the beliefs in the "belief_set".
The step forward is taking assuming we are in a random state s (weighted by the belief),
then taking the best action a based on the belief with probability 'epsilon'.
These lead to a new state s_p and a observation o.
From this action a and observation o we can update our belief. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The POMDP model on which to expand the belief set on.</dd>
<dt><strong><code>belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>List of beliefs to expand on.</dd>
<dt><strong><code>value_function</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>Used to find the best action knowing the belief.</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code></dt>
<dd>Parameter tuning how often we take a greedy approach and how often we move randomly.</dd>
<dt><strong><code>max_generation</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>The max amount of beliefs that can be added to the belief set at once.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>belief_set_new</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>Union of the belief_set and the expansions of the beliefs in the belief_set</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_ssga(self, model:Model, belief_set:BeliefSet, value_function:ValueFunction, epsilon:float=0.1, max_generation:int=10) -&gt; BeliefSet:
    &#39;&#39;&#39;
    Stochastic Simulation with Greedy Action.
    Simulates running a single-step forward from the beliefs in the &#34;belief_set&#34;.
    The step forward is taking assuming we are in a random state s (weighted by the belief),
     then taking the best action a based on the belief with probability &#39;epsilon&#39;.
    These lead to a new state s_p and a observation o.
    From this action a and observation o we can update our belief. 

    Parameters
    ----------
    model : pomdp.Model
        The POMDP model on which to expand the belief set on.
    belief_set : BeliefSet
        List of beliefs to expand on.
    value_function : ValueFunction
        Used to find the best action knowing the belief.
    eps : float
        Parameter tuning how often we take a greedy approach and how often we move randomly.
    max_generation : int, default=10
        The max amount of beliefs that can be added to the belief set at once.

    Returns
    -------
    belief_set_new : BeliefSet
        Union of the belief_set and the expansions of the beliefs in the belief_set
    &#39;&#39;&#39;
    xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

    old_shape = belief_set.belief_array.shape
    to_generate = min(max_generation, old_shape[0])

    new_belief_array = xp.empty((to_generate, old_shape[1]))

    # Random previous beliefs
    rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)

    for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):
        b = Belief(model, belief_vector)
        s = b.random_state()
        
        if random.random() &lt; epsilon:
            a = random.choice(model.actions)
        else:
            best_alpha_index = xp.argmax(xp.dot(value_function.alpha_vector_array, b.values))
            a = value_function.actions[best_alpha_index]
        
        s_p = model.transition(s, a)
        o = model.observe(s_p, a)
        b_new = b.update(a, o)
        
        new_belief_array[i] = b_new.values
        
    return BeliefSet(model, new_belief_array)</code></pre>
</details>
</dd>
<dt id="src.pomdp.PBVI_Solver.expand_ssra"><code class="name flex">
<span>def <span class="ident">expand_ssra</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, belief_set: <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a>, max_generation: int = 10) ‑> <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></span>
</code></dt>
<dd>
<div class="desc"><p>Stochastic Simulation with Random Action.
Simulates running a single-step forward from the beliefs in the "belief_set".
The step forward is taking assuming we are in a random state (weighted by the belief) and taking a random action leading to a state s_p and a observation o.
From this action a and observation o we can update our belief.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The POMDP model on which to expand the belief set on.</dd>
<dt><strong><code>belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>List of beliefs to expand on.</dd>
<dt><strong><code>max_generation</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>The max amount of beliefs that can be added to the belief set at once.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>belief_set_new</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>Union of the belief_set and the expansions of the beliefs in the belief_set</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expand_ssra(self, model:Model, belief_set:BeliefSet, max_generation:int=10) -&gt; BeliefSet:
    &#39;&#39;&#39;
    Stochastic Simulation with Random Action.
    Simulates running a single-step forward from the beliefs in the &#34;belief_set&#34;.
    The step forward is taking assuming we are in a random state (weighted by the belief) and taking a random action leading to a state s_p and a observation o.
    From this action a and observation o we can update our belief.

    Parameters
    ----------
    model : pomdp.Model
        The POMDP model on which to expand the belief set on.
    belief_set : BeliefSet
        List of beliefs to expand on.
    max_generation : int, default=10
        The max amount of beliefs that can be added to the belief set at once.

    Returns
    -------
    belief_set_new : BeliefSet
        Union of the belief_set and the expansions of the beliefs in the belief_set
    &#39;&#39;&#39;
    xp = np if not gpu_support else cp.get_array_module(belief_set.belief_array)

    old_shape = belief_set.belief_array.shape
    to_generate = min(max_generation, old_shape[0])

    new_belief_array = xp.empty((to_generate, old_shape[1]))

    # Random previous beliefs
    rand_ind = np.random.choice(np.arange(old_shape[0]), to_generate, replace=False)

    for i, belief_vector in enumerate(belief_set.belief_array[rand_ind]):
        b = Belief(model, belief_vector)
        s = b.random_state()
        a = random.choice(model.actions)
        s_p = model.transition(s, a)
        o = model.observe(s_p, a)
        b_new = b.update(a, o)
        
        new_belief_array[i] = b_new.values
        
    return BeliefSet(model, new_belief_array)</code></pre>
</details>
</dd>
<dt id="src.pomdp.PBVI_Solver.solve"><code class="name flex">
<span>def <span class="ident">solve</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, expansions: int, full_backup: Optional[bool] = None, update_passes: int = 1, max_belief_growth: int = 10, initial_belief: Union[<a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a>, <a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>, ForwardRef(None)] = None, initial_value_function: Optional[<a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>] = None, prune_level: int = 1, prune_interval: int = 10, use_gpu: bool = False, history_tracking_level: int = 1, print_progress: bool = True) ‑> tuple[<a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>, <a title="src.pomdp.SolverHistory" href="#src.pomdp.SolverHistory">SolverHistory</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Main loop of the Point-Based Value Iteration algorithm.
It consists in 2 steps, Backup and Expand.
1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function
2. Backup: Updates the alpha vectors based on the current belief set</p>
<p>Depending on the expand strategy chosen, various extra parameters are needed. List of the available expand strategies and their extra required parameters:
- ssra: Stochastic Simulation with Random Action. Extra params: /
- ssga: Stochastic Simulation with Greedy Action. Extra params: epsilon (float)
- ssea: Stochastic Simulation with Exploratory Action. Extra params: /
- ger: Greedy Error Reduction. Extra params: /
- hsvi: Heuristic Search Value Iteration. Extra param: mdp_policy (ValueFunction)
- fsvi: Forward Search Value Iteration: Extra param: mdp_policy (ValueFunction)
- perseus: Perseus. Extra params: /</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The model to solve.</dd>
<dt><strong><code>expansions</code></strong> :&ensp;<code>int</code></dt>
<dd>How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)</dd>
<dt><strong><code>full_backup</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to force the backup function has to be run on the full set beliefs uncovered since the beginning or only on the new points.
By default, it will be determined by which expand function is chosen (False if: fsvi, hsvi, perseus; True otherwise)</dd>
<dt><strong><code>update_passes</code></strong> :&ensp;<code>int</code>, default=<code>1</code></dt>
<dd>How many times the backup function has to be run every time the belief set is expanded.</dd>
<dt><strong><code>max_belief_growth</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>How many beliefs can be added at every expansion step to the belief set.</dd>
<dt><strong><code>initial_belief</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code> or <code><a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></code>, optional</dt>
<dd>An initial list of beliefs to start with.</dd>
<dt><strong><code>initial_value_function</code></strong> :&ensp;<code>ValueFunction</code>, optional</dt>
<dd>An initial value function to start the solving process with.</dd>
<dt><strong><code>prune_level</code></strong> :&ensp;<code>int</code>, default=<code>1</code></dt>
<dd>Parameter to prune the value function further before the expand function.</dd>
<dt><strong><code>prune_interval</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>How often to prune the value function. It is counted in number of backup iterations.</dd>
<dt><strong><code>use_gpu</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to use the GPU with cupy array to accelerate solving.</dd>
<dt><strong><code>history_tracking_level</code></strong> :&ensp;<code>int</code>, default=<code>1</code></dt>
<dd>How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)</dd>
<dt><strong><code>print_progress</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether or not to print out the progress of the value iteration process.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>value_function</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>The alpha vectors approximating the value function.</dd>
<dt><strong><code>solver_history</code></strong> :&ensp;<code><a title="src.pomdp.SolverHistory" href="#src.pomdp.SolverHistory">SolverHistory</a></code></dt>
<dd>The history of the solving process with some plotting options.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve(self,
          model:Model,
          expansions:int,
          full_backup:Union[bool,None]=None,
          update_passes:int=1,
          max_belief_growth:int=10,
          initial_belief:Union[BeliefSet, Belief, None]=None,
          initial_value_function:Union[ValueFunction,None]=None,
          prune_level:int=1,
          prune_interval:int=10,
          use_gpu:bool=False,
          history_tracking_level:int=1,
          print_progress:bool=True
          ) -&gt; tuple[ValueFunction, SolverHistory]:
    &#39;&#39;&#39;
    Main loop of the Point-Based Value Iteration algorithm.
    It consists in 2 steps, Backup and Expand.
    1. Expand: Expands the belief set base with a expansion strategy given by the parameter expand_function
    2. Backup: Updates the alpha vectors based on the current belief set

    Depending on the expand strategy chosen, various extra parameters are needed. List of the available expand strategies and their extra required parameters:
        - ssra: Stochastic Simulation with Random Action. Extra params: /
        - ssga: Stochastic Simulation with Greedy Action. Extra params: epsilon (float)
        - ssea: Stochastic Simulation with Exploratory Action. Extra params: /
        - ger: Greedy Error Reduction. Extra params: /
        - hsvi: Heuristic Search Value Iteration. Extra param: mdp_policy (ValueFunction)
        - fsvi: Forward Search Value Iteration: Extra param: mdp_policy (ValueFunction)
        - perseus: Perseus. Extra params: /

    Parameters
    ----------
    model : pomdp.Model
        The model to solve.
    expansions : int
        How many times the algorithm has to expand the belief set. (the size will be doubled every time, eg: for 5, the belief set will be of size 32)
    full_backup : bool, optional
        Whether to force the backup function has to be run on the full set beliefs uncovered since the beginning or only on the new points.
        By default, it will be determined by which expand function is chosen (False if: fsvi, hsvi, perseus; True otherwise)
    update_passes : int, default=1
        How many times the backup function has to be run every time the belief set is expanded.
    max_belief_growth : int, default=10
        How many beliefs can be added at every expansion step to the belief set.
    initial_belief : BeliefSet or Belief, optional
        An initial list of beliefs to start with.
    initial_value_function : ValueFunction, optional
        An initial value function to start the solving process with.
    prune_level : int, default=1
        Parameter to prune the value function further before the expand function.
    prune_interval : int, default=10
        How often to prune the value function. It is counted in number of backup iterations.
    use_gpu : bool, default=False
        Whether to use the GPU with cupy array to accelerate solving.
    history_tracking_level : int, default=1
        How thorough the tracking of the solving process should be. (0: Nothing; 1: Times and sizes of belief sets and value function; 2: The actual value functions and beliefs sets)
    print_progress : bool, default=True
        Whether or not to print out the progress of the value iteration process.

    Returns
    -------
    value_function : ValueFunction
        The alpha vectors approximating the value function.
    solver_history : SolverHistory
        The history of the solving process with some plotting options.
    &#39;&#39;&#39;
    # numpy or cupy module
    xp = np

    # If GPU usage
    if use_gpu:
        assert gpu_support, &#34;GPU support is not enabled, Cupy might need to be installed...&#34;
        model = model.gpu_model

        # Replace numpy module by cupy for computations
        xp = cp

    # Initial belief
    if initial_belief is None:
        belief_set = BeliefSet(model, [Belief(model)])
    elif isinstance(initial_belief, BeliefSet):
        belief_set = initial_belief.to_gpu() if use_gpu else initial_belief 
    else:
        initial_belief = Belief(model, xp.array(initial_belief.values))
        belief_set = BeliefSet(model, [initial_belief])
    
    # Initial value function
    if initial_value_function is None:
        value_function = ValueFunction(model, model.expected_rewards_table.T, model.actions)
    else:
        value_function = initial_value_function.to_gpu() if use_gpu else initial_value_function

    # Full backup setter if not forced
    if full_backup is None:
        full_backup = any([self.expand_function in func for func in [&#39;expand_ra&#39;, &#39;expand_ssra&#39;, &#39;expand_ssga&#39;, &#39;expand_ssea&#39;, &#39;expand_ger&#39;]])

    # For hsvi of fsvi, mdp policy is required as upper bound, so if it is not required, generate it
    if (&#39;mdp_policy&#39; not in self.expand_function_params) or (self.expand_function_params[&#39;mdp_policy&#39;] is None):
        log(&#39;[Warning] MDP solution not provided, running value iteration on the problem to retrieve it...&#39;)
        vi_solver = VI_Solver(gamma=self.gamma, eps=self.eps)

        log(&#39;    &gt; Starting MDP Value Iteration...&#39;)
        mdp_solution, hist = vi_solver.solve(model,
                                             use_gpu=use_gpu,
                                             print_progress=False)
        
        log(f&#39;    &gt; Value Iteration stopped or converged in {sum(hist.iteration_times):.3f}s, and after {len(hist.iteration_times)} iteration.\n&#39;)

        self.expand_function_params[&#39;mdp_policy&#39;] = mdp_solution

    # Convergence check boundary
    max_allowed_change = self.eps * (self.gamma / (1-self.gamma))

    # History tracking
    solver_history = SolverHistory(tracking_level=history_tracking_level,
                                   model=model,
                                   gamma=self.gamma,
                                   eps=self.eps,
                                   expand_function=self.expand_function,
                                   expand_append=full_backup,
                                   initial_value_function=value_function,
                                   initial_belief_set=belief_set)

    # Loop
    iteration = 0
    expand_value_function = value_function
    old_value_function = value_function

    try:
        for expansion_i in range(expansions) if not print_progress else trange(expansions, desc=&#39;Expansions&#39;):

            # 1: Expand belief set
            start_ts = datetime.now()

            new_belief_set = self.expand(model=model,
                                         belief_set=belief_set,
                                         value_function=value_function,
                                         max_generation=max_belief_growth,
                                         **self.expand_function_params)

            # If full backup append the newly generated set to the old belief_set
            if full_backup:
                belief_set = BeliefSet(model, xp.vstack((belief_set.belief_array, new_belief_set.belief_array)))
            else:
                belief_set = new_belief_set

            expand_time = (datetime.now() - start_ts).total_seconds()
            solver_history.add_expand_step(expansion_time=expand_time, belief_set=belief_set)

            # 2: Backup, update value function (alpha vector set)
            for _ in range(update_passes) if (not print_progress or update_passes &lt;= 1) else trange(update_passes, desc=f&#39;Backups {expansion_i}&#39;):
                start_ts = datetime.now()

                # Backup step
                value_function = self.backup(model,
                                             belief_set,
                                             value_function,
                                             append=(not full_backup),
                                             belief_dominance_prune=False)
                backup_time = (datetime.now() - start_ts).total_seconds()

                # Additional pruning
                if (iteration % prune_interval) == 0 and iteration &gt; 0:
                    start_ts = datetime.now()
                    vf_len = len(value_function)

                    value_function.prune(prune_level)

                    prune_time = (datetime.now() - start_ts).total_seconds()
                    alpha_vectors_pruned = len(value_function) - vf_len
                    solver_history.add_prune_step(prune_time, alpha_vectors_pruned)
                
                # Compute the change between value functions
                max_change = self.compute_change(value_function, old_value_function, belief_set)

                # History tracking
                solver_history.add_backup_step(backup_time, max_change, value_function)

                # Convergence check
                if max_change &lt; max_allowed_change:
                    break

                old_value_function = value_function

                # Update iteration counter
                iteration += 1

            # Compute change with old expansion value function
            expand_max_change = self.compute_change(expand_value_function, value_function, belief_set)

            if expand_max_change &lt; max_allowed_change:
                print(&#39;Converged!&#39;)
                break

            expand_value_function = value_function
    except MemoryError as e:
        print(f&#39;Memory full: {e}&#39;)
        print(&#39;Returning value function and history as is...\n&#39;)

    # Final pruning
    start_ts = datetime.now()
    vf_len = len(value_function)

    value_function.prune(prune_level)

    prune_time = (datetime.now() - start_ts).total_seconds()
    alpha_vectors_pruned = len(value_function) - vf_len
    solver_history.add_prune_step(prune_time, alpha_vectors_pruned)

    return value_function, solver_history</code></pre>
</details>
</dd>
<dt id="src.pomdp.PBVI_Solver.test_n_simulations"><code class="name flex">
<span>def <span class="ident">test_n_simulations</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, value_function: <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>, n: int = 1000, horizon: int = 300, print_progress: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Function that tests a value function with n simulations. It returns the start states, the amount of steps in which the simulation reached an end state, the rewards received and the discounted rewards received.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The model on which to run the simulations.</dd>
<dt><strong><code>value_function</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>The value function that will be evaluated.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code>, default=<code>1000</code></dt>
<dd>The amount of simulations to run.</dd>
<dt><strong><code>horizon</code></strong> :&ensp;<code>int</code>, default=<code>300</code></dt>
<dd>The maximum amount of steps the simulation can run for.</dd>
<dt><strong><code>print_progress</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Whether to display a progress bar of how many simulation steps have been run so far.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test_n_simulations(self, model:Model, value_function:ValueFunction, n:int=1000, horizon:int=300, print_progress:bool=False):
    &#39;&#39;&#39;
    Function that tests a value function with n simulations. It returns the start states, the amount of steps in which the simulation reached an end state, the rewards received and the discounted rewards received.

    Parameters
    ----------
    model : pomdp.Model
        The model on which to run the simulations.
    value_function : ValueFunction
        The value function that will be evaluated.
    n : int, default=1000
        The amount of simulations to run.
    horizon : int, default=300
        The maximum amount of steps the simulation can run for.
    print_progress : bool, default=False
        Whether to display a progress bar of how many simulation steps have been run so far. 
    &#39;&#39;&#39;
    # GPU support
    xp = np if not value_function.is_on_gpu else cp
    model = model.cpu_model if not value_function.is_on_gpu else model.gpu_model

    # Genetion of an array of n beliefs
    initial_beliefs = xp.repeat(Belief(model).values[None,:], n, axis=0)

    # Generating n initial positions
    start_states = xp.random.choice(model.states, size=n, p=model.start_probabilities)
    
    # Belief and state arrays
    beliefs = initial_beliefs
    new_beliefs = None
    states = start_states
    next_states = None

    # Tracking what simulations are done
    sim_is_done = xp.zeros(n, dtype=bool)
    done_at_step = xp.full(n, -1)

    # Speedup item
    simulations = xp.arange(n)
    flatten_offset = (simulations[:,None] * model.state_count)
    flat_shape = (n, (model.state_count * model.reachable_state_count))

    # 2D bincount for belief set update
    def bincount2D_vectorized(a, w):    
        a_offs = a + flatten_offset
        return xp.bincount(a_offs.ravel(), weights=w.ravel(), minlength=a.shape[0]*model.state_count).reshape(-1,model.state_count)

    # Results
    discount = self.gamma
    rewards = []
    discounted_rewards = []
    
    iterator = trange(horizon) if print_progress else range(horizon)
    for i in iterator:
        # Retrieving the top vectors according to the value function
        best_vectors = xp.argmax(xp.matmul(beliefs, value_function.alpha_vector_array.T), axis=1)

        # Retrieving the actions associated with the vectors chosen
        best_actions = value_function.actions[best_vectors]

        # Get each reachable next states for each action
        reachable_state_per_actions = model.reachable_states[:, best_actions, :]

        # Gathering new states based on the transition function and the chosen actions
        next_state_potentials = reachable_state_per_actions[states, simulations]
        if model.reachable_state_count == 1:
            next_states = next_state_potentials[:,0]
        else:
            potential_probabilities = model.reachable_probabilities[states[:,None], best_actions[:,None]][:,0,:]
            chosen_indices = xp.apply_along_axis(lambda x: xp.random.choice(len(x), size=1, p=x), axis=1, arr=potential_probabilities)
            next_states = next_state_potentials[chosen_indices][:,0,0]

        # Making observations based on the states landed in and the action that was taken
        observation_probabilities = model.observation_table[next_states[:,None], best_actions[:,None]][:,0,:]
        observations = xp.sum(xp.random.random(n)[:,None] &gt; xp.cumsum(observation_probabilities[:,:-1], axis=1), axis=1)

        # Belief set update
        reachable_probabilities = (model.reachable_transitional_observation_table[:,best_actions,observations,:] * beliefs.T[:,:,None])
        new_beliefs = bincount2D_vectorized(a=reachable_state_per_actions.swapaxes(0,1).reshape(flat_shape),
                                            w=reachable_probabilities.swapaxes(0,1).reshape(flat_shape))

        new_beliefs /= xp.sum(new_beliefs, axis=1)[:,None]

        # Rewards computation
        step_rewards = xp.array([model.immediate_reward_function(s,a,s_p,o) for s,a,s_p,o in zip(states, best_actions, next_states, observations)])
        rewards.append(xp.where(~sim_is_done, step_rewards, 0))
        discounted_rewards.append(xp.where(~sim_is_done, step_rewards * discount, 0))

        # Checking for done condition
        are_done = xp.isin(next_states, xp.array(model.end_states))
        done_at_step[sim_is_done ^ are_done] = i+1
        sim_is_done |= are_done

        # Update iterator postfix
        if print_progress:
            iterator.set_postfix({&#39;done&#39;: xp.sum(sim_is_done)})

        # Replacing old with new
        states = next_states
        beliefs = new_beliefs
        discount *= self.gamma

        # Early stopping
        if xp.all(sim_is_done):
            break

    return start_states, done_at_step, rewards, discounted_rewards</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.pomdp.Simulation"><code class="flex name class">
<span>class <span class="ident">Simulation</span></span>
<span>(</span><span>model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to reprensent a simulation process for a POMDP model.
An initial random state is given and action can be applied to the model that impact the actual state of the agent along with returning a reward and an observation.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The POMDP model the simulation will be applied on.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>agent_state</code></strong> :&ensp;<code>int</code></dt>
<dd>The agent's state in the running simulation</dd>
<dt><strong><code>is_done</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether or not the agent has reached an end state or performed an ending action.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Simulation(MDP_Simulation):
    &#39;&#39;&#39;
    Class to reprensent a simulation process for a POMDP model.
    An initial random state is given and action can be applied to the model that impact the actual state of the agent along with returning a reward and an observation.

    ...

    Parameters
    ----------
    model: pomdp.Model
        The POMDP model the simulation will be applied on.

    Attributes
    ----------
    model: pomdp.Model
    agent_state : int
        The agent&#39;s state in the running simulation
    is_done : bool
        Whether or not the agent has reached an end state or performed an ending action.
    &#39;&#39;&#39;
    def __init__(self, model:Model) -&gt; None:
        super().__init__(model)
        self.model = model


    def run_action(self, a:int) -&gt; tuple[Union[int,float], int]:
        &#39;&#39;&#39;
        Run one step of simulation with action a.

        Parameters
        ----------
        a : int
            The action to take in the simulation.

        Returns
        -------
        r : int or float
            The reward given when doing action a in state s and landing in state s_p. (s and s_p are hidden from agent)
        o : int
            The observation following the action applied on the previous state.
        &#39;&#39;&#39;
        assert not self.is_done, &#34;Action run when simulation is done.&#34;

        s = self.agent_state
        s_p = self.model.transition(s, a)
        o = self.model.observe(s_p, a)
        r = self.model.reward(s, a, s_p, o)

        # Update agent state
        self.agent_state = s_p

        # State Done check
        if s_p in self.model.end_states:
            self.is_done = True

        # Action Done check
        if a in self.model.end_actions:
            self.is_done = True

        return (r, o)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="src.mdp.Simulation" href="mdp.html#src.mdp.Simulation">Simulation</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.pomdp.Simulation.run_action"><code class="name flex">
<span>def <span class="ident">run_action</span></span>(<span>self, a: int) ‑> tuple[typing.Union[int, float], int]</span>
</code></dt>
<dd>
<div class="desc"><p>Run one step of simulation with action a.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>a</code></strong> :&ensp;<code>int</code></dt>
<dd>The action to take in the simulation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>r</code></strong> :&ensp;<code>int</code> or <code>float</code></dt>
<dd>The reward given when doing action a in state s and landing in state s_p. (s and s_p are hidden from agent)</dd>
<dt><strong><code>o</code></strong> :&ensp;<code>int</code></dt>
<dd>The observation following the action applied on the previous state.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_action(self, a:int) -&gt; tuple[Union[int,float], int]:
    &#39;&#39;&#39;
    Run one step of simulation with action a.

    Parameters
    ----------
    a : int
        The action to take in the simulation.

    Returns
    -------
    r : int or float
        The reward given when doing action a in state s and landing in state s_p. (s and s_p are hidden from agent)
    o : int
        The observation following the action applied on the previous state.
    &#39;&#39;&#39;
    assert not self.is_done, &#34;Action run when simulation is done.&#34;

    s = self.agent_state
    s_p = self.model.transition(s, a)
    o = self.model.observe(s_p, a)
    r = self.model.reward(s, a, s_p, o)

    # Update agent state
    self.agent_state = s_p

    # State Done check
    if s_p in self.model.end_states:
        self.is_done = True

    # Action Done check
    if a in self.model.end_actions:
        self.is_done = True

    return (r, o)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="src.mdp.Simulation" href="mdp.html#src.mdp.Simulation">Simulation</a></b></code>:
<ul class="hlist">
<li><code><a title="src.mdp.Simulation.initialize_simulation" href="mdp.html#src.mdp.Simulation.initialize_simulation">initialize_simulation</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="src.pomdp.SimulationHistory"><code class="flex name class">
<span>class <span class="ident">SimulationHistory</span></span>
<span>(</span><span>model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, start_state: int, start_belief: <a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to represent a list of rewards received during a Simulation.
The main purpose of the class is to provide a set of visualization options of the rewards received.</p>
<p>Multiple types of plots can be done:
- Totals: to plot a graph of the accumulated rewards over time.
- Moving average: to plot the moving average of the rewards received over time.
- Histogram: to plot a histogram of the various rewards received.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>The model on which the simulation happened on.</dd>
<dt><strong><code>start_state</code></strong> :&ensp;<code>int</code></dt>
<dd>The initial state in the simulation.</dd>
<dt><strong><code>start_belief</code></strong> :&ensp;<code><a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></code></dt>
<dd>The initial belief the agent starts with during the simulation.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>mdp.Model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>states</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>A list of recorded states through which the agent passed by during the simulation process.</dd>
<dt><strong><code>grid_point_sequence</code></strong> :&ensp;<code>list[list[int]]</code></dt>
<dd>A list of 2D points of the grid state through which the agent passed by during the simulation process.</dd>
<dt><strong><code>actions</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>A list of recorded actions the agent took during the simulation process.</dd>
<dt><strong><code>rewards</code></strong> :&ensp;<code>RewardSet</code></dt>
<dd>The set of rewards received by the agent throughout the simulation process.</dd>
<dt><strong><code>beliefs</code></strong> :&ensp;<code>list[<a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>]</code></dt>
<dd>A list of recorded beliefs the agent is in throughout the simulation process.</dd>
<dt><strong><code>observations</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>A list of recorded observations gotten by the agent during the simulation process.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SimulationHistory(MDP_SimulationHistory):
    &#39;&#39;&#39;
    Class to represent a list of rewards received during a Simulation.
    The main purpose of the class is to provide a set of visualization options of the rewards received.

    Multiple types of plots can be done:
        - Totals: to plot a graph of the accumulated rewards over time.
        - Moving average: to plot the moving average of the rewards received over time.
        - Histogram: to plot a histogram of the various rewards received.

    ...

    Parameters
    ----------
    model: mdp.Model
        The model on which the simulation happened on.
    start_state: int
        The initial state in the simulation.
    start_belief: Belief
        The initial belief the agent starts with during the simulation.

    Attributes
    ----------
    model : mdp.Model
    states : list[int]
        A list of recorded states through which the agent passed by during the simulation process.
    grid_point_sequence : list[list[int]]
        A list of 2D points of the grid state through which the agent passed by during the simulation process.
    actions : list[int]
        A list of recorded actions the agent took during the simulation process.
    rewards: RewardSet
        The set of rewards received by the agent throughout the simulation process.
    beliefs : list[Belief]
        A list of recorded beliefs the agent is in throughout the simulation process.
    observations : list[int]
        A list of recorded observations gotten by the agent during the simulation process.
    &#39;&#39;&#39;
    def __init__(self, model:Model, start_state:int, start_belief:Belief):
        super().__init__(model, start_state)
        self._beliefs = [start_belief]
        self.observations = []


    @property
    def beliefs(self) -&gt; list[Belief]:
        if len(self._beliefs) &lt; len(self):
            init_belief = self._beliefs[0]

            self._beliefs = [init_belief]

            # Generation of belief sequence based on actions and observations
            belief = init_belief
            for a, o in zip(self.actions, self.observations):
                new_belief = belief.update(a,o)
                self._beliefs.append(new_belief)
                belief = new_belief

        return self._beliefs


    def add(self, action:int, reward, next_state:int, next_belief:Belief, observation:int) -&gt; None:
        &#39;&#39;&#39;
        Function to add a step in the simulation history

        Parameters
        ----------
        action : int
            The action that was taken by the agent.
        reward
            The reward received by the agent after having taken action.
        next_state : int
            The state that was reached by the agent after having taken action.
        next_belief : Belief
            The new belief of the agent after having taken an action and received an observation.
        observation : int
            The observation the agent received after having made an action.
        &#39;&#39;&#39;
        super().add(action, reward, next_state)
        self._beliefs.append(next_belief)
        self.observations.append(observation)


    # Overwritten
    def to_dataframe(self, include_beliefs:bool=False) -&gt; pd.DataFrame:
        &#39;&#39;&#39;
        Returns a pandas dataframe representation of the simulation history.

        Note: Beliefs not saved as the sequence can be recreated from the sequence of action-observation pairs.

        Parameters
        ----------
        include_beliefs : bool, default=False # TODO: implement option
            Whether or not to include the beliefs in the dataframe or not. Doing so requires potential a large amount of memory.
        &#39;&#39;&#39;
        df = super().to_dataframe()
        df[&#39;Observations&#39;] = self.observations + [None]

        if include_beliefs:
            belief_array = np.array([b.values.tolist() for b in self.beliefs])
            belief_df = pd.DataFrame(belief_array, columns=[f&#39;B_{sl}&#39; for sl in self.model.state_labels])

            df = pd.concat([df, belief_df], axis=1)

        return df
    

    # Overwritten
    def save(self, path:str=&#39;./Simulations&#39;, file_name:Union[str,None]=None, include_beliefs:bool=False) -&gt; None:
        &#39;&#39;&#39;
        Function to save the simulation history in a file at a given path. If no path is provided, it will be saved in a subfolder (Simulations) inside the current working directory.
        If no file_name is provided, it be saved as &#39;&lt;current_timestamp&gt;_simulation.csv&#39;.

        Parameters
        ----------
        path : str, default=&#39;./Simulations&#39;
            The path at which the csv will be saved.
        file_name : str, default=&#39;&lt;current_timestamp&gt;_simulation.csv&#39;
            The file name used to save in.
        &#39;&#39;&#39;
        if not os.path.exists(path):
            print(&#39;Path &#34;{path}&#34; does not exist yet, creating it...&#39;)
            os.makedirs(path)
            
        if file_name is None:
            timestamp = datetime.now().strftime(&#39;%Y%m%d_%H%M%S&#39;)
            file_name = timestamp + &#39;_simulation.csv&#39;

        if not file_name.endswith(&#39;.csv&#39;):
            file_name += &#39;.csv&#39;
        
        if not include_beliefs:
            print(&#39;[Warning] Beliefs not saved with simulation history but the belief sequence can be recreated from the actions and observations.&#39;)

        df = self.to_dataframe(include_beliefs=include_beliefs)

        df.to_csv(path + &#39;/&#39; + file_name, index=False)
        print(f&#39;Saved to: {path}/{file_name}&#39;)
    

    # Overwritten
    def _plot_to_frame_on_ax(self, frame_i, ax):
        model = self.model.cpu_model

        # Data
        data = np.array(self.grid_point_sequence)[:(frame_i+1),:]
        belief = self.beliefs[frame_i]
        belief_values = belief.values if (not gpu_support) or (cp.get_array_module(belief.values) == np) else cp.asnumpy(belief.values)
        observations = self.observations[:(frame_i)]
        obs_colors = [&#39;#000000&#39;] + [COLOR_LIST[o][&#39;hex&#39;] for o in observations]

        # Ticks
        dimensions = model.state_grid.shape
        x_ticks = np.arange(0, dimensions[1], (1 if dimensions[1] &lt; 10 else int(dimensions[1] / 10)))
        y_ticks = np.arange(0, dimensions[0], (1 if dimensions[0] &lt; 5 else int(dimensions[0] / 5)))

        # Plotting
        ax.clear()
        ax.set_title(f&#39;Simulation (Frame {frame_i})&#39;)

        # Observation labels legend
        proxy = [patches.Rectangle((0,0),1,1,fc = COLOR_LIST[o][&#39;id&#39;]) for o in model.observations]
        ax.legend(proxy, model.observation_labels, title=&#39;Observations&#39;) # type: ignore

        grid_values = belief_values[model.state_grid]
        ax.imshow(grid_values, cmap=&#39;Blues&#39;)
        ax.plot(data[:,1], data[:,0], color=&#39;red&#39;, zorder=-1)
        ax.scatter(data[:,1], data[:,0], c=obs_colors)

        ax.set_xticks(x_ticks)
        ax.set_yticks(y_ticks)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="src.mdp.SimulationHistory" href="mdp.html#src.mdp.SimulationHistory">SimulationHistory</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="src.pomdp.SimulationHistory.beliefs"><code class="name">var <span class="ident">beliefs</span> : list[<a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>]</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def beliefs(self) -&gt; list[Belief]:
    if len(self._beliefs) &lt; len(self):
        init_belief = self._beliefs[0]

        self._beliefs = [init_belief]

        # Generation of belief sequence based on actions and observations
        belief = init_belief
        for a, o in zip(self.actions, self.observations):
            new_belief = belief.update(a,o)
            self._beliefs.append(new_belief)
            belief = new_belief

    return self._beliefs</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.pomdp.SimulationHistory.add"><code class="name flex">
<span>def <span class="ident">add</span></span>(<span>self, action: int, reward, next_state: int, next_belief: <a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a>, observation: int) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to add a step in the simulation history</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>action</code></strong> :&ensp;<code>int</code></dt>
<dd>The action that was taken by the agent.</dd>
<dt><strong><code>reward</code></strong></dt>
<dd>The reward received by the agent after having taken action.</dd>
<dt><strong><code>next_state</code></strong> :&ensp;<code>int</code></dt>
<dd>The state that was reached by the agent after having taken action.</dd>
<dt><strong><code>next_belief</code></strong> :&ensp;<code><a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></code></dt>
<dd>The new belief of the agent after having taken an action and received an observation.</dd>
<dt><strong><code>observation</code></strong> :&ensp;<code>int</code></dt>
<dd>The observation the agent received after having made an action.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add(self, action:int, reward, next_state:int, next_belief:Belief, observation:int) -&gt; None:
    &#39;&#39;&#39;
    Function to add a step in the simulation history

    Parameters
    ----------
    action : int
        The action that was taken by the agent.
    reward
        The reward received by the agent after having taken action.
    next_state : int
        The state that was reached by the agent after having taken action.
    next_belief : Belief
        The new belief of the agent after having taken an action and received an observation.
    observation : int
        The observation the agent received after having made an action.
    &#39;&#39;&#39;
    super().add(action, reward, next_state)
    self._beliefs.append(next_belief)
    self.observations.append(observation)</code></pre>
</details>
</dd>
<dt id="src.pomdp.SimulationHistory.to_dataframe"><code class="name flex">
<span>def <span class="ident">to_dataframe</span></span>(<span>self, include_beliefs: bool = False) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a pandas dataframe representation of the simulation history.</p>
<p>Note: Beliefs not saved as the sequence can be recreated from the sequence of action-observation pairs.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>include_beliefs</code></strong> :&ensp;<code>bool</code>, default=<code>False # TODO: implement option</code></dt>
<dd>Whether or not to include the beliefs in the dataframe or not. Doing so requires potential a large amount of memory.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dataframe(self, include_beliefs:bool=False) -&gt; pd.DataFrame:
    &#39;&#39;&#39;
    Returns a pandas dataframe representation of the simulation history.

    Note: Beliefs not saved as the sequence can be recreated from the sequence of action-observation pairs.

    Parameters
    ----------
    include_beliefs : bool, default=False # TODO: implement option
        Whether or not to include the beliefs in the dataframe or not. Doing so requires potential a large amount of memory.
    &#39;&#39;&#39;
    df = super().to_dataframe()
    df[&#39;Observations&#39;] = self.observations + [None]

    if include_beliefs:
        belief_array = np.array([b.values.tolist() for b in self.beliefs])
        belief_df = pd.DataFrame(belief_array, columns=[f&#39;B_{sl}&#39; for sl in self.model.state_labels])

        df = pd.concat([df, belief_df], axis=1)

    return df</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="src.mdp.SimulationHistory" href="mdp.html#src.mdp.SimulationHistory">SimulationHistory</a></b></code>:
<ul class="hlist">
<li><code><a title="src.mdp.SimulationHistory.plot_simulation_steps" href="mdp.html#src.mdp.SimulationHistory.plot_simulation_steps">plot_simulation_steps</a></code></li>
<li><code><a title="src.mdp.SimulationHistory.save" href="mdp.html#src.mdp.SimulationHistory.save">save</a></code></li>
<li><code><a title="src.mdp.SimulationHistory.save_simulation_video" href="mdp.html#src.mdp.SimulationHistory.save_simulation_video">save_simulation_video</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="src.pomdp.Solver"><code class="flex name class">
<span>class <span class="ident">Solver</span></span>
</code></dt>
<dd>
<div class="desc"><p>POMDP Model Solver - abstract class</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Solver(MDP_Solver):
    &#39;&#39;&#39;
    POMDP Model Solver - abstract class
    &#39;&#39;&#39;
    def solve(self, model: Model) -&gt; tuple[ValueFunction, SolverHistory]:
        raise Exception(&#34;Method has to be implemented by subclass...&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="src.mdp.Solver" href="mdp.html#src.mdp.Solver">Solver</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="src.pomdp.PBVI_Solver" href="#src.pomdp.PBVI_Solver">PBVI_Solver</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.pomdp.Solver.solve"><code class="name flex">
<span>def <span class="ident">solve</span></span>(<span>self, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>) ‑> tuple[<a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>, <a title="src.pomdp.SolverHistory" href="#src.pomdp.SolverHistory">SolverHistory</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def solve(self, model: Model) -&gt; tuple[ValueFunction, SolverHistory]:
    raise Exception(&#34;Method has to be implemented by subclass...&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.pomdp.SolverHistory"><code class="flex name class">
<span>class <span class="ident">SolverHistory</span></span>
<span>(</span><span>tracking_level: int, model: <a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a>, gamma: float, eps: float, expand_function: str, expand_append: bool, initial_value_function: <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>, initial_belief_set: <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to represent the history of a solver for a POMDP solver.
It has mainly the purpose to have visualizations for the solution, belief set and the whole solving history.
The visualizations available are:
- Belief set plot
- Solution plot
- Video of value function and belief set evolution over training.</p>
<p>&hellip;</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tracking_level</code></strong> :&ensp;<code>int</code></dt>
<dd>The tracking level of the solver.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>The model the solver has solved.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>The gamma parameter used by the solver (learning rate).</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code></dt>
<dd>The epsilon parameter used by the solver (covergence bound).</dd>
<dt><strong><code>expand_function</code></strong> :&ensp;<code>str</code></dt>
<dd>The expand (exploration) function used by the solver.</dd>
<dt><strong><code>expand_append</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether the expand function appends new belief points to the belief set of reloads it all.</dd>
<dt><strong><code>initial_value_function</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>The initial value function the solver will use to start the solving process.</dd>
<dt><strong><code>initial_belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>The initial belief set the solver will use to start the solving process.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>tracking_level</code></strong> :&ensp;<code>int</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>pomdp.Model</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>expand_function</code></strong> :&ensp;<code>str</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>expand_append</code></strong> :&ensp;<code>bool</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>run_ts</code></strong> :&ensp;<code>datetime</code></dt>
<dd>The time at which the SolverHistory object was instantiated which is assumed to be the start of the solving run.</dd>
<dt><strong><code>expansion_times</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>A list of recorded times of the expand function.</dd>
<dt><strong><code>backup_times</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>A list of recorded times of the backup function.</dd>
<dt><strong><code>alpha_vector_counts</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>A list of recorded alpha vector count making up the value function over the solving process.</dd>
<dt><strong><code>beliefs_counts</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>A list of recorded belief count making up the belief set over the solving process.</dd>
<dt><strong><code>value_function_changes</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>A list of recorded value function changes (the maximum changed value between 2 value functions).</dd>
<dt><strong><code>value_functions</code></strong> :&ensp;<code>list[ValueFunction]</code></dt>
<dd>A list of recorded value functions.</dd>
<dt><strong><code>belief_sets</code></strong> :&ensp;<code>list[<a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a>]</code></dt>
<dd>A list of recorded belief sets.</dd>
<dt><strong><code>solution</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>explored_beliefs</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SolverHistory:
    &#39;&#39;&#39;
    Class to represent the history of a solver for a POMDP solver.
    It has mainly the purpose to have visualizations for the solution, belief set and the whole solving history.
    The visualizations available are:
        - Belief set plot
        - Solution plot
        - Video of value function and belief set evolution over training.

    ...

    Parameters
    ----------
    tracking_level : int
        The tracking level of the solver.
    model : pomdp.Model
        The model the solver has solved.
    gamma : float
        The gamma parameter used by the solver (learning rate).
    eps : float
        The epsilon parameter used by the solver (covergence bound).
    expand_function : str
        The expand (exploration) function used by the solver.
    expand_append : bool
        Whether the expand function appends new belief points to the belief set of reloads it all.
    initial_value_function : ValueFunction
        The initial value function the solver will use to start the solving process.
    initial_belief_set : BeliefSet
        The initial belief set the solver will use to start the solving process.

    Attributes
    ----------
    tracking_level : int
    model : pomdp.Model
    gamma : float
    eps : float
    expand_function : str
    expand_append : bool
    run_ts : datetime
        The time at which the SolverHistory object was instantiated which is assumed to be the start of the solving run.
    expansion_times : list[float]
        A list of recorded times of the expand function.
    backup_times : list[float]
        A list of recorded times of the backup function.
    alpha_vector_counts : list[int]
        A list of recorded alpha vector count making up the value function over the solving process.
    beliefs_counts : list[int]
        A list of recorded belief count making up the belief set over the solving process.
    value_function_changes : list[float]
        A list of recorded value function changes (the maximum changed value between 2 value functions).
    value_functions : list[ValueFunction]
        A list of recorded value functions.
    belief_sets : list[BeliefSet]
        A list of recorded belief sets.
    solution : ValueFunction
    explored_beliefs : BeliefSet
    &#39;&#39;&#39;
    def __init__(self,
                 tracking_level:int,
                 model:Model,
                 gamma:float,
                 eps:float,
                 expand_function:str,
                 expand_append:bool,
                 initial_value_function:ValueFunction,
                 initial_belief_set:BeliefSet
                 ):
        
        self.tracking_level = tracking_level
        self.model = model
        self.gamma = gamma
        self.eps = eps
        self.run_ts = datetime.now()
        
        self.expand_function = expand_function
        self.expand_append = expand_append

        # Time tracking
        self.expansion_times = []
        self.backup_times = []
        self.pruning_times = []

        # Value function and belief set sizes tracking
        self.alpha_vector_counts = []
        self.beliefs_counts = []
        self.prune_counts = []

        if self.tracking_level &gt;= 1:
            self.alpha_vector_counts.append(len(initial_value_function))
            self.beliefs_counts.append(len(initial_belief_set))

        # Value function and belief set tracking
        self.belief_sets = []
        self.value_functions = []
        self.value_function_changes = []

        if self.tracking_level &gt;= 2:
            self.belief_sets.append(initial_belief_set)
            self.value_functions.append(initial_value_function)


    @property
    def solution(self) -&gt; ValueFunction:
        &#39;&#39;&#39;
        The last value function of the solving process.
        &#39;&#39;&#39;
        assert self.tracking_level &gt;= 2, &#34;Tracking level is set too low, increase it to 2 if you want to have value function tracking as well.&#34;
        return self.value_functions[-1]
    

    @property
    def explored_beliefs(self) -&gt; BeliefSet:
        &#39;&#39;&#39;
        The final set of beliefs explored during the solving.
        &#39;&#39;&#39;
        assert self.tracking_level &gt;= 2, &#34;Tracking level is set too low, increase it to 2 if you want to have belief sets tracking as well.&#34;
        return self.belief_sets[-1]
    

    def add_expand_step(self,
                        expansion_time:float,
                        belief_set:BeliefSet
                        ) -&gt; None:
        &#39;&#39;&#39;
        Function to add an expansion step in the simulation history by the explored belief set the expand function generated.

        Parameters
        ----------
        expansion_time : float
            The time it took to run a step of expansion of the belief set. (Also known as the exploration step.)
        belief_set : BeliefSet
            The belief set used for the Update step of the solving process.
        &#39;&#39;&#39;
        if self.tracking_level &gt;= 1:
            self.expansion_times.append(float(expansion_time))
            self.beliefs_counts.append(len(belief_set))

        if self.tracking_level &gt;= 2:
            self.belief_sets.append(belief_set if not belief_set.is_on_gpu else belief_set.to_cpu())


    def add_backup_step(self,
                        backup_time:float,
                        value_function_change:float,
                        value_function:ValueFunction
                        ) -&gt; None:
        &#39;&#39;&#39;
        Function to add a backup step in the simulation history by recording the value function the backup function generated.

        Parameters
        ----------
        backup_time : float
            The time it took to run a step of backup of the value function. (Also known as the value function update.)
        value_function_change : float
            The change between the value function of this iteration and of the previous iteration.
        value_function : ValueFunction
            The value function resulting after a step of the solving process.
        &#39;&#39;&#39;
        if self.tracking_level &gt;= 1:
            self.backup_times.append(float(backup_time))
            self.alpha_vector_counts.append(len(value_function))
            self.value_function_changes.append(float(value_function_change))

        if self.tracking_level &gt;= 2:
            self.value_functions.append(value_function if not value_function.is_on_gpu else value_function.to_cpu())


    def add_prune_step(self,
                       prune_time:float,
                       alpha_vectors_pruned:int
                       ) -&gt; None:
        &#39;&#39;&#39;
        Function to add a prune step in the simulation history by recording the amount of alpha vectors that were pruned by the pruning function and how long it took.

        Parameters
        ----------
        prune_time : float
            The time it took to run the pruning step.
        alpha_vectors_pruned : int
            How many alpha vectors were pruned.
        &#39;&#39;&#39;
        if self.tracking_level &gt;= 1:
            self.pruning_times.append(prune_time)
            self.prune_counts.append(alpha_vectors_pruned)


    @property
    def summary(self) -&gt; str:
        &#39;&#39;&#39;
        A summary as a string of the information recorded.

        Returns
        -------
        summary_str : str
            The summary of the information.
        &#39;&#39;&#39;
        summary_str =  f&#39;Summary of Value Iteration run&#39;
        summary_str += f&#39;\n  - Model: {self.model.state_count} state, {self.model.action_count} action, {self.model.observation_count} observations&#39;
        summary_str += f&#39;\n  - Converged or stopped after {len(self.expansion_times)} expansion steps and {len(self.backup_times)} backup steps.&#39;

        if self.tracking_level &gt;= 1:
            summary_str += f&#39;\n  - Resulting value function has {self.alpha_vector_counts[-1]} alpha vectors.&#39;
            summary_str += f&#39;\n  - Converged in {(sum(self.expansion_times) + sum(self.backup_times)):.4f}s&#39;
            summary_str += f&#39;\n&#39;

            summary_str += f&#39;\n  - Expand function took on average {sum(self.expansion_times) / len(self.expansion_times):.4f}s &#39;
            if self.expand_append:
                summary_str += f&#39;and yielded on average {sum(np.diff(self.beliefs_counts)) / len(self.beliefs_counts[1:]):.2f} beliefs per iteration.&#39;
            else:
                summary_str += f&#39;and yielded on average {sum(self.beliefs_counts[1:]) / len(self.beliefs_counts[1:]):.2f} beliefs per iteration.&#39;
            summary_str += f&#39; ({np.sum(np.divide(self.expansion_times, self.beliefs_counts[1:])) / len(self.expansion_times):.4f}s/it/belief)&#39;
            
            summary_str += f&#39;\n  - Backup function took on average {sum(self.backup_times) /len(self.backup_times):.4f}s &#39;
            summary_str += f&#39;and yielded on average value functions of size {sum(self.alpha_vector_counts[1:]) / len(self.alpha_vector_counts[1:]):.2f} per iteration.&#39;
            summary_str += f&#39; ({np.sum(np.divide(self.backup_times, self.alpha_vector_counts[1:])) / len(self.backup_times):.4f}s/it/alpha)&#39;

            summary_str += f&#39;\n  - Pruning function took on average {sum(self.pruning_times) /len(self.pruning_times):.4f}s &#39;
            summary_str += f&#39;and yielded on average prunings of {sum(self.prune_counts) / len(self.prune_counts):.2f} alpha vectors per iteration.&#39;
        
        return summary_str
    

    def plot_belief_set(self, size:int=15) -&gt; None:
        &#39;&#39;&#39;
        Function to plot the last belief set explored during the solving process.

        Parameters
        ----------
        size : int, default=15
            The scale of the plot.
        &#39;&#39;&#39;
        self.explored_beliefs.plot(size=size)


    def plot_solution(self, size:int=5, plot_belief:bool=True) -&gt; None:
        &#39;&#39;&#39;
        Function to plot the value function of the solution.
        Note: only works for 2 and 3 states models

        Parameters
        ----------
        size : int, default=5
            The figure size and general scaling factor.
        plot_belief : bool, default=True
            Whether to plot the belief set along with the value function.
        &#39;&#39;&#39;
        self.solution.plot(size=size, belief_set=(self.explored_beliefs if plot_belief else None))


    def save_history_video(self,
                           custom_name:Union[str,None]=None,
                           compare_with:Union[list, ValueFunction, MDP_SolverHistory]=[],
                           graph_names:list[str]=[],
                           fps:int=10
                           ) -&gt; None:
        &#39;&#39;&#39;
        Function to generate a video of the training history. Another solved solver or list of solvers can be put in the &#39;compare_with&#39; parameter.
        These other solver&#39;s value function will be overlapped with the 1st value function.
        The explored beliefs of the main solver are also mapped out. (other solvers&#39;s explored beliefs will not be plotted)
        Also, a single value function or list of value functions can be sent in but they will be fixed in the video.

        Note: only works for 2-state models.

        Parameters
        ----------
        custom_name : str, optional
            The name the video will be saved with.
        compare_with : PBVI or ValueFunction or list, default=[]
            Value functions or other solvers to plot against the current solver&#39;s history.
        graph_names : list[str], default=[]
            Names of the graphs for the legend of which graph is being plot.
        fps : int, default=10
            How many frames per second should the saved video have.
        &#39;&#39;&#39;
        assert self.tracking_level &gt;= 2, &#34;Tracking level is set too low, increase it to 2 if you want to have value function and belief sets tracking as well.&#34;
        assert self.model.state_count in [2,3], &#34;Can&#39;t plot for models with state count other than 2 or 3&#34; # TODO Make support for gird videos

        if self.model.state_count == 2:
            self._save_history_video_2D(custom_name, compare_with, copy.copy(graph_names), fps)
        elif self.model.state_count == 3:
            raise Exception(&#39;Not implemented...&#39;)


    def _save_history_video_2D(self, custom_name=None, compare_with=[], graph_names=[], fps=10):
        # Figure definition
        grid_spec = {&#39;height_ratios&#39;: [19,1]}
        fig, (ax1,ax2) = plt.subplots(2, 1, sharex=True, gridspec_kw=grid_spec)

        # Figure title
        fig.suptitle(f&#34;{self.model.state_count}-s {self.model.action_count}-a {self.model.observation_count}-o POMDP model solve history&#34;, fontsize=16)
        title = f&#39;{self.expand_function} expand strat, {self.gamma}-gamma, {self.eps}-eps &#39;

        # X-axis setting
        ticks = [0,0.25,0.5,0.75,1]
        x_ticks = [str(t) for t in ticks]
        x_ticks[0] = self.model.state_labels[0]
        x_ticks[-1] = self.model.state_labels[1]

        # Colors and lines
        line_types = [&#39;-&#39;, &#39;--&#39;, &#39;-.&#39;, &#39;:&#39;]

        proxy = [Rectangle((0,0),1,1,fc = COLOR_LIST[a][&#39;id&#39;]) for a in self.model.actions]

        # Solver list
        if isinstance(compare_with, ValueFunction) or isinstance(compare_with, MDP_SolverHistory):
            compare_with_list = [compare_with] # Single item
        else:
            compare_with_list = compare_with # Already group of items
        solver_histories = [self] + compare_with_list
        
        assert len(solver_histories) &lt;= len(line_types), f&#34;Plotting can only happen for up to {len(line_types)} solvers...&#34;
        line_types = line_types[:len(solver_histories)]

        assert len(graph_names) in [0, len(solver_histories)], &#34;Not enough graph names provided&#34;
        if len(graph_names) == 0:
            graph_names.append(&#39;Main graph&#39;)
            for i in range(1,len(solver_histories)):
                graph_names.append(f&#39;Comparison {i}&#39;)

        def plot_on_ax(history:Union[ValueFunction,&#39;SolverHistory&#39;], frame_i:int, ax, line_type:str):
            if isinstance(history, ValueFunction):
                value_function = history
            else:
                frame_i = frame_i if frame_i &lt; len(history.value_functions) else (len(history.value_functions) - 1)
                value_function = history.value_functions[frame_i]

            alpha_vects = value_function.alpha_vector_array
            m = np.subtract(alpha_vects[:,1], alpha_vects[:,0])
            m = m.reshape(m.shape[0],1)

            x = np.linspace(0, 1, 100)
            x = x.reshape((1,x.shape[0])).repeat(m.shape[0],axis=0)
            y = np.add((m*x), alpha_vects[:,0].reshape(m.shape[0],1))

            for i, alpha in enumerate(value_function.alpha_vector_list):
                ax.plot(x[i,:], y[i,:], line_type, color=COLOR_LIST[alpha.action][&#39;id&#39;])

        def plt_frame(frame_i):
            ax1.clear()
            ax2.clear()

            # Axes labels
            ax1.set_ylabel(&#39;V(b)&#39;)
            ax2.set_xlabel(&#39;Belief space&#39;)

            self_frame_i = frame_i if frame_i &lt; len(self.value_functions) else (len(self.value_functions) - 1)

            # Subtitle
            ax1.set_title(title + f&#39;(Frame {frame_i})&#39;)

            # Color legend
            leg1 = ax1.legend(proxy, self.model.action_labels, loc=&#39;upper center&#39;)
            ax1.set_xticks(ticks, x_ticks)
            ax1.add_artist(leg1)

            # Line legend
            lines = []
            point = self.value_functions[self_frame_i].alpha_vector_array[0,0]
            for l in line_types:
                lines.append(Line2D([0,point],[0,point],linestyle=l))
            ax1.legend(lines, graph_names, loc=&#39;lower center&#39;)

            # Alpha vector plotting
            for history, line_type in zip(solver_histories, line_types):
                plot_on_ax(history, frame_i, ax1, line_type)

            # Belief plotting
            beliefs_x = self.belief_sets[frame_i if frame_i &lt; len(self.belief_sets) else -1].belief_array[:,1]
            ax2.scatter(beliefs_x, np.zeros(beliefs_x.shape[0]), c=&#39;red&#39;)
            ax2.get_yaxis().set_visible(False)
            ax2.axhline(0, color=&#39;black&#39;)

        max_steps = max([len(history.value_functions) for history in solver_histories if not isinstance(history,ValueFunction)])
        ani = animation.FuncAnimation(fig, plt_frame, frames=max_steps, repeat=False)
        
        # File Title
        solved_time = self.run_ts.strftime(&#39;%Y%m%d_%H%M%S&#39;)

        video_title = f&#39;{custom_name}-&#39; if custom_name is not None else &#39;&#39; # Base
        video_title += f&#39;s{self.model.state_count}-a{self.model.action_count}-&#39; # Model params
        video_title += f&#39;{self.expand_function}-&#39; # Expand function used
        video_title += f&#39;g{self.gamma}-e{self.eps}-&#39; # Solving params
        video_title += f&#39;{solved_time}.mp4&#39;

        # Video saving
        if not os.path.exists(&#39;./Results&#39;):
            print(&#39;Folder does not exist yet, creating it...&#39;)
            os.makedirs(&#39;./Results&#39;)

        writervideo = animation.FFMpegWriter(fps=fps)
        ani.save(&#39;./Results/&#39; + video_title, writer=writervideo)
        print(f&#39;Video saved at \&#39;Results/{video_title}\&#39;...&#39;)
        plt.close()</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="src.pomdp.SolverHistory.explored_beliefs"><code class="name">var <span class="ident">explored_beliefs</span> : <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>
<div class="desc"><p>The final set of beliefs explored during the solving.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def explored_beliefs(self) -&gt; BeliefSet:
    &#39;&#39;&#39;
    The final set of beliefs explored during the solving.
    &#39;&#39;&#39;
    assert self.tracking_level &gt;= 2, &#34;Tracking level is set too low, increase it to 2 if you want to have belief sets tracking as well.&#34;
    return self.belief_sets[-1]</code></pre>
</details>
</dd>
<dt id="src.pomdp.SolverHistory.solution"><code class="name">var <span class="ident">solution</span> : <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a></code></dt>
<dd>
<div class="desc"><p>The last value function of the solving process.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def solution(self) -&gt; ValueFunction:
    &#39;&#39;&#39;
    The last value function of the solving process.
    &#39;&#39;&#39;
    assert self.tracking_level &gt;= 2, &#34;Tracking level is set too low, increase it to 2 if you want to have value function tracking as well.&#34;
    return self.value_functions[-1]</code></pre>
</details>
</dd>
<dt id="src.pomdp.SolverHistory.summary"><code class="name">var <span class="ident">summary</span> : str</code></dt>
<dd>
<div class="desc"><p>A summary as a string of the information recorded.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>summary_str</code></strong> :&ensp;<code>str</code></dt>
<dd>The summary of the information.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def summary(self) -&gt; str:
    &#39;&#39;&#39;
    A summary as a string of the information recorded.

    Returns
    -------
    summary_str : str
        The summary of the information.
    &#39;&#39;&#39;
    summary_str =  f&#39;Summary of Value Iteration run&#39;
    summary_str += f&#39;\n  - Model: {self.model.state_count} state, {self.model.action_count} action, {self.model.observation_count} observations&#39;
    summary_str += f&#39;\n  - Converged or stopped after {len(self.expansion_times)} expansion steps and {len(self.backup_times)} backup steps.&#39;

    if self.tracking_level &gt;= 1:
        summary_str += f&#39;\n  - Resulting value function has {self.alpha_vector_counts[-1]} alpha vectors.&#39;
        summary_str += f&#39;\n  - Converged in {(sum(self.expansion_times) + sum(self.backup_times)):.4f}s&#39;
        summary_str += f&#39;\n&#39;

        summary_str += f&#39;\n  - Expand function took on average {sum(self.expansion_times) / len(self.expansion_times):.4f}s &#39;
        if self.expand_append:
            summary_str += f&#39;and yielded on average {sum(np.diff(self.beliefs_counts)) / len(self.beliefs_counts[1:]):.2f} beliefs per iteration.&#39;
        else:
            summary_str += f&#39;and yielded on average {sum(self.beliefs_counts[1:]) / len(self.beliefs_counts[1:]):.2f} beliefs per iteration.&#39;
        summary_str += f&#39; ({np.sum(np.divide(self.expansion_times, self.beliefs_counts[1:])) / len(self.expansion_times):.4f}s/it/belief)&#39;
        
        summary_str += f&#39;\n  - Backup function took on average {sum(self.backup_times) /len(self.backup_times):.4f}s &#39;
        summary_str += f&#39;and yielded on average value functions of size {sum(self.alpha_vector_counts[1:]) / len(self.alpha_vector_counts[1:]):.2f} per iteration.&#39;
        summary_str += f&#39; ({np.sum(np.divide(self.backup_times, self.alpha_vector_counts[1:])) / len(self.backup_times):.4f}s/it/alpha)&#39;

        summary_str += f&#39;\n  - Pruning function took on average {sum(self.pruning_times) /len(self.pruning_times):.4f}s &#39;
        summary_str += f&#39;and yielded on average prunings of {sum(self.prune_counts) / len(self.prune_counts):.2f} alpha vectors per iteration.&#39;
    
    return summary_str</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.pomdp.SolverHistory.add_backup_step"><code class="name flex">
<span>def <span class="ident">add_backup_step</span></span>(<span>self, backup_time: float, value_function_change: float, value_function: <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to add a backup step in the simulation history by recording the value function the backup function generated.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>backup_time</code></strong> :&ensp;<code>float</code></dt>
<dd>The time it took to run a step of backup of the value function. (Also known as the value function update.)</dd>
<dt><strong><code>value_function_change</code></strong> :&ensp;<code>float</code></dt>
<dd>The change between the value function of this iteration and of the previous iteration.</dd>
<dt><strong><code>value_function</code></strong> :&ensp;<code>ValueFunction</code></dt>
<dd>The value function resulting after a step of the solving process.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_backup_step(self,
                    backup_time:float,
                    value_function_change:float,
                    value_function:ValueFunction
                    ) -&gt; None:
    &#39;&#39;&#39;
    Function to add a backup step in the simulation history by recording the value function the backup function generated.

    Parameters
    ----------
    backup_time : float
        The time it took to run a step of backup of the value function. (Also known as the value function update.)
    value_function_change : float
        The change between the value function of this iteration and of the previous iteration.
    value_function : ValueFunction
        The value function resulting after a step of the solving process.
    &#39;&#39;&#39;
    if self.tracking_level &gt;= 1:
        self.backup_times.append(float(backup_time))
        self.alpha_vector_counts.append(len(value_function))
        self.value_function_changes.append(float(value_function_change))

    if self.tracking_level &gt;= 2:
        self.value_functions.append(value_function if not value_function.is_on_gpu else value_function.to_cpu())</code></pre>
</details>
</dd>
<dt id="src.pomdp.SolverHistory.add_expand_step"><code class="name flex">
<span>def <span class="ident">add_expand_step</span></span>(<span>self, expansion_time: float, belief_set: <a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a>) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to add an expansion step in the simulation history by the explored belief set the expand function generated.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>expansion_time</code></strong> :&ensp;<code>float</code></dt>
<dd>The time it took to run a step of expansion of the belief set. (Also known as the exploration step.)</dd>
<dt><strong><code>belief_set</code></strong> :&ensp;<code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></dt>
<dd>The belief set used for the Update step of the solving process.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_expand_step(self,
                    expansion_time:float,
                    belief_set:BeliefSet
                    ) -&gt; None:
    &#39;&#39;&#39;
    Function to add an expansion step in the simulation history by the explored belief set the expand function generated.

    Parameters
    ----------
    expansion_time : float
        The time it took to run a step of expansion of the belief set. (Also known as the exploration step.)
    belief_set : BeliefSet
        The belief set used for the Update step of the solving process.
    &#39;&#39;&#39;
    if self.tracking_level &gt;= 1:
        self.expansion_times.append(float(expansion_time))
        self.beliefs_counts.append(len(belief_set))

    if self.tracking_level &gt;= 2:
        self.belief_sets.append(belief_set if not belief_set.is_on_gpu else belief_set.to_cpu())</code></pre>
</details>
</dd>
<dt id="src.pomdp.SolverHistory.add_prune_step"><code class="name flex">
<span>def <span class="ident">add_prune_step</span></span>(<span>self, prune_time: float, alpha_vectors_pruned: int) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to add a prune step in the simulation history by recording the amount of alpha vectors that were pruned by the pruning function and how long it took.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>prune_time</code></strong> :&ensp;<code>float</code></dt>
<dd>The time it took to run the pruning step.</dd>
<dt><strong><code>alpha_vectors_pruned</code></strong> :&ensp;<code>int</code></dt>
<dd>How many alpha vectors were pruned.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_prune_step(self,
                   prune_time:float,
                   alpha_vectors_pruned:int
                   ) -&gt; None:
    &#39;&#39;&#39;
    Function to add a prune step in the simulation history by recording the amount of alpha vectors that were pruned by the pruning function and how long it took.

    Parameters
    ----------
    prune_time : float
        The time it took to run the pruning step.
    alpha_vectors_pruned : int
        How many alpha vectors were pruned.
    &#39;&#39;&#39;
    if self.tracking_level &gt;= 1:
        self.pruning_times.append(prune_time)
        self.prune_counts.append(alpha_vectors_pruned)</code></pre>
</details>
</dd>
<dt id="src.pomdp.SolverHistory.plot_belief_set"><code class="name flex">
<span>def <span class="ident">plot_belief_set</span></span>(<span>self, size: int = 15) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to plot the last belief set explored during the solving process.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code>, default=<code>15</code></dt>
<dd>The scale of the plot.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_belief_set(self, size:int=15) -&gt; None:
    &#39;&#39;&#39;
    Function to plot the last belief set explored during the solving process.

    Parameters
    ----------
    size : int, default=15
        The scale of the plot.
    &#39;&#39;&#39;
    self.explored_beliefs.plot(size=size)</code></pre>
</details>
</dd>
<dt id="src.pomdp.SolverHistory.plot_solution"><code class="name flex">
<span>def <span class="ident">plot_solution</span></span>(<span>self, size: int = 5, plot_belief: bool = True) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to plot the value function of the solution.
Note: only works for 2 and 3 states models</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code>, default=<code>5</code></dt>
<dd>The figure size and general scaling factor.</dd>
<dt><strong><code>plot_belief</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Whether to plot the belief set along with the value function.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_solution(self, size:int=5, plot_belief:bool=True) -&gt; None:
    &#39;&#39;&#39;
    Function to plot the value function of the solution.
    Note: only works for 2 and 3 states models

    Parameters
    ----------
    size : int, default=5
        The figure size and general scaling factor.
    plot_belief : bool, default=True
        Whether to plot the belief set along with the value function.
    &#39;&#39;&#39;
    self.solution.plot(size=size, belief_set=(self.explored_beliefs if plot_belief else None))</code></pre>
</details>
</dd>
<dt id="src.pomdp.SolverHistory.save_history_video"><code class="name flex">
<span>def <span class="ident">save_history_video</span></span>(<span>self, custom_name: Optional[str] = None, compare_with: Union[list, <a title="src.mdp.ValueFunction" href="mdp.html#src.mdp.ValueFunction">ValueFunction</a>, <a title="src.mdp.SolverHistory" href="mdp.html#src.mdp.SolverHistory">SolverHistory</a>] = [], graph_names: list[str] = [], fps: int = 10) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Function to generate a video of the training history. Another solved solver or list of solvers can be put in the 'compare_with' parameter.
These other solver's value function will be overlapped with the 1st value function.
The explored beliefs of the main solver are also mapped out. (other solvers's explored beliefs will not be plotted)
Also, a single value function or list of value functions can be sent in but they will be fixed in the video.</p>
<p>Note: only works for 2-state models.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>custom_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The name the video will be saved with.</dd>
<dt><strong><code>compare_with</code></strong> :&ensp;<code>PBVI</code> or <code>ValueFunction</code> or <code>list</code>, default=<code>[]</code></dt>
<dd>Value functions or other solvers to plot against the current solver's history.</dd>
<dt><strong><code>graph_names</code></strong> :&ensp;<code>list[str]</code>, default=<code>[]</code></dt>
<dd>Names of the graphs for the legend of which graph is being plot.</dd>
<dt><strong><code>fps</code></strong> :&ensp;<code>int</code>, default=<code>10</code></dt>
<dd>How many frames per second should the saved video have.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_history_video(self,
                       custom_name:Union[str,None]=None,
                       compare_with:Union[list, ValueFunction, MDP_SolverHistory]=[],
                       graph_names:list[str]=[],
                       fps:int=10
                       ) -&gt; None:
    &#39;&#39;&#39;
    Function to generate a video of the training history. Another solved solver or list of solvers can be put in the &#39;compare_with&#39; parameter.
    These other solver&#39;s value function will be overlapped with the 1st value function.
    The explored beliefs of the main solver are also mapped out. (other solvers&#39;s explored beliefs will not be plotted)
    Also, a single value function or list of value functions can be sent in but they will be fixed in the video.

    Note: only works for 2-state models.

    Parameters
    ----------
    custom_name : str, optional
        The name the video will be saved with.
    compare_with : PBVI or ValueFunction or list, default=[]
        Value functions or other solvers to plot against the current solver&#39;s history.
    graph_names : list[str], default=[]
        Names of the graphs for the legend of which graph is being plot.
    fps : int, default=10
        How many frames per second should the saved video have.
    &#39;&#39;&#39;
    assert self.tracking_level &gt;= 2, &#34;Tracking level is set too low, increase it to 2 if you want to have value function and belief sets tracking as well.&#34;
    assert self.model.state_count in [2,3], &#34;Can&#39;t plot for models with state count other than 2 or 3&#34; # TODO Make support for gird videos

    if self.model.state_count == 2:
        self._save_history_video_2D(custom_name, compare_with, copy.copy(graph_names), fps)
    elif self.model.state_count == 3:
        raise Exception(&#39;Not implemented...&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.pomdp.load_POMDP_file" href="#src.pomdp.load_POMDP_file">load_POMDP_file</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.pomdp.Agent" href="#src.pomdp.Agent">Agent</a></code></h4>
<ul class="">
<li><code><a title="src.pomdp.Agent.get_best_action" href="#src.pomdp.Agent.get_best_action">get_best_action</a></code></li>
<li><code><a title="src.pomdp.Agent.run_n_simulations" href="#src.pomdp.Agent.run_n_simulations">run_n_simulations</a></code></li>
<li><code><a title="src.pomdp.Agent.run_n_simulations_parallel" href="#src.pomdp.Agent.run_n_simulations_parallel">run_n_simulations_parallel</a></code></li>
<li><code><a title="src.pomdp.Agent.simulate" href="#src.pomdp.Agent.simulate">simulate</a></code></li>
<li><code><a title="src.pomdp.Agent.train" href="#src.pomdp.Agent.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.pomdp.Belief" href="#src.pomdp.Belief">Belief</a></code></h4>
<ul class="two-column">
<li><code><a title="src.pomdp.Belief.bytes_repr" href="#src.pomdp.Belief.bytes_repr">bytes_repr</a></code></li>
<li><code><a title="src.pomdp.Belief.generate_successors" href="#src.pomdp.Belief.generate_successors">generate_successors</a></code></li>
<li><code><a title="src.pomdp.Belief.plot" href="#src.pomdp.Belief.plot">plot</a></code></li>
<li><code><a title="src.pomdp.Belief.random_state" href="#src.pomdp.Belief.random_state">random_state</a></code></li>
<li><code><a title="src.pomdp.Belief.update" href="#src.pomdp.Belief.update">update</a></code></li>
<li><code><a title="src.pomdp.Belief.values" href="#src.pomdp.Belief.values">values</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.pomdp.BeliefSet" href="#src.pomdp.BeliefSet">BeliefSet</a></code></h4>
<ul class="">
<li><code><a title="src.pomdp.BeliefSet.belief_array" href="#src.pomdp.BeliefSet.belief_array">belief_array</a></code></li>
<li><code><a title="src.pomdp.BeliefSet.belief_list" href="#src.pomdp.BeliefSet.belief_list">belief_list</a></code></li>
<li><code><a title="src.pomdp.BeliefSet.generate_all_successors" href="#src.pomdp.BeliefSet.generate_all_successors">generate_all_successors</a></code></li>
<li><code><a title="src.pomdp.BeliefSet.plot" href="#src.pomdp.BeliefSet.plot">plot</a></code></li>
<li><code><a title="src.pomdp.BeliefSet.to_cpu" href="#src.pomdp.BeliefSet.to_cpu">to_cpu</a></code></li>
<li><code><a title="src.pomdp.BeliefSet.to_gpu" href="#src.pomdp.BeliefSet.to_gpu">to_gpu</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.pomdp.BeliefValueMapping" href="#src.pomdp.BeliefValueMapping">BeliefValueMapping</a></code></h4>
<ul class="">
<li><code><a title="src.pomdp.BeliefValueMapping.add" href="#src.pomdp.BeliefValueMapping.add">add</a></code></li>
<li><code><a title="src.pomdp.BeliefValueMapping.belief_array" href="#src.pomdp.BeliefValueMapping.belief_array">belief_array</a></code></li>
<li><code><a title="src.pomdp.BeliefValueMapping.evaluate" href="#src.pomdp.BeliefValueMapping.evaluate">evaluate</a></code></li>
<li><code><a title="src.pomdp.BeliefValueMapping.update" href="#src.pomdp.BeliefValueMapping.update">update</a></code></li>
<li><code><a title="src.pomdp.BeliefValueMapping.value_array" href="#src.pomdp.BeliefValueMapping.value_array">value_array</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.pomdp.FSVI_Solver" href="#src.pomdp.FSVI_Solver">FSVI_Solver</a></code></h4>
</li>
<li>
<h4><code><a title="src.pomdp.HSVI_Solver" href="#src.pomdp.HSVI_Solver">HSVI_Solver</a></code></h4>
</li>
<li>
<h4><code><a title="src.pomdp.Model" href="#src.pomdp.Model">Model</a></code></h4>
<ul class="">
<li><code><a title="src.pomdp.Model.observe" href="#src.pomdp.Model.observe">observe</a></code></li>
<li><code><a title="src.pomdp.Model.reward" href="#src.pomdp.Model.reward">reward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.pomdp.PBVI_Solver" href="#src.pomdp.PBVI_Solver">PBVI_Solver</a></code></h4>
<ul class="two-column">
<li><code><a title="src.pomdp.PBVI_Solver.backup" href="#src.pomdp.PBVI_Solver.backup">backup</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.compute_change" href="#src.pomdp.PBVI_Solver.compute_change">compute_change</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand" href="#src.pomdp.PBVI_Solver.expand">expand</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_fsvi" href="#src.pomdp.PBVI_Solver.expand_fsvi">expand_fsvi</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ger" href="#src.pomdp.PBVI_Solver.expand_ger">expand_ger</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_hsvi" href="#src.pomdp.PBVI_Solver.expand_hsvi">expand_hsvi</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_perseus" href="#src.pomdp.PBVI_Solver.expand_perseus">expand_perseus</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ra" href="#src.pomdp.PBVI_Solver.expand_ra">expand_ra</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ssea" href="#src.pomdp.PBVI_Solver.expand_ssea">expand_ssea</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ssga" href="#src.pomdp.PBVI_Solver.expand_ssga">expand_ssga</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.expand_ssra" href="#src.pomdp.PBVI_Solver.expand_ssra">expand_ssra</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.solve" href="#src.pomdp.PBVI_Solver.solve">solve</a></code></li>
<li><code><a title="src.pomdp.PBVI_Solver.test_n_simulations" href="#src.pomdp.PBVI_Solver.test_n_simulations">test_n_simulations</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.pomdp.Simulation" href="#src.pomdp.Simulation">Simulation</a></code></h4>
<ul class="">
<li><code><a title="src.pomdp.Simulation.run_action" href="#src.pomdp.Simulation.run_action">run_action</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.pomdp.SimulationHistory" href="#src.pomdp.SimulationHistory">SimulationHistory</a></code></h4>
<ul class="">
<li><code><a title="src.pomdp.SimulationHistory.add" href="#src.pomdp.SimulationHistory.add">add</a></code></li>
<li><code><a title="src.pomdp.SimulationHistory.beliefs" href="#src.pomdp.SimulationHistory.beliefs">beliefs</a></code></li>
<li><code><a title="src.pomdp.SimulationHistory.to_dataframe" href="#src.pomdp.SimulationHistory.to_dataframe">to_dataframe</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.pomdp.Solver" href="#src.pomdp.Solver">Solver</a></code></h4>
<ul class="">
<li><code><a title="src.pomdp.Solver.solve" href="#src.pomdp.Solver.solve">solve</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.pomdp.SolverHistory" href="#src.pomdp.SolverHistory">SolverHistory</a></code></h4>
<ul class="two-column">
<li><code><a title="src.pomdp.SolverHistory.add_backup_step" href="#src.pomdp.SolverHistory.add_backup_step">add_backup_step</a></code></li>
<li><code><a title="src.pomdp.SolverHistory.add_expand_step" href="#src.pomdp.SolverHistory.add_expand_step">add_expand_step</a></code></li>
<li><code><a title="src.pomdp.SolverHistory.add_prune_step" href="#src.pomdp.SolverHistory.add_prune_step">add_prune_step</a></code></li>
<li><code><a title="src.pomdp.SolverHistory.explored_beliefs" href="#src.pomdp.SolverHistory.explored_beliefs">explored_beliefs</a></code></li>
<li><code><a title="src.pomdp.SolverHistory.plot_belief_set" href="#src.pomdp.SolverHistory.plot_belief_set">plot_belief_set</a></code></li>
<li><code><a title="src.pomdp.SolverHistory.plot_solution" href="#src.pomdp.SolverHistory.plot_solution">plot_solution</a></code></li>
<li><code><a title="src.pomdp.SolverHistory.save_history_video" href="#src.pomdp.SolverHistory.save_history_video">save_history_video</a></code></li>
<li><code><a title="src.pomdp.SolverHistory.solution" href="#src.pomdp.SolverHistory.solution">solution</a></code></li>
<li><code><a title="src.pomdp.SolverHistory.summary" href="#src.pomdp.SolverHistory.summary">summary</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>